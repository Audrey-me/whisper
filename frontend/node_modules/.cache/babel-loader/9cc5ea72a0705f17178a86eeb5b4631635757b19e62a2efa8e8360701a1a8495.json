{"ast":null,"code":"/**\n * @file Tokenizers are used to prepare textual inputs for a model.\n * \n * **Example:** Create an `AutoTokenizer` and use it to tokenize a sentence.\n * This will automatically detect the tokenizer type based on the tokenizer class defined in `tokenizer.json`.\n * ```javascript\n * import { AutoTokenizer } from '@xenova/transformers';\n * \n * let tokenizer = await AutoTokenizer.from_pretrained('bert-base-uncased');\n * let { input_ids } = await tokenizer('I love transformers!');\n * // Tensor {\n * //   data: BigInt64Array(6) [101n, 1045n, 2293n, 19081n, 999n, 102n],\n * //   dims: [1, 6],\n * //   type: 'int64',\n * //   size: 6,\n * // }\n * ```\n * \n * @module tokenizers\n */\n\nimport { Callable, reverseDictionary, escapeRegExp, isIntegralNumber, mergeArrays } from './utils/core.js';\nimport { getModelJSON } from './utils/hub.js';\nimport { max, min } from './utils/maths.js';\nimport { Tensor } from './utils/tensor.js';\n\n/**\n * @typedef {import('./utils/hub.js').PretrainedOptions} PretrainedOptions\n */\n\n/**\n * Loads a tokenizer from the specified path.\n * @param {string} pretrained_model_name_or_path The path to the tokenizer directory.\n * @param {PretrainedOptions} options Additional options for loading the tokenizer.\n * @returns {Promise<Array>} A promise that resolves with information about the loaded tokenizer.\n */\nasync function loadTokenizer(pretrained_model_name_or_path, options) {\n  let info = await Promise.all([getModelJSON(pretrained_model_name_or_path, 'tokenizer.json', true, options), getModelJSON(pretrained_model_name_or_path, 'tokenizer_config.json', true, options)]);\n  return info;\n}\n\n/**\n * Helper method to construct a pattern from a config object.\n * @param {Object} pattern The pattern object.\n * @returns {RegExp|string|null} The compiled pattern.\n */\nfunction createPattern(pattern) {\n  if (pattern.Regex) {\n    return new RegExp(pattern.Regex, 'gu');\n  } else if (pattern.String) {\n    return pattern.String;\n  } else {\n    console.warn('Unknown pattern type:', pattern);\n    return null;\n  }\n}\n\n/**\n * Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms\n * @param {string} text The text to clean up.\n * @returns {string} The cleaned up text.\n */\nfunction clean_up_tokenization(text) {\n  // Clean up a list of simple English tokenization artifacts\n  // like spaces before punctuations and abbreviated forms\n  return text.replace(/ \\./g, '.').replace(/ \\?/g, '?').replace(/ \\!/g, '!').replace(/ ,/g, ',').replace(/ \\' /g, \"'\").replace(/ n\\'t/g, \"n't\").replace(/ \\'m/g, \"'m\").replace(/ \\'s/g, \"'s\").replace(/ \\'ve/g, \"'ve\").replace(/ \\'re/g, \"'re\");\n}\n\n/**\n * Helper function to fuse consecutive values in an array equal to the specified value.\n * @param {Array} arr The input array\n * @param {any} value The value to fuse on.\n */\nfunction fuse(arr, value) {\n  let fused = [];\n  let i = 0;\n  while (i < arr.length) {\n    fused.push(arr[i]);\n    if (arr[i] !== value) {\n      ++i;\n      continue;\n    }\n    while (i < arr.length && arr[i] === value) {\n      ++i;\n    }\n  }\n  return fused;\n}\n\n/**\n * Split a string on whitespace.\n * @param {string} text The text to split.\n * @returns {string[]} The split string.\n */\nfunction whitespace_split(text) {\n  return text.match(/\\S+/g) || [];\n}\n\n/**\n * Abstract base class for tokenizer models.\n *\n * @extends Callable\n */\nexport class TokenizerModel extends Callable {\n  /**\n   * Creates a new instance of TokenizerModel.\n   * @param {Object} config The configuration object for the TokenizerModel.\n   */\n  constructor(config) {\n    super();\n    this.config = config;\n\n    /** @type {string[]} */\n    this.vocab = [];\n\n    /**\n     * A mapping of tokens to ids.\n     * @type {Map<string, number>}\n     */\n    this.tokens_to_ids = new Map();\n    this.unk_token_id = undefined;\n    this.unk_token = undefined;\n    this.end_of_word_suffix = undefined;\n\n    /** @type {boolean} Whether to fuse unknown tokens when encoding. Defaults to false. */\n    this.fuse_unk = false;\n  }\n\n  /**\n   * Instantiates a new TokenizerModel instance based on the configuration object provided.\n   * @param {Object} config The configuration object for the TokenizerModel.\n   * @param {...*} args Optional arguments to pass to the specific TokenizerModel constructor.\n   * @returns {TokenizerModel} A new instance of a TokenizerModel.\n   * @throws Will throw an error if the TokenizerModel type in the config is not recognized.\n   */\n  static fromConfig(config) {\n    for (var _len = arguments.length, args = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      args[_key - 1] = arguments[_key];\n    }\n    switch (config.type) {\n      case 'WordPiece':\n        return new WordPieceTokenizer(config);\n      case 'Unigram':\n        // @ts-ignore\n        return new Unigram(config, ...args);\n      case 'BPE':\n        // @ts-ignore\n        return new BPE(config, ...args);\n      default:\n        throw new Error(`Unknown TokenizerModel type: ${config.type}`);\n    }\n  }\n\n  /**\n   * Internal function to call the TokenizerModel instance.\n   * @param {string[]} tokens The tokens to encode.\n   * @returns {number[]} The encoded token IDs.\n   */\n  _call(tokens) {\n    return this.encode(tokens);\n  }\n\n  /**\n   * Encodes a list of tokens into a list of token IDs.\n   * @param {string[]} tokens The tokens to encode.\n   * @returns {number[]} The encoded token IDs.\n   * @throws Will throw an error if not implemented in a subclass.\n   */\n  encode(tokens) {\n    throw Error(\"encode should be implemented in subclass.\");\n  }\n\n  /**\n   * Converts a list of tokens into a list of token IDs.\n   * @param {string[]} tokens The tokens to convert.\n   * @returns {number[]} The converted token IDs.\n   */\n  convert_tokens_to_ids(tokens) {\n    let ids = tokens.map(t => this.tokens_to_ids.get(t) ?? this.unk_token_id);\n    if (this.fuse_unk) {\n      // Fuse unknown tokens\n      ids = fuse(ids, this.unk_token_id);\n    }\n    return ids;\n  }\n\n  /**\n   * Converts a list of token IDs into a list of tokens.\n   * @param {number[]} ids The token IDs to convert.\n   * @returns {string[]} The converted tokens.\n   */\n  convert_ids_to_tokens(ids) {\n    return ids.map(i => this.vocab[i] ?? this.unk_token);\n  }\n}\n\n/**\n * A subclass of TokenizerModel that uses WordPiece encoding to encode tokens.\n * @extends TokenizerModel\n */\nclass WordPieceTokenizer extends TokenizerModel {\n  /**\n   * @param {Object} config The configuration object.\n   * @param {Map<string, number>} config.vocab A mapping of tokens to ids.\n   * @param {string} config.unk_token The unknown token string.\n   * @param {string} config.continuing_subword_prefix The prefix to use for continuing subwords.\n   */\n  constructor(config) {\n    super(config);\n    /**\n     * A mapping of tokens to ids.\n     * @type {Map<string, number>}\n     */\n    this.tokens_to_ids = config.vocab;\n\n    /**\n     * The id of the unknown token.\n     * @type {number}\n     */\n    this.unk_token_id = this.tokens_to_ids.get(config.unk_token);\n\n    /**\n     * The unknown token string.\n     * @type {string}\n     */\n    this.unk_token = config.unk_token;\n\n    /**\n     * An array of tokens.\n     * @type {string[]}\n     */\n    this.vocab = new Array(this.tokens_to_ids.size);\n    for (const [key, value] of this.tokens_to_ids) {\n      this.vocab[value] = key;\n    }\n  }\n\n  /**\n   * Encodes an array of tokens using WordPiece encoding.\n   * @param {Array} tokens The tokens to encode.\n   * @returns {Array} An array of encoded tokens.\n   */\n  encode(tokens) {\n    let outputTokens = [];\n    for (let token of tokens) {\n      let chars = [...token];\n      // TODO add\n      // if len(chars) > self.max_input_chars_per_word:\n      //     output_tokens.append(self.unk_token)\n      //     continue\n\n      let isUnknown = false;\n      let start = 0;\n      let subTokens = [];\n      while (start < chars.length) {\n        let end = chars.length;\n        let currentSubstring = null;\n        while (start < end) {\n          let substr = chars.slice(start, end).join('');\n          if (start > 0) {\n            substr = this.config.continuing_subword_prefix + substr;\n          }\n          if (this.tokens_to_ids.has(substr)) {\n            currentSubstring = substr;\n            break;\n          }\n          --end;\n        }\n        if (currentSubstring === null) {\n          isUnknown = true;\n          break;\n        }\n        subTokens.push(currentSubstring);\n        start = end;\n      }\n      if (isUnknown) {\n        outputTokens.push(this.unk_token);\n      } else {\n        outputTokens.push(...subTokens);\n      }\n    }\n    return outputTokens;\n  }\n}\n\n/**\n * Class representing a Unigram tokenizer model.\n * @extends TokenizerModel\n */\nclass Unigram extends TokenizerModel {\n  /**\n   * Create a new Unigram tokenizer model.\n   * @param {Object} config The configuration object for the Unigram model.\n   * @param {number} config.unk_id The ID of the unknown token\n   * @param {Map<string, number>} config.vocab A mapping of tokens to scores.\n   * @param {Object} moreConfig Additional configuration object for the Unigram model.\n   */\n  constructor(config, moreConfig) {\n    super(config);\n    this.vocab = new Array(config.vocab.size);\n    this.scores = new Array(config.vocab.size);\n    let count = 0;\n    config.vocab.forEach((value, key) => {\n      this.vocab[count] = key;\n      this.scores[count] = value;\n      ++count;\n    });\n    this.unk_token_id = config.unk_id;\n    this.unk_token = this.vocab[config.unk_id];\n    this.tokens_to_ids = new Map(this.vocab.map((x, i) => [x, i]));\n    this.bosToken = ' '; // beginning of a sentence token\n\n    this.bosTokenId = this.tokens_to_ids.get(this.bosToken); // NOTE: may be undefined\n    this.eosToken = moreConfig.eos_token;\n    this.eosTokenId = this.tokens_to_ids.get(this.eosToken);\n    this.unkToken = this.vocab[this.unk_token_id];\n    this.minScore = min(this.scores)[0];\n    this.unkScore = this.minScore - 10.0;\n    this.scores[this.unk_token_id] = this.unkScore;\n    this.trie = new CharTrie();\n    this.trie.extend(this.vocab);\n\n    // NOTE: `fuse_unk` is hardcoded to true for Unigram models\n    // See: https://github.com/huggingface/tokenizers/blob/b58227c7f1ccf8b73ee2268354336da56d91e492/tokenizers/src/models/unigram/model.rs#L119\n    this.fuse_unk = true;\n  }\n\n  /**\n   * Populates lattice nodes.\n   * @param {TokenLattice} lattice The token lattice to populate with nodes.\n   */\n  populateNodes(lattice) {\n    const sentence = lattice.sentence;\n    const len = sentence.length;\n    let beginPos = 0;\n    while (beginPos < len) {\n      const mblen = 1;\n      let hasSingleNode = false;\n      const tokens = [];\n      for (let token of this.trie.commonPrefixSearch(sentence.slice(beginPos))) {\n        tokens.push(token);\n        const tokenId = this.tokens_to_ids.get(token);\n        const tokenScore = this.scores[tokenId];\n        const n = token.length;\n        lattice.insert(beginPos, n, tokenScore, tokenId);\n        if (!hasSingleNode && n === mblen) {\n          hasSingleNode = true;\n        }\n      }\n      if (!hasSingleNode) {\n        lattice.insert(beginPos, mblen, this.unkScore, this.unk_token_id);\n      }\n      beginPos += mblen;\n    }\n  }\n\n  /**\n   * Encodes an array of tokens into an array of subtokens using the unigram model.\n   *\n   * @param {string} normalized The normalized string.\n   * @returns {string[]} An array of subtokens obtained by encoding the input tokens using the unigram model.\n   */\n  tokenize(normalized) {\n    const lattice = new TokenLattice(normalized, this.bosTokenId, this.eosTokenId);\n    this.populateNodes(lattice);\n    return lattice.tokens();\n  }\n\n  /**\n   * Encodes an array of tokens using WordPiece encoding.\n   * @param {Array} tokens The tokens to encode.\n   * @returns {Array} An array of encoded tokens.\n   */\n  encode(tokens) {\n    let toReturn = [];\n    for (let token of tokens) {\n      const tokenized = this.tokenize(token);\n      toReturn.push(...tokenized);\n    }\n    return toReturn;\n  }\n}\n\n/**\n * Returns list of utf-8 byte and a mapping to unicode strings.\n * Specifically avoids mapping to whitespace/control characters the BPE code barfs on.\n * @returns {Object} Object with utf-8 byte keys and unicode string values.\n */\nconst BYTES_TO_UNICODE = (() => {\n  // Returns list of utf-8 byte and a mapping to unicode strings.\n  // We specifically avoids mapping to whitespace/control characters\n  // the bpe code barfs on.\n\n  const bs = [...Array.from({\n    length: \"~\".charCodeAt(0) - \"!\".charCodeAt(0) + 1\n  }, (_, i) => i + \"!\".charCodeAt(0)), ...Array.from({\n    length: \"¬\".charCodeAt(0) - \"¡\".charCodeAt(0) + 1\n  }, (_, i) => i + \"¡\".charCodeAt(0)), ...Array.from({\n    length: \"ÿ\".charCodeAt(0) - \"®\".charCodeAt(0) + 1\n  }, (_, i) => i + \"®\".charCodeAt(0))];\n  let cs = bs.slice();\n  let n = 0;\n  for (let b = 0; b < 256; ++b) {\n    if (!bs.includes(b)) {\n      bs.push(b);\n      cs.push(256 + n);\n      n += 1;\n    }\n  }\n  let ccs = cs.map(n => String.fromCharCode(n));\n  return Object.fromEntries(bs.map((b, i) => [b, ccs[i]]));\n})();\nconst UNICODE_TO_BYTES = reverseDictionary(BYTES_TO_UNICODE);\n\n/**\n * BPE class for encoding text into Byte-Pair-Encoding (BPE) tokens.\n * @extends TokenizerModel\n */\nclass BPE extends TokenizerModel {\n  /**\n   * Create a BPE instance.\n   * @param {Object} config The configuration object for BPE.\n   * @param {Map<string, number>} config.vocab A mapping of tokens to ids.\n   * @param {string} config.unk_token The unknown token used for out of vocabulary words.\n   * @param {string} config.end_of_word_suffix The suffix to place at the end of each word.\n   * @param {Array} config.merges An array of BPE merges as strings.\n   */\n  constructor(config) {\n    super(config);\n    this.tokens_to_ids = config.vocab;\n    this.unk_token_id = this.tokens_to_ids.get(config.unk_token);\n    this.unk_token = config.unk_token;\n    this.vocab = new Array(this.tokens_to_ids.size);\n    for (const [key, value] of this.tokens_to_ids) {\n      this.vocab[value] = key;\n    }\n    this.bpe_ranks = Object.fromEntries(config.merges.map((x, i) => [x, i]));\n    this.merges = config.merges.map(x => x.split(/\\s+/));\n    this.end_of_word_suffix = config.end_of_word_suffix;\n    this.byte_fallback = this.config.byte_fallback ?? false;\n    if (this.byte_fallback) {\n      this.text_encoder = new TextEncoder();\n    }\n    this.cache = Object.create(null);\n    this.fuse_unk ??= this.config.fuse_unk;\n  }\n\n  /**\n   * Get all the possible pairs of characters in a word.\n   * @param {string[]} word The word to get pairs from.\n   * @returns {Array} An array of pairs.\n   */\n  get_pairs(word) {\n    let pairs = new Set();\n    let prev_char = word[0];\n    for (let i = 1; i < word.length; ++i) {\n      let char = word[i];\n      pairs.add(`${prev_char} ${char}`);\n      prev_char = char;\n    }\n    return Array.from(pairs);\n  }\n\n  /**\n   * Apply Byte-Pair-Encoding (BPE) to a given token.\n   * @param {string} token The token to encode.\n   * @returns {string} The BPE encoded token.\n   */\n  bpe(token) {\n    if (token in this.cache) {\n      return this.cache[token];\n    }\n    let word = Array.from(token);\n    if (this.end_of_word_suffix) {\n      word[word.length - 1] += this.end_of_word_suffix;\n    }\n    let pairs = this.get_pairs(word);\n    if (!pairs.length) {\n      if (this.end_of_word_suffix) {\n        token += this.end_of_word_suffix;\n      }\n      return token;\n    }\n    while (true) {\n      let bigram = pairs.reduce((a, b) => {\n        let c = this.bpe_ranks[a] ?? Infinity;\n        let d = this.bpe_ranks[b] ?? Infinity;\n        return c <= d ? a : b;\n      });\n      if (!(bigram in this.bpe_ranks)) {\n        break;\n      }\n      let [first, second] = bigram.split(/\\s+/g);\n      let new_word = [];\n      let i = 0;\n      let j = -1;\n      while (i < word.length) {\n        try {\n          j = word.indexOf(first, i);\n          if (j === -1) throw \"Error\";\n        } catch (e) {\n          new_word.push(...word.slice(i));\n          break;\n        }\n        new_word.push(...word.slice(i, j));\n        i = j;\n        if (word[i] === first && i < word.length - 1 && word[i + 1] === second) {\n          new_word.push(first + second);\n          i += 2;\n        } else {\n          new_word.push(word[i]);\n          i += 1;\n        }\n      }\n      word = new_word;\n      if (word.length === 1) {\n        break;\n      } else {\n        pairs = this.get_pairs(word);\n      }\n    }\n    let final_word = word.join(\" \");\n    this.cache[token] = final_word;\n    return final_word;\n  }\n\n  /**\n   * Encodes the input sequence of tokens using the BPE algorithm and returns the resulting subword tokens.\n   * @param {Array} tokens The input sequence of tokens to encode.\n   * @returns {Array} The resulting subword tokens after applying the BPE algorithm to the input sequence of tokens.\n   */\n  encode(tokens) {\n    let outputTokens = [];\n    for (let token of tokens) {\n      let bpe_token_list = this.bpe(token).split(' ');\n      for (let t of bpe_token_list) {\n        if (this.tokens_to_ids.has(t)) {\n          outputTokens.push(t);\n        } else {\n          if (this.byte_fallback) {\n            outputTokens.push(...Array.from(this.text_encoder.encode(t)).map(x => `<0x${x.toString(16).toUpperCase().padStart(2, '0')}>`));\n          } else {\n            outputTokens.push(this.unk_token);\n          }\n        }\n      }\n    }\n    return outputTokens;\n  }\n}\n\n/**\n * A base class for text normalization.\n * @abstract\n */\nclass Normalizer extends Callable {\n  /**\n   * @param {Object} config The configuration object for the normalizer.\n   */\n  constructor(config) {\n    super();\n    this.config = config;\n  }\n\n  /**\n   * Factory method for creating normalizers from config objects.\n   * @static\n   * @param {Object} config The configuration object for the normalizer.\n   * @returns {Normalizer} A Normalizer object.\n   * @throws {Error} If an unknown Normalizer type is specified in the config.\n   */\n  static fromConfig(config) {\n    if (config === null) return null;\n    switch (config.type) {\n      case 'BertNormalizer':\n        return new BertNormalizer(config);\n      case 'Precompiled':\n        return new Precompiled(config);\n      case 'Sequence':\n        return new NormalizerSequence(config);\n      case 'Replace':\n        return new Replace(config);\n      case 'NFC':\n        return new NFC(config);\n      case 'NFKD':\n        return new NFKD(config);\n      case 'StripAccents':\n        return new StripAccents(config);\n      case 'Lowercase':\n        return new Lowercase(config);\n      case 'Prepend':\n        return new Prepend(config);\n      default:\n        throw new Error(`Unknown Normalizer type: ${config.type}`);\n    }\n  }\n\n  /**\n   * Normalize the input text.\n   * @abstract\n   * @param {string} text The text to normalize.\n   * @returns {string} The normalized text.\n   * @throws {Error} If this method is not implemented in a subclass.\n   */\n  normalize(text) {\n    throw Error(\"normalize should be implemented in subclass.\");\n  }\n\n  /**\n   * Alias for {@link Normalizer#normalize}.\n   * @param {string} text The text to normalize.\n   * @returns {string} The normalized text.\n   */\n  _call(text) {\n    return this.normalize(text);\n  }\n}\n\n/**\n * Replace normalizer that replaces occurrences of a pattern with a given string or regular expression.\n * @extends Normalizer\n */\nclass Replace extends Normalizer {\n  /**\n   * Normalize the input text by replacing the pattern with the content.\n   * @param {string} text The input text to be normalized.\n   * @returns {string} The normalized text after replacing the pattern with the content.\n   */\n  normalize(text) {\n    let pattern = createPattern(this.config.pattern);\n    if (pattern === null) {\n      return text;\n    }\n    text = text.replaceAll(pattern, this.config.content);\n    return text;\n  }\n}\n\n/**\n * A normalizer that applies Unicode normalization form C (NFC) to the input text.\n * @extends Normalizer\n */\nclass NFC extends Normalizer {\n  /**\n   * Normalize the input text by applying Unicode normalization form C (NFC).\n   * @param {string} text The input text to be normalized.\n   * @returns {string} The normalized text.\n   */\n  normalize(text) {\n    text = text.normalize('NFC');\n    return text;\n  }\n}\n\n/**\n * NFKD Normalizer.\n * @extends Normalizer\n */\nclass NFKD extends Normalizer {\n  /**\n   * Normalize text using NFKD normalization.\n   * @param {string} text The text to be normalized.\n   * @returns {string} The normalized text.\n   */\n  normalize(text) {\n    text = text.normalize('NFKD');\n    return text;\n  }\n}\n\n/**\n * StripAccents normalizer removes all accents from the text.\n * @extends Normalizer\n */\nclass StripAccents extends Normalizer {\n  /**\n   * Remove all accents from the text.\n   * @param {string} text The input text.\n   * @returns {string} The normalized text without accents.\n   */\n  normalize(text) {\n    text = text.replace(/[\\u0300-\\u036f]/g, '');\n    return text;\n  }\n}\n\n/**\n * A Normalizer that lowercases the input string.\n * @extends Normalizer\n */\nclass Lowercase extends Normalizer {\n  /**\n   * Lowercases the input string.\n   * @param {string} text The text to normalize.\n   * @returns {string} The normalized text.\n   */\n  normalize(text) {\n    text = text.toLowerCase();\n    return text;\n  }\n}\n\n/**\n * A Normalizer that prepends a string to the input string.\n * @extends Normalizer\n */\nclass Prepend extends Normalizer {\n  /**\n   * Prepends the input string.\n   * @param {string} text The text to normalize.\n   * @returns {string} The normalized text.\n   */\n  normalize(text) {\n    text = this.config.prepend + text;\n    return text;\n  }\n}\n\n/**\n * A Normalizer that applies a sequence of Normalizers.\n * @extends Normalizer\n */\nclass NormalizerSequence extends Normalizer {\n  /**\n  * Create a new instance of NormalizerSequence.\n  * @param {Object} config The configuration object.\n  * @param {Object[]} config.normalizers An array of Normalizer configuration objects.\n  */\n  constructor(config) {\n    super(config);\n    this.normalizers = config.normalizers.map(x => Normalizer.fromConfig(x));\n  }\n  /**\n  * Apply a sequence of Normalizers to the input text.\n  * @param {string} text The text to normalize.\n  * @returns {string} The normalized text.\n  */\n  normalize(text) {\n    return this.normalizers.reduce((t, normalizer) => {\n      return normalizer.normalize(t);\n    }, text);\n  }\n}\n\n/**\n * A class representing a normalizer used in BERT tokenization.\n * @extends Normalizer\n */\nclass BertNormalizer extends Normalizer {\n  /**\n   * Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the input text.\n   *\n   * @param {string} text The input text to tokenize.\n   * @returns {string} The tokenized text with whitespace added around CJK characters.\n   */\n  _tokenize_chinese_chars(text) {\n    /* Adds whitespace around any CJK character. */\n    let output = [];\n    for (let i = 0; i < text.length; ++i) {\n      let char = text[i];\n      let cp = char.charCodeAt(0);\n      if (this._is_chinese_char(cp)) {\n        output.push(\" \");\n        output.push(char);\n        output.push(\" \");\n      } else {\n        output.push(char);\n      }\n    }\n    return output.join(\"\");\n  }\n\n  /**\n   * Checks whether the given Unicode codepoint represents a CJK (Chinese, Japanese, or Korean) character.\n   *\n   * A \"chinese character\" is defined as anything in the CJK Unicode block:\n   * https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n   *\n   * Note that the CJK Unicode block is NOT all Japanese and Korean characters, despite its name.\n   * The modern Korean Hangul alphabet is a different block, as is Japanese Hiragana and Katakana.\n   * Those alphabets are used to write space-separated words, so they are not treated specially\n   * and are handled like all other languages.\n   *\n   * @param {number} cp The Unicode codepoint to check.\n   * @returns {boolean} True if the codepoint represents a CJK character, false otherwise.\n   */\n  _is_chinese_char(cp) {\n    return cp >= 0x4E00 && cp <= 0x9FFF || cp >= 0x3400 && cp <= 0x4DBF || cp >= 0x20000 && cp <= 0x2A6DF || cp >= 0x2A700 && cp <= 0x2B73F || cp >= 0x2B740 && cp <= 0x2B81F || cp >= 0x2B820 && cp <= 0x2CEAF || cp >= 0xF900 && cp <= 0xFAFF || cp >= 0x2F800 && cp <= 0x2FA1F;\n  }\n  /**\n   * Strips accents from the given text.\n   * @param {string} text The text to strip accents from.\n   * @returns {string} The text with accents removed.\n   */\n  stripAccents(text) {\n    return text.normalize('NFD').replace(/[\\u0300-\\u036f]/g, '');\n  }\n\n  /**\n   * Normalizes the given text based on the configuration.\n   * @param {string} text The text to normalize.\n   * @returns {string} The normalized text.\n   */\n  normalize(text) {\n    // TODO use rest of config\n    // config.clean_text,\n    // config.handle_chinese_chars,\n    // config.strip_accents,\n    // config.lowercase,\n\n    if (this.config.handle_chinese_chars) {\n      text = this._tokenize_chinese_chars(text);\n    }\n    if (this.config.lowercase) {\n      text = text.toLowerCase();\n      if (this.config.strip_accents !== false) {\n        text = this.stripAccents(text);\n      }\n    } else if (this.config.strip_accents) {\n      text = this.stripAccents(text);\n    }\n    return text;\n  }\n}\n\n/**\n * A callable class representing a pre-tokenizer used in tokenization. Subclasses\n * should implement the `pre_tokenize_text` method to define the specific pre-tokenization logic.\n * @extends Callable\n */\nclass PreTokenizer extends Callable {\n  /**\n  * Factory method that returns an instance of a subclass of `PreTokenizer` based on the provided configuration.\n  *\n  * @static\n  * @param {Object} config A configuration object for the pre-tokenizer.\n  * @returns {PreTokenizer} An instance of a subclass of `PreTokenizer`.\n  * @throws {Error} If the provided configuration object does not correspond to any known pre-tokenizer.\n  */\n  static fromConfig(config) {\n    if (config === null) return null;\n    switch (config.type) {\n      case 'BertPreTokenizer':\n        return new BertPreTokenizer(config);\n      case 'Sequence':\n        return new PreTokenizerSequence(config);\n      case 'WhitespaceSplit':\n        return new WhitespaceSplit(config);\n      case 'Metaspace':\n        return new MetaspacePreTokenizer(config);\n      case 'ByteLevel':\n        return new ByteLevelPreTokenizer(config);\n      case 'Split':\n        return new SplitPreTokenizer(config);\n      default:\n        throw new Error(`Unknown PreTokenizer type: ${config.type}`);\n    }\n  }\n\n  /**\n  * Method that should be implemented by subclasses to define the specific pre-tokenization logic.\n  *\n  * @abstract\n  * @param {string} text The text to pre-tokenize.\n  * @returns {string[]} The pre-tokenized text.\n  * @throws {Error} If the method is not implemented in the subclass.\n  */\n  pre_tokenize_text(text) {\n    throw Error(\"pre_tokenize_text should be implemented in subclass.\");\n  }\n\n  /**\n   * Tokenizes the given text into pre-tokens.\n   * @param {string|string[]} text The text or array of texts to pre-tokenize.\n   * @returns {string[]} An array of pre-tokens.\n   */\n  pre_tokenize(text) {\n    let result = [];\n    if (Array.isArray(text)) {\n      result = text.map(x => this.pre_tokenize_text(x));\n    } else {\n      result = this.pre_tokenize_text(text);\n    }\n    return result.flat();\n  }\n\n  /**\n   * Alias for {@link PreTokenizer#pre_tokenize}.\n   * @param {string|string[]} text The text or array of texts to pre-tokenize.\n   * @returns {string[]} An array of pre-tokens.\n   */\n  _call(text) {\n    return this.pre_tokenize(text);\n  }\n}\n\n/**\n * @extends PreTokenizer\n */\nclass BertPreTokenizer extends PreTokenizer {\n  /**\n   * A PreTokenizer that splits text into wordpieces using a basic tokenization scheme\n   * similar to that used in the original implementation of BERT.\n   * \n   * @param {Object} config The configuration object.\n   */\n  constructor(config) {\n    super();\n    // TODO use config\n\n    // Construct a pattern which matches the rust implementation:\n    // https://github.com/huggingface/tokenizers/blob/b4fcc9ce6e4ad5806e82826f816acfdfdc4fcc67/tokenizers/src/pre_tokenizers/bert.rs#L11\n    // Equivalent to removing whitespace and splitting on punctuation (both \\p{P} and other ascii characters)\n    const punctuation = '\\\\p{P}\\\\u0021-\\\\u002F\\\\u003A-\\\\u0040\\\\u005B-\\\\u0060\\\\u007B-\\\\u007E';\n    this.pattern = new RegExp(`[^\\\\s${punctuation}]+|[${punctuation}]`, 'gu');\n  }\n  /**\n   * Tokenizes a single text using the BERT pre-tokenization scheme.\n   * \n   * @param {string} text The text to tokenize.\n   * @returns {Array<string>} An array of tokens.\n   */\n  pre_tokenize_text(text) {\n    return text.trim().match(this.pattern) || [];\n  }\n}\n\n/**\n * A pre-tokenizer that splits text into Byte-Pair-Encoding (BPE) subwords.\n * @extends PreTokenizer\n */\nclass ByteLevelPreTokenizer extends PreTokenizer {\n  /**\n   * Creates a new instance of the `ByteLevelPreTokenizer` class.\n   * @param {Object} config The configuration object.\n   */\n  constructor(config) {\n    super();\n    this.config = config;\n\n    /**\n     * @type {boolean} Whether to add a leading space to the first word.\n     * This allows to treat the leading word just as any other word.\n     */\n    this.add_prefix_space = this.config.add_prefix_space;\n\n    /**\n     * @type {boolean} Whether the post processing step should trim offsets\n     * to avoid including whitespaces.\n     * @todo Use this in the pretokenization step.\n     */\n    this.trim_offsets = this.config.trim_offsets;\n\n    /**\n     * @type {boolean} Whether to use the standard GPT2 regex for whitespace splitting.\n     * Set it to False if you want to use your own splitting. Defaults to true.\n     */\n    this.use_regex = this.config.use_regex ?? true;\n    this.pattern = /'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+/gu;\n    this.byte_encoder = BYTES_TO_UNICODE;\n    this.text_encoder = new TextEncoder();\n  }\n\n  /**\n   * Tokenizes a single piece of text using byte-level tokenization.\n   * @param {string} text The text to tokenize.\n   * @returns {string[]} An array of tokens.\n   */\n  pre_tokenize_text(text) {\n    // Split on whitespace and punctuation\n    let tokens = this.use_regex ? text.match(this.pattern) || [] : [text];\n    return tokens.map(token => {\n      if (this.add_prefix_space && !token.startsWith(' ')) {\n        token = ' ' + token;\n      }\n\n      // Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n      token = Array.from(this.text_encoder.encode(token), byte => this.byte_encoder[byte]).join('');\n      return token;\n    });\n  }\n}\n\n/**\n * Splits text using a given pattern.\n * @extends PreTokenizer\n */\nclass SplitPreTokenizer extends PreTokenizer {\n  /**\n   * @param {Object} config The configuration options for the pre-tokenizer.\n   * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.\n   * @param {string|undefined} config.pattern.String The string to use for splitting. Only defined if the pattern is a string.\n   * @param {string|undefined} config.pattern.Regex The regex to use for splitting. Only defined if the pattern is a regex.\n   * @param {'isolated'|'removed'} config.behavior The behavior to use when splitting.\n   */\n  constructor(config) {\n    super();\n    this.config = config;\n  }\n\n  /**\n   * Tokenizes text by splitting it using the given pattern.\n   * @param {string} text The text to tokenize.\n   * @returns {string[]} An array of tokens.\n   */\n  pre_tokenize_text(text) {\n    let pattern = createPattern(this.config.pattern);\n    if (pattern === null) {\n      return [];\n    }\n    switch (this.config.behavior.toLowerCase()) {\n      // TODO add merged_with_previous, merged_with_next, contiguous\n      // TODO these should act slightly differently. Currently, we haven't found a tokenizer which produces different results.\n      case 'isolated':\n      case 'removed':\n        return text.match(pattern) || [];\n      default:\n        console.warn(`Unknown split behavior: \"${this.config.behavior}\"`);\n        return [];\n    }\n  }\n}\n\n/**\n * @extends Callable\n */\nclass PostProcessor extends Callable {\n  /**\n   * @param {Object} config The configuration for the post-processor.\n   */\n  constructor(config) {\n    super();\n    this.config = config;\n  }\n\n  /**\n   * Factory method to create a PostProcessor object from a configuration object.\n   *\n   * @param {Object} config Configuration object representing a PostProcessor.\n   * @returns {PostProcessor} A PostProcessor object created from the given configuration.\n   * @throws {Error} If an unknown PostProcessor type is encountered.\n   */\n  static fromConfig(config) {\n    switch (config.type) {\n      case 'TemplateProcessing':\n        return new TemplateProcessing(config);\n      case 'ByteLevel':\n        return new ByteLevelPostProcessor(config);\n      case 'RobertaProcessing':\n        return new RobertaProcessing(config);\n      default:\n        throw new Error(`Unknown PostProcessor type: ${config.type}`);\n    }\n  }\n\n  /**\n   * Method to be implemented in subclass to apply post-processing on the given tokens.\n   *\n   * @param {Array} tokens The input tokens to be post-processed.\n   * @param {...*} args Additional arguments required by the post-processing logic.\n   * @returns {Array} The post-processed tokens.\n   * @throws {Error} If the method is not implemented in subclass.\n   */\n  post_process(tokens) {\n    throw Error(\"post_process should be implemented in subclass.\");\n  }\n\n  /**\n   * Alias for {@link PostProcessor#post_process}.\n   * @param {Array} tokens The text or array of texts to post-process.\n   * @param {...*} args Additional arguments required by the post-processing logic.\n   * @returns {Array} An array of post-processed tokens.\n   */\n  _call(tokens) {\n    for (var _len2 = arguments.length, args = new Array(_len2 > 1 ? _len2 - 1 : 0), _key2 = 1; _key2 < _len2; _key2++) {\n      args[_key2 - 1] = arguments[_key2];\n    }\n    return this.post_process(tokens, ...args);\n  }\n}\n\n/**\n * A post-processor that adds special tokens to the beginning and end of the input.\n * @extends PostProcessor\n */\nclass RobertaProcessing extends PostProcessor {\n  /**\n   * @param {Object} config The configuration for the post-processor.\n   * @param {string[]} config.cls The special tokens to add to the beginning of the input.\n   * @param {string[]} config.sep The special tokens to add to the end of the input.\n   */\n  constructor(config) {\n    super(config);\n    // TODO use all of config: add_prefix_space, trim_offsets\n\n    this.cls = config.cls[0];\n    this.sep = config.sep[0];\n  }\n\n  /**\n   * Adds the special tokens to the beginning and end of the input.\n   * @param {string[]} tokens The input tokens.\n   * @param {string[]|null} tokens_pair An optional second set of input tokens.\n   * @returns {string[]} The input tokens with the special tokens added to the beginning and end.\n   */\n  post_process(tokens) {\n    let tokens_pair = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n    tokens = mergeArrays([this.cls], tokens, [this.sep]);\n\n    // NOTE: It is intended to add 2 EOS tokens after the first set of tokens\n    // https://github.com/huggingface/tokenizers/issues/983\n    if (tokens_pair !== null) {\n      tokens = mergeArrays(tokens, [this.sep], tokens_pair, [this.sep]);\n    }\n    return tokens;\n  }\n}\n\n/**\n * Post processor that replaces special tokens in a template with actual tokens.\n * @extends PostProcessor\n */\nclass TemplateProcessing extends PostProcessor {\n  /**\n   * Creates a new instance of `TemplateProcessing`.\n   * @param {Object} config The configuration options for the post processor.\n   * @param {Array} config.single The template for a single sequence of tokens.\n   * @param {Array} config.pair The template for a pair of sequences of tokens.\n   */\n  constructor(config) {\n    super(config);\n    this.single = config.single;\n    this.pair = config.pair;\n  }\n\n  /**\n   * Replaces special tokens in the template with actual tokens.\n   * @param {Array} tokens The list of tokens for the first sequence.\n   * @param {Array} [tokens_pair=null] The list of tokens for the second sequence (optional).\n   * @returns {Array} The list of tokens with the special tokens replaced with actual tokens.\n   */\n  post_process(tokens) {\n    let tokens_pair = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n    let type = tokens_pair === null ? this.single : this.pair;\n    let toReturn = [];\n    for (let item of type) {\n      if ('SpecialToken' in item) {\n        toReturn.push(item.SpecialToken.id);\n      } else if ('Sequence' in item) {\n        if (item.Sequence.id === 'A') {\n          toReturn = mergeArrays(toReturn, tokens);\n        } else if (item.Sequence.id === 'B') {\n          toReturn = mergeArrays(toReturn, tokens_pair);\n        }\n      }\n    }\n    return toReturn;\n  }\n}\n\n/**\n * A PostProcessor that returns the given tokens as is.\n * @extends PostProcessor\n */\nclass ByteLevelPostProcessor extends PostProcessor {\n  /**\n   * Post process the given tokens.\n   * @param {string[]} tokens The tokens to be post processed.\n   * @returns {string[]} The post processed tokens.\n   */\n  post_process(tokens) {\n    return tokens;\n  }\n}\n\n/**\n * The base class for token decoders.\n * @extends Callable\n */\nclass Decoder extends Callable {\n  /**\n  * Creates an instance of `Decoder`.\n  *\n  * @param {Object} config The configuration object.\n  */\n  constructor(config) {\n    super();\n    this.config = config;\n    this.added_tokens = [];\n    this.end_of_word_suffix = null;\n    this.trim_offsets = config.trim_offsets;\n  }\n\n  /**\n  * Creates a decoder instance based on the provided configuration.\n  *\n  * @param {Object} config The configuration object.\n  * @returns {Decoder} A decoder instance.\n  * @throws {Error} If an unknown decoder type is provided.\n  */\n  static fromConfig(config) {\n    switch (config.type) {\n      case 'WordPiece':\n        return new WordPieceDecoder(config);\n      case 'Metaspace':\n        return new MetaspaceDecoder(config);\n      case 'ByteLevel':\n        return new ByteLevelDecoder(config);\n      case 'Replace':\n        return new ReplaceDecoder(config);\n      case 'ByteFallback':\n        return new ByteFallback(config);\n      case 'Fuse':\n        return new FuseDecoder(config);\n      case 'Strip':\n        return new StripDecoder(config);\n      case 'Sequence':\n        return new DecoderSequence(config);\n      default:\n        throw new Error(`Unknown Decoder type: ${config.type}`);\n    }\n  }\n\n  /**\n  * Calls the `decode` method.\n  *\n  * @param {string[]} tokens The list of tokens.\n  * @returns {string} The decoded string.\n  */\n  _call(tokens) {\n    return this.decode(tokens);\n  }\n\n  /**\n  * Decodes a list of tokens.\n  * @param {string[]} tokens The list of tokens.\n  * @returns {string} The decoded string.\n  */\n  decode(tokens) {\n    return this.decode_chain(tokens).join('');\n  }\n\n  /**\n   * Apply the decoder to a list of tokens.\n   * \n   * @param {string[]} tokens The list of tokens.\n   * @returns {string[]} The decoded list of tokens.\n   * @throws {Error} If the `decode_chain` method is not implemented in the subclass.\n   */\n  decode_chain(tokens) {\n    throw Error(\"`decode_chain` should be implemented in subclass.\");\n  }\n}\nclass ReplaceDecoder extends Decoder {\n  constructor(config) {\n    super(config);\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    let pattern = createPattern(this.config.pattern);\n    if (pattern === null) {\n      return tokens;\n    }\n    return tokens.map(token => token.replaceAll(pattern, this.config.content));\n  }\n}\nclass ByteFallback extends Decoder {\n  constructor(config) {\n    super(config);\n    this.text_decoder = new TextDecoder();\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    let new_tokens = [];\n    let previous_byte_tokens = [];\n    for (let token of tokens) {\n      let bytes = null;\n      if (token.length === 6 && token.startsWith('<0x') && token.endsWith('>')) {\n        let byte = parseInt(token.slice(3, 5), 16);\n        if (!isNaN(byte)) {\n          bytes = byte;\n        }\n      }\n      if (bytes !== null) {\n        previous_byte_tokens.push(bytes);\n      } else {\n        if (previous_byte_tokens.length > 0) {\n          let string = this.text_decoder.decode(Uint8Array.from(previous_byte_tokens));\n          new_tokens.push(string);\n          previous_byte_tokens = [];\n        }\n        new_tokens.push(token);\n      }\n    }\n    if (previous_byte_tokens.length > 0) {\n      let string = this.text_decoder.decode(Uint8Array.from(previous_byte_tokens));\n      new_tokens.push(string);\n      previous_byte_tokens = [];\n    }\n    return new_tokens;\n  }\n}\n\n/**\n * Fuse simply fuses all tokens into one big string.\n * It's usually the last decoding step anyway, but this decoder\n * exists incase some decoders need to happen after that step\n */\nclass FuseDecoder extends Decoder {\n  constructor(config) {\n    super(config);\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    return [tokens.join('')];\n  }\n}\nclass StripDecoder extends Decoder {\n  constructor(config) {\n    super(config);\n    this.content = this.config.content;\n    this.start = this.config.start;\n    this.stop = this.config.stop;\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    return tokens.map(token => {\n      let start_cut = 0;\n      for (let i = 0; i < this.start; ++i) {\n        if (token[i] === this.content) {\n          start_cut = i + 1;\n          continue;\n        } else {\n          break;\n        }\n      }\n      let stop_cut = token.length;\n      for (let i = 0; i < this.stop; ++i) {\n        const index = token.length - i - 1;\n        if (token[index] === this.content) {\n          stop_cut = index;\n          continue;\n        } else {\n          break;\n        }\n      }\n      return token.slice(start_cut, stop_cut);\n    });\n  }\n}\n\n/**\n * A decoder that decodes a list of WordPiece tokens into a single string.\n * @extends Decoder\n */\nclass WordPieceDecoder extends Decoder {\n  /**\n   * Creates a new instance of WordPieceDecoder.\n   * @param {Object} config The configuration object.\n   * @param {string} config.prefix The prefix used for WordPiece encoding.\n   * @param {boolean} config.cleanup Whether to cleanup the decoded string.\n   */\n  constructor(config) {\n    super(config);\n    this.cleanup = config.cleanup;\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    return tokens.map((token, i) => {\n      if (i !== 0) {\n        if (token.startsWith(this.config.prefix)) {\n          // NOTE: .replace() is intended; only replace first occurrence\n          token = token.replace(this.config.prefix, '');\n        } else {\n          token = ' ' + token;\n        }\n      }\n      if (this.cleanup) {\n        token = clean_up_tokenization(token);\n      }\n      return token;\n    });\n  }\n}\n\n/**\n * Byte-level decoder for tokenization output. Inherits from the `Decoder` class.\n * @extends Decoder\n */\nclass ByteLevelDecoder extends Decoder {\n  /**\n   * Create a `ByteLevelDecoder` object.\n   * @param {Object} config Configuration object.\n   */\n  constructor(config) {\n    super(config);\n    this.byte_decoder = UNICODE_TO_BYTES;\n    this.text_decoder = new TextDecoder(\"utf-8\", {\n      fatal: false,\n      ignoreBOM: true\n    });\n    this.end_of_word_suffix = null;\n  }\n\n  /**\n   * Convert an array of tokens to string by decoding each byte.\n   * @param {string[]} tokens Array of tokens to be decoded.\n   * @returns {string} The decoded string.\n   */\n  convert_tokens_to_string(tokens) {\n    let text = tokens.join('');\n    let byteArray = new Uint8Array([...text].map(c => this.byte_decoder[c]));\n    let decoded_text = this.text_decoder.decode(byteArray);\n    return decoded_text;\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    // TODO move to base class (like HF)\n    // tokens === filtered_tokens\n\n    // To avoid mixing byte-level and unicode for byte-level BPT\n    // we need to build string separately for added tokens and byte-level tokens\n    // cf. https://github.com/huggingface/transformers/issues/1133\n    let sub_texts = [];\n    let current_sub_text = [];\n    for (let token of tokens) {\n      // tokens sent here are already filtered, so we don't need to do this\n      // if (skip_special_tokens && this.all_special_ids.includes(token)) {\n      //     continue;\n      // }\n\n      if (this.added_tokens.includes(token)) {\n        if (current_sub_text.length > 0) {\n          sub_texts.push(this.convert_tokens_to_string(current_sub_text));\n          current_sub_text = [];\n        }\n        sub_texts.push(token);\n      } else {\n        current_sub_text.push(token);\n      }\n    }\n    if (current_sub_text.length > 0) {\n      sub_texts.push(this.convert_tokens_to_string(current_sub_text));\n    }\n\n    // TODO add spaces_between_special_tokens and clean_up_tokenization_spaces options\n\n    return sub_texts;\n  }\n}\n\n/**\n * Apply a sequence of decoders.\n * @extends Decoder\n */\nclass DecoderSequence extends Decoder {\n  /**\n   * Creates a new instance of DecoderSequence.\n   * @param {Object} config The configuration object.\n   * @param {Decoder[]} config.decoders The list of decoders to apply.\n   */\n  constructor(config) {\n    super(config);\n    this.decoders = config.decoders.map(x => Decoder.fromConfig(x));\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    // Use reduce to apply each decoder to the tokens\n    return this.decoders.reduce((toks, decoder) => {\n      return decoder.decode_chain(toks);\n    }, tokens);\n  }\n}\n\n/**\n * This PreTokenizer replaces spaces with the given replacement character, adds a prefix space if requested,\n * and returns a list of tokens.\n * @extends PreTokenizer\n */\nclass MetaspacePreTokenizer extends PreTokenizer {\n  /**\n   * @param {Object} config The configuration object for the MetaspacePreTokenizer.\n   * @param {boolean} config.add_prefix_space Whether to add a prefix space to the first token.\n   * @param {string} config.replacement The character to replace spaces with.\n   * @param {string} [config.str_rep=config.replacement] An optional string representation of the replacement character.\n   */\n  constructor(config) {\n    super();\n    this.addPrefixSpace = config.add_prefix_space;\n    this.replacement = config.replacement;\n    this.strRep = config.str_rep || this.replacement;\n  }\n\n  /**\n   * This method takes a list of normalized tokens, replaces spaces with the replacement character,\n   * adds a prefix space if requested, and returns a new list of tokens.\n   * @param {string[]|string} normalizedTokens The list of normalized tokens to pre-tokenize.\n   * @returns {string[]} A new list of pre-tokenized tokens.\n   */\n  pre_tokenize(normalizedTokens) {\n    if (typeof normalizedTokens === 'string') {\n      // Metaspace acts on a list of tokens. If passing in a string, first split on whitespace\n      // NOTE: For some reason, metaspace includes trailing whitespace, so we only trim leading whitespace.\n      // See: https://github.com/huggingface/tokenizers/issues/1250\n      normalizedTokens = normalizedTokens.trimStart().split(/\\s+/);\n    }\n    const result = [];\n    for (let token of normalizedTokens) {\n      let normalized = token.replaceAll(' ', this.strRep);\n      if (this.addPrefixSpace && !normalized.startsWith(this.replacement)) {\n        normalized = this.strRep + normalized;\n      }\n      result.push(normalized);\n    }\n    return result;\n  }\n}\n\n/**\n * MetaspaceDecoder class extends the Decoder class and decodes Metaspace tokenization.\n * @extends Decoder\n */\nclass MetaspaceDecoder extends Decoder {\n  /**\n   * Constructs a new MetaspaceDecoder object.\n   * @param {Object} config The configuration object for the MetaspaceDecoder.\n   * @param {boolean} config.add_prefix_space Whether to add a prefix space to the decoded string.\n   * @param {string} config.replacement The string to replace spaces with.\n   */\n  constructor(config) {\n    super(config);\n    this.addPrefixSpace = config.add_prefix_space;\n    this.replacement = config.replacement;\n  }\n\n  /** @type {Decoder['decode_chain']} */\n  decode_chain(tokens) {\n    let result = [];\n    for (let i = 0; i < tokens.length; ++i) {\n      let normalized = tokens[i].replaceAll(this.replacement, ' ');\n      if (this.addPrefixSpace && i == 0 && normalized.startsWith(' ')) {\n        normalized = normalized.substring(1);\n      }\n      result.push(normalized);\n    }\n    return result;\n  }\n}\n\n/**\n * A normalizer that applies a precompiled charsmap.\n * This is useful for applying complex normalizations in C++ and exposing them to JavaScript.\n * @extends Normalizer\n * @param {Object} config The configuration object for the Precompiled normalizer.\n * @param {Object} config.precompiled_charsmap The precompiled charsmap object.\n */\nclass Precompiled extends Normalizer {\n  /**\n   * Create a new instance of Precompiled normalizer.\n   * @param {Object} config The configuration object.\n   * @param {any} config.precompiled_charsmap Precompiled chars mapping.\n   */\n  constructor(config) {\n    super(config);\n    this.charsmap = config.precompiled_charsmap;\n  }\n\n  /**\n   * Normalizes the given text by applying the precompiled charsmap.\n   * @param {string} text The text to normalize.\n   * @returns {string} The normalized text.\n   */\n  normalize(text) {\n    // TODO use this.charsmap\n    return text;\n  }\n}\n\n/**\n * A pre-tokenizer that applies a sequence of pre-tokenizers to the input text.\n * @extends PreTokenizer\n */\nclass PreTokenizerSequence extends PreTokenizer {\n  /**\n   * Creates an instance of PreTokenizerSequence.\n   * @param {Object} config The configuration object for the pre-tokenizer sequence.\n   * @param {Object[]} config.pretokenizers An array of pre-tokenizer configurations.\n   */\n  constructor(config) {\n    super();\n    this.tokenizers = config.pretokenizers.map(x => PreTokenizer.fromConfig(x));\n  }\n\n  /**\n   * Applies each pre-tokenizer in the sequence to the input text in turn.\n   * @param {string|string[]} text The text(s) to pre-tokenize.\n   * @returns {string[]} The pre-tokenized text.\n   */\n  pre_tokenize_text(text) {\n    if (typeof text === 'string') {\n      text = [text];\n    }\n    // Use reduce to apply each tokenizer to the text\n    return this.tokenizers.reduce((preTokenizedText, tokenizer) => {\n      return tokenizer.pre_tokenize(preTokenizedText);\n    }, text);\n  }\n}\n\n/**\n * Splits a string of text by whitespace characters into individual tokens.\n * @extends PreTokenizer\n */\nclass WhitespaceSplit extends PreTokenizer {\n  /**\n   * Creates an instance of WhitespaceSplit.\n   * @param {Object} config The configuration object for the pre-tokenizer sequence.\n   */\n  constructor(config) {\n    super();\n  }\n  /**\n   * Pre-tokenizes the input text by splitting it on whitespace characters.\n   * @param {string} text The text to be pre-tokenized.\n   * @returns {string[]} An array of tokens produced by splitting the input text on whitespace.\n   */\n  pre_tokenize_text(text) {\n    return whitespace_split(text);\n  }\n}\nexport class PreTrainedTokenizer extends Callable {\n  /**\n   * Create a new PreTrainedTokenizer instance.\n   * @param {Object} tokenizerJSON The JSON of the tokenizer.\n   * @param {Object} tokenizerConfig The config of the tokenizer.\n   */\n  constructor(tokenizerJSON, tokenizerConfig) {\n    super();\n\n    // Construct parts of the tokenizer from the JSON\n    this.normalizer = Normalizer.fromConfig(tokenizerJSON.normalizer);\n    this.pre_tokenizer = PreTokenizer.fromConfig(tokenizerJSON.pre_tokenizer);\n\n    // Convert the vocabulary to a map, if it exists\n    if (tokenizerJSON.model.vocab) {\n      if (!Array.isArray(tokenizerJSON.model.vocab)) {\n        tokenizerJSON.model.vocab = Object.entries(tokenizerJSON.model.vocab);\n      }\n      tokenizerJSON.model.vocab = new Map(tokenizerJSON.model.vocab);\n    }\n    this.model = TokenizerModel.fromConfig(tokenizerJSON.model, tokenizerConfig);\n    this.post_processor = PostProcessor.fromConfig(tokenizerJSON.post_processor);\n\n    // TODO: maybe, allow this to be null; in which case, we use model as decoder too?\n    this.decoder = Decoder.fromConfig(tokenizerJSON.decoder);\n\n    // Another slight hack to add `end_of_word_suffix` (if present) to the decoder\n    // This is needed for cases where BPE model and ByteLevel decoder are used\n    // For more information, see https://github.com/xenova/transformers.js/issues/74\n    // TODO: save this to the decoder when exporting?\n    this.decoder.end_of_word_suffix = this.model.end_of_word_suffix;\n\n    // Add added_tokens to model\n    this.special_tokens = [];\n    this.all_special_ids = [];\n    this.added_tokens = [];\n    for (let addedToken of tokenizerJSON.added_tokens) {\n      let id = addedToken.id;\n      let content = addedToken.content;\n      this.added_tokens.push(content);\n      this.model.tokens_to_ids.set(content, id);\n      this.model.vocab[id] = content;\n      if (addedToken.special) {\n        this.special_tokens.push(content);\n        this.all_special_ids.push(id);\n      }\n    }\n\n    // Slight hack, but it prevents code duplication:\n    this.decoder.added_tokens = this.added_tokens;\n    this.added_tokens_regex = new RegExp('(' + this.added_tokens.map(escapeRegExp).join('|') + ')');\n\n    // Set mask token if present (otherwise will be undefined, which is fine)\n    this.mask_token = this.getToken(tokenizerConfig, 'mask_token');\n    this.mask_token_id = this.model.tokens_to_ids.get(this.mask_token);\n    this.pad_token = this.getToken(tokenizerConfig, 'pad_token', 'eos_token');\n    this.pad_token_id = this.model.tokens_to_ids.get(this.pad_token);\n    this.sep_token = this.getToken(tokenizerConfig, 'sep_token');\n    this.sep_token_id = this.model.tokens_to_ids.get(this.sep_token);\n    this.model_max_length = tokenizerConfig.model_max_length;\n\n    /** @type {boolean} Whether or not to strip the text when tokenizing (removing excess spaces before and after the string). */\n    this.remove_space = tokenizerConfig.remove_space;\n    this.clean_up_tokenization_spaces = tokenizerConfig.clean_up_tokenization_spaces ?? true;\n\n    // TODO allow user to change this\n    this.padding_side = 'right';\n  }\n\n  /**\n   * Returns the value of the first matching key in the tokenizer config object.\n   * @param {...string} keys One or more keys to search for in the tokenizer config object.\n   * @returns {string|null} The value associated with the first matching key, or null if no match is found.\n   * @throws {Error} If an object is found for a matching key and its __type property is not \"AddedToken\".\n   */\n  getToken(tokenizerConfig) {\n    for (var _len3 = arguments.length, keys = new Array(_len3 > 1 ? _len3 - 1 : 0), _key3 = 1; _key3 < _len3; _key3++) {\n      keys[_key3 - 1] = arguments[_key3];\n    }\n    for (let key of keys) {\n      let item = tokenizerConfig[key];\n      if (!item) continue;\n      if (typeof item === 'object') {\n        if (item.__type === 'AddedToken') {\n          return item.content;\n        } else {\n          throw Error(`Unknown token: ${item}`);\n        }\n      } else {\n        return item;\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`. \n   * \n   * @param {string} pretrained_model_name_or_path The path to the pre-trained tokenizer.\n   * @param {PretrainedOptions} options Additional options for loading the tokenizer.\n   * \n   * @throws {Error} Throws an error if the tokenizer.json or tokenizer_config.json files are not found in the `pretrained_model_name_or_path`.\n   * @returns {Promise<PreTrainedTokenizer>} A new instance of the `PreTrainedTokenizer` class.\n   */\n  static async from_pretrained(pretrained_model_name_or_path) {\n    let {\n      progress_callback = null,\n      config = null,\n      cache_dir = null,\n      local_files_only = false,\n      revision = 'main'\n    } = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let info = await loadTokenizer(pretrained_model_name_or_path, {\n      progress_callback,\n      config,\n      cache_dir,\n      local_files_only,\n      revision\n    });\n\n    // @ts-ignore\n    return new this(...info);\n  }\n\n  /**\n   * This function can be overridden by a subclass to apply additional preprocessing\n   * to a model's input data.\n   * @param {Object} inputs An object containing input data as properties.\n   * @returns {Object} The modified inputs object.\n   */\n  prepare_model_inputs(inputs) {\n    return inputs;\n  }\n\n  /**\n   * Encode/tokenize the given text(s).\n   * @param {string|string[]} text The text to tokenize.\n   * @param {Object} options An optional object containing the following properties:\n   * @param {string|string[]} [options.text_pair=null] Optional second sequence to be encoded. If set, must be the same type as text.\n   * @param {boolean} [options.padding=false] Whether to pad the input sequences.\n   * @param {boolean} [options.truncation=null] Whether to truncate the input sequences.\n   * @param {number} [options.max_length=null] Maximum length of the returned list and optionally padding length.\n   * @param {boolean} [options.return_tensor=true] Whether to return the results as Tensors or arrays.\n   * @returns {{ input_ids: number[]|number[][]|Tensor, attention_mask: any[]|Tensor }} Object to be passed to the model.\n   */\n  _call(\n  // Required positional arguments\n  text) {\n    let {\n      text_pair = null,\n      // add_special_tokens = true, // TODO\n      padding = false,\n      truncation = null,\n      max_length = null,\n      return_tensor = true // Different to HF\n    } = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    /** @type {number[]|number[][]|Tensor} */\n    let tokens;\n    if (Array.isArray(text)) {\n      if (text.length === 0) {\n        throw Error('text array must be non-empty');\n      }\n      if (text_pair !== null) {\n        if (!Array.isArray(text_pair)) {\n          throw Error('text_pair must also be an array');\n        } else if (text.length !== text_pair.length) {\n          throw Error('text and text_pair must have the same length');\n        }\n        tokens = text.map((t, i) => this.encode(t, text_pair[i]));\n      } else {\n        tokens = text.map(x => this.encode(x));\n      }\n    } else {\n      if (text === null) {\n        throw Error('text may not be null');\n      }\n      if (Array.isArray(text_pair)) {\n        throw Error('When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).');\n      }\n\n      // For single input, we just wrap in an array, and then unwrap later.\n      tokens = [this.encode(text, text_pair)];\n    }\n    // At this point, tokens is batched: [batch_size, tokens]\n    // However, array may be jagged. So, we pad to max_length\n\n    let maxLengthOfBatch = max(tokens.map(x => x.length))[0];\n\n    // If null, we calculate max length from sequences\n    if (max_length === null) {\n      max_length = maxLengthOfBatch;\n    }\n\n    // Ensure it is less than model max length\n    max_length = Math.min(max_length, this.model_max_length);\n\n    /** @type {any[]|Tensor} */\n    let attention_mask = [];\n    if (padding || truncation) {\n      // Perform padding and/or truncation\n      for (let i = 0; i < tokens.length; ++i) {\n        if (tokens[i].length === max_length) {\n          attention_mask.push(new Array(tokens[i].length).fill(1));\n          continue;\n        } else if (tokens[i].length > max_length) {\n          // possibly truncate\n          if (truncation) {\n            tokens[i] = tokens[i].slice(0, max_length);\n          }\n          attention_mask.push(new Array(tokens[i].length).fill(1));\n        } else {\n          // t.length < max_length\n          if (padding) {\n            let diff = max_length - tokens[i].length;\n            if (this.padding_side === 'right') {\n              attention_mask.push(new Array(tokens[i].length).fill(1).concat(new Array(diff).fill(0)));\n              tokens[i].push(...new Array(diff).fill(this.pad_token_id));\n            } else {\n              // left\n              attention_mask.push(new Array(diff).fill(0).concat(new Array(tokens[i].length).fill(1)));\n              tokens[i].unshift(...new Array(diff).fill(this.pad_token_id));\n            }\n          } else {\n            attention_mask.push(new Array(tokens[i].length).fill(1));\n          }\n        }\n      }\n    } else {\n      attention_mask = tokens.map(x => new Array(x.length).fill(1));\n    }\n    if (return_tensor) {\n      if (!(padding && truncation)) {\n        // Not, guaranteed that all items have same length, so\n        // we perform additional check\n\n        if (tokens.some(x => x.length !== tokens[0].length)) {\n          throw Error(\"Unable to create tensor, you should probably activate truncation and/or padding \" + \"with 'padding=true' and 'truncation=true' to have batched tensors with the same length.\");\n        }\n      }\n\n      // Now we actually convert to tensor\n      // NOTE: In the same way as the python library, we return a batched tensor, regardless of\n      // whether we have a single input or multiple inputs.\n      let dims = [tokens.length, tokens[0].length];\n      tokens = new Tensor('int64', BigInt64Array.from(tokens.flat().map(BigInt)), dims);\n      attention_mask = new Tensor('int64', BigInt64Array.from(attention_mask.flat().map(BigInt)), dims);\n    } else {\n      // If not returning a tensor, we match the input type\n      if (!Array.isArray(text)) {\n        // Input was not batched, so we unwrap\n        tokens = tokens[0];\n        attention_mask = attention_mask[0];\n      }\n    }\n\n    // Finally, add attention mask, and possibly model-specific parameters\n    let modelInputs = {\n      input_ids: tokens,\n      attention_mask: attention_mask\n    };\n\n    // Optional post-processing\n    modelInputs = this.prepare_model_inputs(modelInputs);\n    return modelInputs;\n  }\n\n  /**\n   * Encodes a single text using the preprocessor pipeline of the tokenizer.\n   *\n   * @param {string|null} text The text to encode.\n   * @returns {Array} The encoded tokens.\n   */\n  _encode_text(text) {\n    if (text === null) return null;\n\n    // Actual function which does encoding, for a single text\n    // First, we take care of special tokens. Needed to avoid issues arising from\n    // normalization and/or pretokenization (which may not preserve special tokens)\n    const sections = text.split(this.added_tokens_regex).filter(x => x);\n    let tokens = sections.map(x => {\n      if (this.added_tokens.includes(x)) {\n        // Ignore added tokens\n        return x;\n      } else {\n        if (this.remove_space === true) {\n          x = x.trim().split(/\\s+/).join(' ');\n        }\n        if (this.normalizer !== null) {\n          x = this.normalizer(x);\n        }\n        let sectionTokens = this.pre_tokenizer !== null ? this.pre_tokenizer(x) : [x];\n        let tokens = this.model(sectionTokens);\n        return tokens;\n      }\n    }).flat();\n    return tokens;\n  }\n\n  /**\n   * Encodes a single text or a pair of texts using the model's tokenizer.\n   *\n   * @param {string} text The text to encode.\n   * @param {string|null} text_pair The optional second text to encode.\n   * @returns {number[]} An array of token IDs representing the encoded text(s).\n   */\n  encode(text) {\n    let text_pair = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n    // Function called by users to encode possibly multiple texts\n    let tokens = this._encode_text(text);\n    let tokens2 = this._encode_text(text_pair);\n    let combinedTokens = this.post_processor(tokens, tokens2);\n    let ids = this.model.convert_tokens_to_ids(combinedTokens);\n    return ids;\n  }\n\n  /**\n   * Decode a batch of tokenized sequences.\n   * @param {number[][]} batch List of tokenized input sequences.\n   * @param {Object} decode_args (Optional) Object with decoding arguments.\n   * @returns {string[]} List of decoded sequences.\n   */\n  batch_decode(batch) {\n    let decode_args = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    return batch.map(x => this.decode(x, decode_args));\n  }\n\n  /**\n   * Decodes a sequence of token IDs back to a string.\n   *\n   * @param {number[]} token_ids List of token IDs to decode.\n   * @param {Object} [decode_args={}]\n   * @param {boolean} [decode_args.skip_special_tokens=false] If true, special tokens are removed from the output string.\n   * @param {boolean} [decode_args.clean_up_tokenization_spaces=true] If true, spaces before punctuations and abbreviated forms are removed.\n   *\n   * @returns {string} The decoded string.\n   * @throws {Error} If `token_ids` is not a non-empty array of integers.\n   */\n  decode(token_ids) {\n    let decode_args = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    if (!Array.isArray(token_ids) || token_ids.length === 0 || !isIntegralNumber(token_ids[0])) {\n      throw Error(\"token_ids must be a non-empty array of integers.\");\n    }\n    return this.decode_single(token_ids, decode_args);\n  }\n\n  /**\n   * Decode a single list of token ids to a string.\n   * @param {number[]} token_ids List of token ids to decode\n   * @param {Object} decode_args Optional arguments for decoding\n   * @param {boolean} [decode_args.skip_special_tokens=false] Whether to skip special tokens during decoding\n   * @param {boolean} [decode_args.clean_up_tokenization_spaces=null] Whether to clean up tokenization spaces during decoding.\n   * If null, the value is set to `this.decoder.cleanup` if it exists, falling back to `this.clean_up_tokenization_spaces` if it exists, falling back to `true`.\n   * @returns {string} The decoded string\n   */\n  decode_single(token_ids, _ref) {\n    let {\n      skip_special_tokens = false,\n      clean_up_tokenization_spaces = null\n    } = _ref;\n    let tokens = this.model.convert_ids_to_tokens(token_ids);\n    if (skip_special_tokens) {\n      tokens = tokens.filter(x => !this.special_tokens.includes(x));\n    }\n\n    /** @type {string} */\n    let decoded = this.decoder(tokens);\n\n    // Slight hack, but prevents having to pass `skip_special_tokens` to\n    // each call to `decode`, which would lead to code duplication.\n    if (this.decoder.end_of_word_suffix) {\n      decoded = decoded.replaceAll(this.decoder.end_of_word_suffix, ' ');\n      if (skip_special_tokens) {\n        decoded = decoded.trim();\n      }\n    }\n    if (clean_up_tokenization_spaces ?? this.clean_up_tokenization_spaces) {\n      decoded = clean_up_tokenization(decoded);\n    }\n    return decoded;\n  }\n}\n\n/**\n* Helper method for added `token_type_ids` to model inputs\n* @param {Object} inputs An object containing the input ids and attention mask.\n* @returns {Object} The prepared inputs object.\n*/\nfunction add_token_types(inputs) {\n  if (inputs.input_ids instanceof Tensor) {\n    inputs.token_type_ids = new Tensor('int64', new BigInt64Array(inputs.input_ids.data.length), inputs.input_ids.dims);\n  } else if (Array.isArray(inputs.input_ids)) {\n    if (Array.isArray(inputs.input_ids[0])) {\n      // This means input is batched, so we need to batch the token_type_ids as well\n      inputs.token_type_ids = inputs.input_ids.map(x => new Array(x.length).fill(0));\n    } else {\n      inputs.token_type_ids = new Array(inputs.input_ids.length).fill(0);\n    }\n  } else {\n    throw new Error('Input ids must be a Tensor or an Array');\n  }\n  return inputs;\n}\n\n/**\n * BertTokenizer is a class used to tokenize text for BERT models.\n * @extends PreTrainedTokenizer\n */\nexport class BertTokenizer extends PreTrainedTokenizer {\n  /** @see {@link add_token_types} */\n  prepare_model_inputs(inputs) {\n    return add_token_types(inputs);\n  }\n}\n/**\n * Albert tokenizer\n * @extends PreTrainedTokenizer\n */\nexport class AlbertTokenizer extends PreTrainedTokenizer {\n  /** @see {@link add_token_types} */\n  prepare_model_inputs(inputs) {\n    return add_token_types(inputs);\n  }\n}\nexport class MobileBertTokenizer extends PreTrainedTokenizer {\n  /** @see {@link add_token_types} */\n  prepare_model_inputs(inputs) {\n    return add_token_types(inputs);\n  }\n}\nexport class SqueezeBertTokenizer extends PreTrainedTokenizer {\n  /** @see {@link add_token_types} */\n  prepare_model_inputs(inputs) {\n    return add_token_types(inputs);\n  }\n}\nexport class DistilBertTokenizer extends PreTrainedTokenizer {}\nexport class T5Tokenizer extends PreTrainedTokenizer {}\nexport class GPT2Tokenizer extends PreTrainedTokenizer {}\nexport class BartTokenizer extends PreTrainedTokenizer {}\nexport class RobertaTokenizer extends PreTrainedTokenizer {}\nexport class BloomTokenizer extends PreTrainedTokenizer {}\nexport class LlamaTokenizer extends PreTrainedTokenizer {\n  /** @see {@link add_token_types} */\n  prepare_model_inputs(inputs) {\n    return add_token_types(inputs);\n  }\n}\n/**\n * The NllbTokenizer class is used to tokenize text for NLLB (\"No Language Left Behind\") models.\n * \n * No Language Left Behind (NLLB) is a first-of-its-kind, AI breakthrough project\n * that open-sources models capable of delivering high-quality translations directly\n * between any pair of 200+ languages — including low-resource languages like Asturian,\n * Luganda, Urdu and more. It aims to help people communicate with anyone, anywhere,\n * regardless of their language preferences. For more information, check out their\n * [paper](https://arxiv.org/abs/2207.04672).\n * \n * For a list of supported languages (along with their language codes),\n * @see {@link https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200}\n */\nexport class NllbTokenizer extends PreTrainedTokenizer {\n  constructor(tokenizerJSON, tokenizerConfig) {\n    super(tokenizerJSON, tokenizerConfig);\n    this.languageRegex = /^[a-z]{3}_[A-Z][a-z]{3}$/;\n    this.language_codes = this.special_tokens.filter(x => this.languageRegex.test(x));\n  }\n\n  /**\n   * Helper function to build translation inputs for an `NllbTokenizer`.\n   * @param {string|string[]} raw_inputs The text to tokenize.\n   * @param {Object} tokenizer_options Options to be sent to the tokenizer\n   * @param {Object} generate_kwargs Generation options.\n   * @returns {Object} Object to be passed to the model.\n   */\n  _build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs) {\n    // Check that the target language is valid:\n    if (!this.language_codes.includes(generate_kwargs.tgt_lang)) {\n      throw new Error(`Target language code \"${generate_kwargs.tgt_lang}\" is not valid. Must be one of: {${this.language_codes.join(', ')}}`);\n    }\n\n    // Allow `src_lang` to be optional. If not set, we'll use the tokenizer's default.\n    if (generate_kwargs.src_lang !== undefined) {\n      // Check that the source language is valid:\n      if (!this.language_codes.includes(generate_kwargs.src_lang)) {\n        throw new Error(`Source language code \"${generate_kwargs.src_lang}\" is not valid. Must be one of: {${this.language_codes.join(', ')}}`);\n      }\n\n      // In the same way as the Python library, we override the post-processor\n      // to force the source language to be first:\n      for (let item of this.post_processor.config.single) {\n        if ('SpecialToken' in item && this.languageRegex.test(item.SpecialToken.id)) {\n          item.SpecialToken.id = generate_kwargs.src_lang;\n          break;\n        }\n      }\n    }\n\n    // Override the `forced_bos_token_id` to force the correct language\n    generate_kwargs.forced_bos_token_id = this.model.convert_tokens_to_ids([generate_kwargs.tgt_lang])[0];\n    return this._call(raw_inputs, tokenizer_options);\n  }\n}\nconst WHISPER_LANGUAGES = [[\"en\", \"english\"], [\"zh\", \"chinese\"], [\"de\", \"german\"], [\"es\", \"spanish\"], [\"ru\", \"russian\"], [\"ko\", \"korean\"], [\"fr\", \"french\"], [\"ja\", \"japanese\"], [\"pt\", \"portuguese\"], [\"tr\", \"turkish\"], [\"pl\", \"polish\"], [\"ca\", \"catalan\"], [\"nl\", \"dutch\"], [\"ar\", \"arabic\"], [\"sv\", \"swedish\"], [\"it\", \"italian\"], [\"id\", \"indonesian\"], [\"hi\", \"hindi\"], [\"fi\", \"finnish\"], [\"vi\", \"vietnamese\"], [\"he\", \"hebrew\"], [\"uk\", \"ukrainian\"], [\"el\", \"greek\"], [\"ms\", \"malay\"], [\"cs\", \"czech\"], [\"ro\", \"romanian\"], [\"da\", \"danish\"], [\"hu\", \"hungarian\"], [\"ta\", \"tamil\"], [\"no\", \"norwegian\"], [\"th\", \"thai\"], [\"ur\", \"urdu\"], [\"hr\", \"croatian\"], [\"bg\", \"bulgarian\"], [\"lt\", \"lithuanian\"], [\"la\", \"latin\"], [\"mi\", \"maori\"], [\"ml\", \"malayalam\"], [\"cy\", \"welsh\"], [\"sk\", \"slovak\"], [\"te\", \"telugu\"], [\"fa\", \"persian\"], [\"lv\", \"latvian\"], [\"bn\", \"bengali\"], [\"sr\", \"serbian\"], [\"az\", \"azerbaijani\"], [\"sl\", \"slovenian\"], [\"kn\", \"kannada\"], [\"et\", \"estonian\"], [\"mk\", \"macedonian\"], [\"br\", \"breton\"], [\"eu\", \"basque\"], [\"is\", \"icelandic\"], [\"hy\", \"armenian\"], [\"ne\", \"nepali\"], [\"mn\", \"mongolian\"], [\"bs\", \"bosnian\"], [\"kk\", \"kazakh\"], [\"sq\", \"albanian\"], [\"sw\", \"swahili\"], [\"gl\", \"galician\"], [\"mr\", \"marathi\"], [\"pa\", \"punjabi\"], [\"si\", \"sinhala\"], [\"km\", \"khmer\"], [\"sn\", \"shona\"], [\"yo\", \"yoruba\"], [\"so\", \"somali\"], [\"af\", \"afrikaans\"], [\"oc\", \"occitan\"], [\"ka\", \"georgian\"], [\"be\", \"belarusian\"], [\"tg\", \"tajik\"], [\"sd\", \"sindhi\"], [\"gu\", \"gujarati\"], [\"am\", \"amharic\"], [\"yi\", \"yiddish\"], [\"lo\", \"lao\"], [\"uz\", \"uzbek\"], [\"fo\", \"faroese\"], [\"ht\", \"haitian creole\"], [\"ps\", \"pashto\"], [\"tk\", \"turkmen\"], [\"nn\", \"nynorsk\"], [\"mt\", \"maltese\"], [\"sa\", \"sanskrit\"], [\"lb\", \"luxembourgish\"], [\"my\", \"myanmar\"], [\"bo\", \"tibetan\"], [\"tl\", \"tagalog\"], [\"mg\", \"malagasy\"], [\"as\", \"assamese\"], [\"tt\", \"tatar\"], [\"haw\", \"hawaiian\"], [\"ln\", \"lingala\"], [\"ha\", \"hausa\"], [\"ba\", \"bashkir\"], [\"jw\", \"javanese\"], [\"su\", \"sundanese\"]];\n\n// @ts-ignore\nconst WHISPER_LANGUAGE_MAPPING = new Map(WHISPER_LANGUAGES);\n// @ts-ignore\nconst WHISPER_TO_LANGUAGE_CODE_MAPPING = new Map([...WHISPER_LANGUAGES.map(_ref2 => {\n  let [k, v] = _ref2;\n  return [v, k];\n}), ...[[\"burmese\", \"my\"], [\"valencian\", \"ca\"], [\"flemish\", \"nl\"], [\"haitian\", \"ht\"], [\"letzeburgesch\", \"lb\"], [\"pushto\", \"ps\"], [\"panjabi\", \"pa\"], [\"moldavian\", \"ro\"], [\"moldovan\", \"ro\"], [\"sinhalese\", \"si\"], [\"castilian\", \"es\"]]]);\n\n/**\n * WhisperTokenizer tokenizer\n * @extends PreTrainedTokenizer\n */\nexport class WhisperTokenizer extends PreTrainedTokenizer {\n  /**\n   * Decodes automatic speech recognition (ASR) sequences.\n   * @param {Array<{tokens: number[], stride: number[]}>} sequences The sequences to decode.\n   * @param {Object} options The options to use for decoding.\n   * @returns {Array<string|{chunks?: undefined|Array<{language: string|null, timestamp: Array<number|null>, text: string}>}>} The decoded sequences.\n   */\n  _decode_asr(sequences) {\n    let {\n      return_timestamps = false,\n      return_language = false,\n      time_precision = null,\n      force_full_sequences = true\n    } = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    // Set force_full_sequences=false if you want streaming\n    // TODO add support for `return_language`\n\n    // Internal method meant to only be used by asr pipeline.\n    // Handles all the little quirks specific to whisper to handle\n    // the various options not allowed in other seq2seq models\n\n    // =========== Overview ============\n    // - iterate over all outputs\n    // - all tokens within output\n    // - Each token can be\n    //   - language token\n    //   - special token\n    //   - timestamp token\n    //   - text token\n    // - We accumulate the text tokens.\n    // - We split on end timestamps\n    // - Lots of complexity comes from stride and timestamps\n\n    if (time_precision === null) {\n      throw Error(\"Must specify time_precision\");\n    }\n    let last_language = null;\n    function new_chunk() {\n      return {\n        \"language\": last_language,\n        \"timestamp\": [null, null],\n        \"text\": \"\"\n      };\n    }\n\n    // Welcome to the state machine!\n    const chunks = [];\n    let chunk = new_chunk();\n    let time_offset = 0.0;\n    const timestamp_begin = this.model.convert_tokens_to_ids([\"<|notimestamps|>\"])[0] + 1;\n    let previous_tokens = [];\n    let skip = false;\n    let right_stride_start = null;\n    const all_special_ids = new Set(this.all_special_ids);\n    for (let output of sequences) {\n      // NOTE: python version has batches, so it uses [0]\n      const token_ids = output.tokens;\n\n      // These keep track of timestamps within strides, which need\n      // to be skipped and resolve all tokens in a single chunk.\n      let last_timestamp = null;\n      let first_timestamp = timestamp_begin;\n      if (\"stride\" in output) {\n        const [chunk_len, stride_left, stride_right] = output.stride;\n\n        // Offset the timings to account for the other `model_outputs`.\n        time_offset -= stride_left;\n        right_stride_start = chunk_len - stride_right;\n\n        // Keeping track of timestamps within strides\n        // We're going to NOT split on those, and delay until we're\n        // out of BOTH stride. Otherwise lots of issues occur and\n        // corner cases\n        if (stride_left) {\n          first_timestamp = stride_left / time_precision + timestamp_begin;\n        }\n        if (stride_right) {\n          for (let i = token_ids.length - 1; i >= 0; --i) {\n            const token = token_ids[i];\n            if (token >= timestamp_begin) {\n              // There can be several token in the right stride\n              // But the last one is ALWAYS going to be skipped\n              if (last_timestamp !== null && (token - timestamp_begin) * time_precision < right_stride_start) {\n                break;\n              }\n              last_timestamp = token;\n            }\n          }\n        }\n      }\n      let current_tokens = [];\n\n      // - all tokens within output\n      for (const token of token_ids) {\n        // 4 possible states for each token\n        // - 1/ Language code\n        // - 2/ all other special tokens (which we ignore)\n        // - 3/ Timestamp\n        // - 4/ Regular text\n\n        if (all_special_ids.has(token)) {\n          const text = this.decode([token]);\n          if (text[0] === \"[\" && text[text.length - 1] === \"]\") {\n            const language = WHISPER_LANGUAGE_MAPPING.get(text.slice(1, -1));\n            if (language !== undefined) {\n              // 1/ Indeed some language\n              // TODO Handle when language is different from the previous\n              // one, and we cannot use timestamped tokens to create chunks\n              if (last_language !== null && language !== last_language && !return_timestamps) {\n                previous_tokens.push(current_tokens);\n                const resolved_tokens = this.findLongestCommonSequence(previous_tokens);\n                const resolved_text = this.decode(resolved_tokens);\n                chunk.text = resolved_text;\n                chunks.push(chunk);\n\n                // Flush all our temporary context\n                previous_tokens = [];\n                current_tokens = [];\n                chunk = new_chunk();\n              }\n              last_language = chunk.language = language;\n            } else {\n              // 2/ This is a regular special token, ignoring it\n            }\n          }\n        } else if (token >= timestamp_begin) {\n          // 3/ Timestamp token\n          const time = (token - timestamp_begin) * time_precision + time_offset;\n          const rounded_time = Math.round(time * 100) / 100;\n          if (last_timestamp !== null && token >= last_timestamp) {\n            // Whisper outputted a timestamp token, but it falls within\n            // our stride, so we're going to skip it for the time being\n            // and resolve this later\n            // Skip is necessary because timestamp tokens always come\n            // by pair, so we need to skip the next one too (which would mark the start of another chunk).\n            skip = true;\n          } else if (skip || previous_tokens.length > 0 && token < first_timestamp) {\n            skip = false;\n          } else if (chunk.timestamp[0] === null) {\n            chunk.timestamp[0] = rounded_time;\n          } else {\n            // This is the end of the timestamp chunk\n            if (rounded_time === chunk.timestamp[0]) {\n              // This is a bug in timestamp token output\n              // where we're taking the duplicate token\n              // as a stop where it should be a start.\n              // This is an issue in the underlying model output\n              // Let's just skip it so it becomes\n            } else {\n              chunk.timestamp[1] = rounded_time;\n\n              // Handling merges\n              previous_tokens.push(current_tokens);\n              const resolved_tokens = this.findLongestCommonSequence(previous_tokens);\n              const resolved_text = this.decode(resolved_tokens);\n              chunk.text = resolved_text;\n              chunks.push(chunk);\n\n              // Flush all our temporary context\n              previous_tokens = [];\n              current_tokens = [];\n              chunk = new_chunk();\n            }\n          }\n        } else {\n          // 4/ Regular token\n          // We just append to the list of all tokens so we can handle\n          // merges later and decode into text.\n          current_tokens.push(token);\n        }\n      }\n      if ('stride' in output) {\n        const [chunk_len, stride_left, stride_right] = output.stride;\n        time_offset += chunk_len - stride_right;\n      }\n\n      // Leftover tokens\n      if (current_tokens.length > 0) {\n        previous_tokens.push(current_tokens);\n      } else if (previous_tokens.every(p => p.length === 0)) {\n        // Flushing previous tokens (END)\"\n        chunk = new_chunk();\n        previous_tokens = [];\n        current_tokens = [];\n      }\n    }\n    if (previous_tokens.length > 0) {\n      if (force_full_sequences && return_timestamps) {\n        // Last token should always be timestamps, so there shouldn't be\n        // leftover\n        throw new Error(\"There was an error while processing timestamps, we haven't found a timestamp as last token.\");\n      }\n\n      // Happens when we don't use timestamps\n      const resolved_tokens = this.findLongestCommonSequence(previous_tokens);\n\n      // Flushing previous tokens (FINAL)\n      const resolved_text = this.decode(resolved_tokens);\n      chunk.text = resolved_text;\n      chunks.push(chunk);\n    }\n    let optional = Object.create(null);\n\n    // Preparing and cleaning up the pipeline output\n    const full_text = chunks.map(chunk => chunk.text).join('');\n    if (return_timestamps || return_language) {\n      for (let i = 0; i < chunks.length; ++i) {\n        const chunk = chunks[i];\n        if (!return_timestamps) {\n          delete chunk[\"timestamp\"];\n        }\n        if (!return_language) {\n          delete chunk[\"language\"];\n        }\n      }\n      optional = {\n        \"chunks\": chunks\n      };\n    }\n    return [full_text, optional];\n  }\n\n  /**\n   * Finds the longest common sequence among the provided sequences.\n   * @param {number[][]} sequences An array of sequences of token ids to compare.\n   * @returns {number[]} The longest common sequence found.\n   * @throws {Error} If there is a bug within the function.\n   */\n  findLongestCommonSequence(sequences) {\n    // It would be much harder to do O(n) because of fault tolerance.\n    // We actually have a really good property which is that the total sequence\n    // MUST be those subsequences in order.\n    let leftSequence = sequences[0];\n    let leftLength = leftSequence.length;\n    let totalSequence = [];\n    for (let i = 1; i < sequences.length; ++i) {\n      const rightSequence = sequences[i];\n      let max = 0.0;\n      let maxIndices = [leftLength, leftLength, 0, 0];\n      // Here we're sliding matches\n      // [a, b, c, d]\n      //          [c, d, f]\n      // =        [c] == [d]\n\n      // [a, b, c, d]\n      //       [c, d, f]\n      // =     [c, d] == [c, d]\n\n      // [a, b, c, d]\n      //    [c, d, f]\n\n      // =  [b, c, d] == [c, d, f]\n\n      // [a, b, c, d]\n      // [c, d, f]\n\n      // [a, b, c] == [c, d, f]\n\n      // [a, b, c, d]\n      // [d, f]\n\n      // [a, b] == [d, f]\n\n      // [a, b, c, d]\n      // [f]\n\n      // [a] == [f]\n\n      const rightLength = rightSequence.length;\n      for (let j = 1; j < leftLength + rightLength; ++j) {\n        const eps = j / 10000.0;\n        const leftStart = Math.max(0, leftLength - j);\n        const leftStop = Math.min(leftLength, leftLength + rightLength - j);\n        const left = leftSequence.slice(leftStart, leftStop);\n        const rightStart = Math.max(0, j - leftLength);\n        const rightStop = Math.min(rightLength, j);\n        const right = rightSequence.slice(rightStart, rightStop);\n        if (left.length !== right.length) {\n          throw new Error(\"There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.\");\n        }\n        const matches = left.filter((elem, idx) => elem === right[idx]).length;\n        const matching = matches / j + eps;\n        if (matches > 1 && matching > max) {\n          max = matching;\n          maxIndices = [leftStart, leftStop, rightStart, rightStop];\n        }\n      }\n      const [leftStart, leftStop, rightStart, rightStop] = maxIndices;\n      const leftMid = Math.floor((leftStop + leftStart) / 2);\n      const rightMid = Math.floor((rightStop + rightStart) / 2);\n      totalSequence.push(...leftSequence.slice(0, leftMid));\n      leftSequence = rightSequence.slice(rightMid);\n      leftLength = leftSequence.length;\n    }\n    totalSequence.push(...leftSequence);\n    return totalSequence;\n  }\n\n  /**\n   * Helper function to build translation inputs for a `WhisperTokenizer`,\n   * depending on the language, task, and whether to predict timestamp tokens.\n   * \n   * Used to override the prefix tokens appended to the start of the label sequence.\n   * \n   * **Example: Get ids for a language**\n   * ```javascript\n   * // instantiate the tokenizer and set the prefix token to Spanish\n   * let tokenizer = await WhisperTokenizer.from_pretrained('Xenova/whisper-tiny');\n   * let forced_decoder_ids = tokenizer.get_decoder_prompt_ids({ language: 'spanish' });\n   * // [(1, 50262), (2, 50363)]\n   * ```\n   * \n   * @param {Object} options Options to generate the decoder prompt.\n   * @param {string} [options.language] The language of the transcription text.\n   * The corresponding language id token is appended to the start of the sequence for multilingual\n   * speech recognition and speech translation tasks, e.g. for \"Spanish\" the token \"<|es|>\" is appended\n   * to the start of sequence.\n   * @param {string} [options.task] Task identifier to append at the start of sequence (if any).\n   * This should be used for mulitlingual fine-tuning, with \"transcribe\" for speech recognition and\n   * \"translate\" for speech translation.\n   * @param {boolean} [options.no_timestamps] Whether to add the <|notimestamps|> token at the start of the sequence.\n   * @returns {number[][]} The decoder prompt ids.\n   */\n  get_decoder_prompt_ids() {\n    let {\n      language = null,\n      task = null,\n      no_timestamps = true\n    } = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    // <|lang_id|> <|task|> <|notimestamps|>\n\n    let forced_decoder_ids = [];\n    if (language) {\n      // User wishes to specify the language\n      language = language.toLowerCase();\n\n      // Map to code from user-friendly name (e.g., \"english\" -> \"en\")\n      let language_code = WHISPER_TO_LANGUAGE_CODE_MAPPING.get(language);\n      if (language_code === undefined) {\n        // User provided something that is not a language name\n\n        if (WHISPER_LANGUAGE_MAPPING.has(language)) {\n          // User provided the language code directly (e.g., \"en\")\n          language_code = language;\n        } else {\n          // User provided something that is not a language code or name\n          const is_language_code = language.length === 2;\n          const langs = is_language_code ? WHISPER_LANGUAGE_MAPPING.keys() : WHISPER_LANGUAGE_MAPPING.values();\n          throw new Error(`Language \"${language}\" is not supported. Must be one of: ${JSON.stringify(langs)}`);\n        }\n      }\n      let language_token_id = this.model.tokens_to_ids.get(`<|${language_code}|>`);\n      if (language_token_id === undefined) {\n        throw new Error(`Unable to find language \"${language_code}\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.`);\n      }\n      forced_decoder_ids.push(language_token_id);\n    } else {\n      // No token will be forced, which leaves the model to predict the language\n      forced_decoder_ids.push(null);\n    }\n    if (task) {\n      task = task.toLowerCase();\n      if (task !== 'transcribe' && task !== 'translate') {\n        throw new Error(`Task \"${task}\" is not supported. Must be one of: [\"transcribe\", \"translate\"]`);\n      }\n      let task_token_id = this.model.tokens_to_ids.get(`<|${task}|>`);\n      if (task_token_id === undefined) {\n        throw new Error(`Unable to find task \"${task}\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.`);\n      }\n      forced_decoder_ids.push(task_token_id);\n    } else {\n      // No token will be forced, which leaves the model to predict the task\n      forced_decoder_ids.push(null);\n    }\n    if (no_timestamps) {\n      let no_timestamps_id = this.model.tokens_to_ids.get(`<|notimestamps|>`);\n      if (no_timestamps_id === undefined) {\n        throw new Error('Unable to find \"<|notimestamps|>\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.');\n      }\n      forced_decoder_ids.push(no_timestamps_id);\n    }\n    return forced_decoder_ids.map((x, i) => [i + 1, x]).filter(x => x[1] !== null);\n  }\n}\nexport class CodeGenTokenizer extends PreTrainedTokenizer {}\nexport class CLIPTokenizer extends PreTrainedTokenizer {}\n\n/**\n * @todo This model is not yet supported by Hugging Face's \"fast\" tokenizers library (https://github.com/huggingface/tokenizers).\n * Therefore, this implementation (which is based on fast tokenizers) may produce slightly inaccurate results.\n */\nexport class MarianTokenizer extends PreTrainedTokenizer {\n  /**\n   * Create a new MarianTokenizer instance.\n   * @param {Object} tokenizerJSON The JSON of the tokenizer.\n   * @param {Object} tokenizerConfig The config of the tokenizer.\n   */\n  constructor(tokenizerJSON, tokenizerConfig) {\n    super(tokenizerJSON, tokenizerConfig);\n    this.languageRegex = /^(>>\\w+<<)\\s*/g;\n    this.supported_language_codes = this.model.vocab.filter(x => this.languageRegex.test(x));\n    console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\\'s \"fast\" tokenizers library. Therefore, you may experience slightly inaccurate results.');\n  }\n\n  /**\n   * Encodes a single text. Overriding this method is necessary since the language codes\n   * must be removed before encoding with sentencepiece model.\n   * @see https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213\n   *\n   * @param {string|null} text The text to encode.\n   * @returns {Array} The encoded tokens.\n   */\n  _encode_text(text) {\n    if (text === null) return null;\n\n    // Check if text starts with language code:\n    let [matchInfo, ...remainder] = text.trim().split(this.languageRegex);\n    if (remainder.length === 0) {\n      // No language code, encode normally\n      return super._encode_text(matchInfo);\n    } else if (remainder.length === 2) {\n      // Text starts with language code, so we do not encode it with sentencepiece.\n      let [language, text] = remainder;\n      if (!this.supported_language_codes.includes(language)) {\n        console.warn(`Unsupported language code \"${language}\" detected, which may lead to unexpected behavior. Should be one of: ${JSON.stringify(this.supported_language_codes)}`);\n      }\n      return mergeArrays([language], super._encode_text(text));\n    }\n  }\n}\n\n/**\n * A trie structure to efficiently store and search for strings.\n */\nclass CharTrie {\n  constructor() {\n    this.root = CharTrieNode.default();\n  }\n\n  /**\n   * Adds one or more `texts` to the trie.\n   * @param {string[]} texts The strings to add to the trie.\n   */\n  extend(texts) {\n    for (let text of texts) {\n      this.push(text);\n    }\n  }\n\n  /**\n   * Adds one or more `texts` to the trie.\n   * @param {*} text The strings to add to the trie.\n   */\n  push(text) {\n    let node = this.root;\n    for (let ch of text) {\n      let child = node.children.get(ch);\n      if (child === undefined) {\n        child = CharTrieNode.default();\n        node.children.set(ch, child);\n      }\n      node = child;\n    }\n    node.isLeaf = true;\n  }\n\n  /**\n   * Searches the trie for all strings with a common prefix of `text`.\n   * @param {string} text The common prefix to search for.\n   * @yields {string} Each string in the trie that has `text` as a prefix.\n   */\n  *commonPrefixSearch(text) {\n    let node = this.root;\n    let prefix = \"\";\n    for (let i = 0; i < text.length && node !== undefined; ++i) {\n      const ch = text[i];\n      prefix += ch;\n      node = node.children.get(ch);\n      if (node !== undefined && node.isLeaf) {\n        yield prefix;\n      }\n    }\n  }\n}\n\n/**\n * Represents a node in a character trie.\n * @param {boolean} isLeaf Whether the node is a leaf node or not.\n * @param {Map<string, CharTrieNode>} children A map containing the node's children, where the key is a character and the value is a `CharTrieNode`.\n */\nclass CharTrieNode {\n  constructor(isLeaf, children) {\n    this.isLeaf = isLeaf;\n    this.children = children;\n  }\n\n  /**\n   * Returns a new `CharTrieNode` instance with default values.\n   * @returns {CharTrieNode} A new `CharTrieNode` instance with `isLeaf` set to `false` and an empty `children` map.\n   */\n  static default() {\n    return new CharTrieNode(false, new Map());\n  }\n}\nclass TokenLattice {\n  /**\n   * Creates a new TokenLattice instance.\n   *\n   * @param {string} sentence The input sentence to be tokenized.\n   * @param {number} bosTokenId The beginning-of-sequence token ID.\n   * @param {number} eosTokenId The end-of-sequence token ID.\n   */\n  constructor(sentence, bosTokenId, eosTokenId) {\n    this.sentence = sentence;\n    this.len = sentence.length;\n    this.bosTokenId = bosTokenId;\n    this.eosTokenId = eosTokenId;\n    this.nodes = [];\n    this.beginNodes = new Array(this.len + 1);\n    this.endNodes = new Array(this.len + 1);\n    for (let i = 0; i < this.len + 1; ++i) {\n      this.beginNodes[i] = [];\n      this.endNodes[i] = [];\n    }\n    const bos = new TokenLatticeNode(this.bosTokenId, 0, 0, 0, 0.0);\n    const eos = new TokenLatticeNode(this.eosTokenId, 1, this.len, 0, 0.0);\n    this.nodes.push(bos.clone());\n    this.nodes.push(eos.clone());\n    this.beginNodes[this.len].push(eos);\n    this.endNodes[0].push(bos);\n  }\n\n  /**\n   * Inserts a new token node into the token lattice.\n   *\n   * @param {number} pos The starting position of the token.\n   * @param {number} length The length of the token.\n   * @param {number} score The score of the token.\n   * @param {number} tokenId The token ID of the token.\n   */\n  insert(pos, length, score, tokenId) {\n    const nodeId = this.nodes.length;\n    const node = new TokenLatticeNode(tokenId, nodeId, pos, length, score);\n    this.beginNodes[pos].push(node);\n    this.endNodes[pos + length].push(node);\n    this.nodes.push(node);\n  }\n\n  /**\n   * Implements the Viterbi algorithm to compute the most likely sequence of tokens.\n   *\n   * @returns {TokenLatticeNode[]} The array of nodes representing the most likely sequence of tokens.\n   */\n  viterbi() {\n    const len = this.len;\n    let pos = 0;\n    while (pos <= len) {\n      if (this.beginNodes[pos].length == 0) {\n        return [];\n      }\n      for (let rnode of this.beginNodes[pos]) {\n        rnode.prev = null;\n        let bestScore = 0.0;\n        let bestNode = null;\n        for (let lnode of this.endNodes[pos]) {\n          const score = lnode.backtraceScore + rnode.score;\n          if (bestNode === null || score > bestScore) {\n            bestNode = lnode.clone();\n            bestScore = score;\n          }\n        }\n        if (bestNode !== null) {\n          rnode.prev = bestNode;\n          rnode.backtraceScore = bestScore;\n        } else {\n          return [];\n        }\n      }\n      ++pos;\n    }\n    const results = [];\n    const root = this.beginNodes[len][0];\n    const prev = root.prev;\n    if (prev === null) {\n      return [];\n    }\n    let node = prev.clone();\n    while (node.prev !== null) {\n      results.push(node.clone());\n      const n = node.clone();\n      node = n.prev.clone();\n    }\n    results.reverse();\n    return results;\n  }\n\n  /**\n   * @param {TokenLatticeNode} node\n   * @returns {string} The array of nodes representing the most likely sequence of tokens.\n   */\n  piece(node) {\n    return this.sentence.slice(node.pos, node.pos + node.length);\n  }\n\n  /**\n   * @returns {Array} The array of nodes representing the most likely sequence of tokens.\n   */\n  tokens() {\n    const nodes = this.viterbi();\n    return nodes.map(x => this.piece(x));\n  }\n\n  /**\n   * @returns {Array} The array of nodes representing the most likely sequence of tokens.\n   */\n  tokenIds() {\n    const nodes = this.viterbi();\n    return nodes.map(x => x.tokenId);\n  }\n}\nclass TokenLatticeNode {\n  /**\n   * Represents a node in a token lattice for a given sentence.\n   * @param {number} tokenId The ID of the token associated with this node.\n   * @param {number} nodeId The ID of this node.\n   * @param {number} pos The starting position of the token in the sentence.\n   * @param {number} length The length of the token.\n   * @param {number} score The score associated with the token.\n   */\n  constructor(tokenId, nodeId, pos, length, score) {\n    this.tokenId = tokenId;\n    this.nodeId = nodeId;\n    this.pos = pos;\n    this.length = length;\n    this.score = score;\n    this.prev = null;\n    this.backtraceScore = 0.0;\n  }\n\n  /**\n   * Returns a clone of this node.\n   * @returns {TokenLatticeNode} A clone of this node.\n   */\n  clone() {\n    const n = new TokenLatticeNode(this.tokenId, this.nodeId, this.pos, this.length, this.score);\n    n.prev = this.prev;\n    n.backtraceScore = this.backtraceScore;\n    return n;\n  }\n}\n\n/**\n * Helper class which is used to instantiate pretrained tokenizers with the `from_pretrained` function.\n * The chosen tokenizer class is determined by the type specified in the tokenizer config.\n * \n * @example\n * let tokenizer = await AutoTokenizer.from_pretrained('bert-base-uncased');\n */\nexport class AutoTokenizer {\n  static TOKENIZER_CLASS_MAPPING = {\n    'T5Tokenizer': T5Tokenizer,\n    'DistilBertTokenizer': DistilBertTokenizer,\n    'BertTokenizer': BertTokenizer,\n    'MobileBertTokenizer': MobileBertTokenizer,\n    'SqueezeBertTokenizer': SqueezeBertTokenizer,\n    'AlbertTokenizer': AlbertTokenizer,\n    'GPT2Tokenizer': GPT2Tokenizer,\n    'BartTokenizer': BartTokenizer,\n    'RobertaTokenizer': RobertaTokenizer,\n    'WhisperTokenizer': WhisperTokenizer,\n    'CodeGenTokenizer': CodeGenTokenizer,\n    'CLIPTokenizer': CLIPTokenizer,\n    'MarianTokenizer': MarianTokenizer,\n    'BloomTokenizer': BloomTokenizer,\n    'NllbTokenizer': NllbTokenizer,\n    'LlamaTokenizer': LlamaTokenizer\n  };\n\n  /**\n   * Instantiate one of the tokenizer classes of the library from a pretrained model.\n   * \n   * The tokenizer class to instantiate is selected based on the `tokenizer_class` property of the config object\n   * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n   * \n   * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n   * - A string, the *model id* of a pretrained tokenizer hosted inside a model repo on huggingface.co.\n   *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n   *   user or organization name, like `dbmdz/bert-base-german-cased`.\n   * - A path to a *directory* containing tokenizer files, e.g., `./my_model_directory/`.\n   * @param {PretrainedOptions} options Additional options for loading the tokenizer.\n   * \n   * @returns {Promise<PreTrainedTokenizer>} A new instance of the PreTrainedTokenizer class.\n   */\n  static async from_pretrained(pretrained_model_name_or_path) {\n    let {\n      quantized = true,\n      progress_callback = null,\n      config = null,\n      cache_dir = null,\n      local_files_only = false,\n      revision = 'main'\n    } = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let [tokenizerJSON, tokenizerConfig] = await loadTokenizer(pretrained_model_name_or_path, {\n      quantized,\n      progress_callback,\n      config,\n      cache_dir,\n      local_files_only,\n      revision\n    });\n\n    // Some tokenizers are saved with the \"Fast\" suffix, so we remove that if present.\n    let tokenizerName = tokenizerConfig.tokenizer_class.replace(/Fast$/, '');\n    let cls = this.TOKENIZER_CLASS_MAPPING[tokenizerName];\n    if (!cls) {\n      console.warn(`Unknown tokenizer class \"${tokenizerName}\", attempting to construct from base class.`);\n      cls = PreTrainedTokenizer;\n    }\n    return new cls(tokenizerJSON, tokenizerConfig);\n  }\n}","map":{"version":3,"names":["Callable","reverseDictionary","escapeRegExp","isIntegralNumber","mergeArrays","getModelJSON","max","min","Tensor","loadTokenizer","pretrained_model_name_or_path","options","info","Promise","all","createPattern","pattern","Regex","RegExp","String","console","warn","clean_up_tokenization","text","replace","fuse","arr","value","fused","i","length","push","whitespace_split","match","TokenizerModel","constructor","config","vocab","tokens_to_ids","Map","unk_token_id","undefined","unk_token","end_of_word_suffix","fuse_unk","fromConfig","_len","arguments","args","Array","_key","type","WordPieceTokenizer","Unigram","BPE","Error","_call","tokens","encode","convert_tokens_to_ids","ids","map","t","get","convert_ids_to_tokens","size","key","outputTokens","token","chars","isUnknown","start","subTokens","end","currentSubstring","substr","slice","join","continuing_subword_prefix","has","moreConfig","scores","count","forEach","unk_id","x","bosToken","bosTokenId","eosToken","eos_token","eosTokenId","unkToken","minScore","unkScore","trie","CharTrie","extend","populateNodes","lattice","sentence","len","beginPos","mblen","hasSingleNode","commonPrefixSearch","tokenId","tokenScore","n","insert","tokenize","normalized","TokenLattice","toReturn","tokenized","BYTES_TO_UNICODE","bs","from","charCodeAt","_","cs","b","includes","ccs","fromCharCode","Object","fromEntries","UNICODE_TO_BYTES","bpe_ranks","merges","split","byte_fallback","text_encoder","TextEncoder","cache","create","get_pairs","word","pairs","Set","prev_char","char","add","bpe","bigram","reduce","a","c","Infinity","d","first","second","new_word","j","indexOf","e","final_word","bpe_token_list","toString","toUpperCase","padStart","Normalizer","BertNormalizer","Precompiled","NormalizerSequence","Replace","NFC","NFKD","StripAccents","Lowercase","Prepend","normalize","replaceAll","content","toLowerCase","prepend","normalizers","normalizer","_tokenize_chinese_chars","output","cp","_is_chinese_char","stripAccents","handle_chinese_chars","lowercase","strip_accents","PreTokenizer","BertPreTokenizer","PreTokenizerSequence","WhitespaceSplit","MetaspacePreTokenizer","ByteLevelPreTokenizer","SplitPreTokenizer","pre_tokenize_text","pre_tokenize","result","isArray","flat","punctuation","trim","add_prefix_space","trim_offsets","use_regex","byte_encoder","startsWith","byte","behavior","PostProcessor","TemplateProcessing","ByteLevelPostProcessor","RobertaProcessing","post_process","_len2","_key2","cls","sep","tokens_pair","single","pair","item","SpecialToken","id","Sequence","Decoder","added_tokens","WordPieceDecoder","MetaspaceDecoder","ByteLevelDecoder","ReplaceDecoder","ByteFallback","FuseDecoder","StripDecoder","DecoderSequence","decode","decode_chain","text_decoder","TextDecoder","new_tokens","previous_byte_tokens","bytes","endsWith","parseInt","isNaN","string","Uint8Array","stop","start_cut","stop_cut","index","cleanup","prefix","byte_decoder","fatal","ignoreBOM","convert_tokens_to_string","byteArray","decoded_text","sub_texts","current_sub_text","decoders","toks","decoder","addPrefixSpace","replacement","strRep","str_rep","normalizedTokens","trimStart","substring","charsmap","precompiled_charsmap","tokenizers","pretokenizers","preTokenizedText","tokenizer","PreTrainedTokenizer","tokenizerJSON","tokenizerConfig","pre_tokenizer","model","entries","post_processor","special_tokens","all_special_ids","addedToken","set","special","added_tokens_regex","mask_token","getToken","mask_token_id","pad_token","pad_token_id","sep_token","sep_token_id","model_max_length","remove_space","clean_up_tokenization_spaces","padding_side","_len3","keys","_key3","__type","from_pretrained","progress_callback","cache_dir","local_files_only","revision","prepare_model_inputs","inputs","text_pair","padding","truncation","max_length","return_tensor","maxLengthOfBatch","Math","attention_mask","fill","diff","concat","unshift","some","dims","BigInt64Array","BigInt","modelInputs","input_ids","_encode_text","sections","filter","sectionTokens","tokens2","combinedTokens","batch_decode","batch","decode_args","token_ids","decode_single","_ref","skip_special_tokens","decoded","add_token_types","token_type_ids","data","BertTokenizer","AlbertTokenizer","MobileBertTokenizer","SqueezeBertTokenizer","DistilBertTokenizer","T5Tokenizer","GPT2Tokenizer","BartTokenizer","RobertaTokenizer","BloomTokenizer","LlamaTokenizer","NllbTokenizer","languageRegex","language_codes","test","_build_translation_inputs","raw_inputs","tokenizer_options","generate_kwargs","tgt_lang","src_lang","forced_bos_token_id","WHISPER_LANGUAGES","WHISPER_LANGUAGE_MAPPING","WHISPER_TO_LANGUAGE_CODE_MAPPING","_ref2","k","v","WhisperTokenizer","_decode_asr","sequences","return_timestamps","return_language","time_precision","force_full_sequences","last_language","new_chunk","chunks","chunk","time_offset","timestamp_begin","previous_tokens","skip","right_stride_start","last_timestamp","first_timestamp","chunk_len","stride_left","stride_right","stride","current_tokens","language","resolved_tokens","findLongestCommonSequence","resolved_text","time","rounded_time","round","timestamp","every","p","optional","full_text","leftSequence","leftLength","totalSequence","rightSequence","maxIndices","rightLength","eps","leftStart","leftStop","left","rightStart","rightStop","right","matches","elem","idx","matching","leftMid","floor","rightMid","get_decoder_prompt_ids","task","no_timestamps","forced_decoder_ids","language_code","is_language_code","langs","values","JSON","stringify","language_token_id","task_token_id","no_timestamps_id","CodeGenTokenizer","CLIPTokenizer","MarianTokenizer","supported_language_codes","matchInfo","remainder","root","CharTrieNode","default","texts","node","ch","child","children","isLeaf","nodes","beginNodes","endNodes","bos","TokenLatticeNode","eos","clone","pos","score","nodeId","viterbi","rnode","prev","bestScore","bestNode","lnode","backtraceScore","results","reverse","piece","tokenIds","AutoTokenizer","TOKENIZER_CLASS_MAPPING","quantized","tokenizerName","tokenizer_class"],"sources":["/Users/phreetech13/Desktop/RealTimeAudioToText/node_modules/@xenova/transformers/src/tokenizers.js"],"sourcesContent":["\n/**\n * @file Tokenizers are used to prepare textual inputs for a model.\n * \n * **Example:** Create an `AutoTokenizer` and use it to tokenize a sentence.\n * This will automatically detect the tokenizer type based on the tokenizer class defined in `tokenizer.json`.\n * ```javascript\n * import { AutoTokenizer } from '@xenova/transformers';\n * \n * let tokenizer = await AutoTokenizer.from_pretrained('bert-base-uncased');\n * let { input_ids } = await tokenizer('I love transformers!');\n * // Tensor {\n * //   data: BigInt64Array(6) [101n, 1045n, 2293n, 19081n, 999n, 102n],\n * //   dims: [1, 6],\n * //   type: 'int64',\n * //   size: 6,\n * // }\n * ```\n * \n * @module tokenizers\n */\n\nimport {\n    Callable,\n    reverseDictionary,\n    escapeRegExp,\n    isIntegralNumber,\n    mergeArrays,\n} from './utils/core.js';\n\nimport {\n    getModelJSON,\n} from './utils/hub.js';\n\nimport { max, min } from './utils/maths.js';\nimport { Tensor } from './utils/tensor.js';\n\n/**\n * @typedef {import('./utils/hub.js').PretrainedOptions} PretrainedOptions\n */\n\n/**\n * Loads a tokenizer from the specified path.\n * @param {string} pretrained_model_name_or_path The path to the tokenizer directory.\n * @param {PretrainedOptions} options Additional options for loading the tokenizer.\n * @returns {Promise<Array>} A promise that resolves with information about the loaded tokenizer.\n */\nasync function loadTokenizer(pretrained_model_name_or_path, options) {\n\n    let info = await Promise.all([\n        getModelJSON(pretrained_model_name_or_path, 'tokenizer.json', true, options),\n        getModelJSON(pretrained_model_name_or_path, 'tokenizer_config.json', true, options),\n    ])\n    return info;\n}\n\n/**\n * Helper method to construct a pattern from a config object.\n * @param {Object} pattern The pattern object.\n * @returns {RegExp|string|null} The compiled pattern.\n */\nfunction createPattern(pattern) {\n    if (pattern.Regex) {\n        return new RegExp(pattern.Regex, 'gu');\n\n    } else if (pattern.String) {\n        return pattern.String;\n\n    } else {\n        console.warn('Unknown pattern type:', pattern)\n        return null;\n    }\n}\n\n/**\n * Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms\n * @param {string} text The text to clean up.\n * @returns {string} The cleaned up text.\n */\nfunction clean_up_tokenization(text) {\n    // Clean up a list of simple English tokenization artifacts\n    // like spaces before punctuations and abbreviated forms\n    return text.replace(/ \\./g, '.')\n        .replace(/ \\?/g, '?')\n        .replace(/ \\!/g, '!')\n        .replace(/ ,/g, ',')\n        .replace(/ \\' /g, \"'\")\n        .replace(/ n\\'t/g, \"n't\")\n        .replace(/ \\'m/g, \"'m\")\n        .replace(/ \\'s/g, \"'s\")\n        .replace(/ \\'ve/g, \"'ve\")\n        .replace(/ \\'re/g, \"'re\");\n}\n\n/**\n * Helper function to fuse consecutive values in an array equal to the specified value.\n * @param {Array} arr The input array\n * @param {any} value The value to fuse on.\n */\nfunction fuse(arr, value) {\n    let fused = [];\n    let i = 0;\n    while (i < arr.length) {\n        fused.push(arr[i])\n        if (arr[i] !== value) {\n            ++i;\n            continue;\n        }\n\n        while (i < arr.length && arr[i] === value) {\n            ++i;\n        }\n    }\n\n    return fused;\n}\n\n/**\n * Split a string on whitespace.\n * @param {string} text The text to split.\n * @returns {string[]} The split string.\n */\nfunction whitespace_split(text) {\n    return text.match(/\\S+/g) || [];\n}\n\n/**\n * Abstract base class for tokenizer models.\n *\n * @extends Callable\n */\nexport class TokenizerModel extends Callable {\n    /**\n     * Creates a new instance of TokenizerModel.\n     * @param {Object} config The configuration object for the TokenizerModel.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n\n        /** @type {string[]} */\n        this.vocab = [];\n\n        /**\n         * A mapping of tokens to ids.\n         * @type {Map<string, number>}\n         */\n        this.tokens_to_ids = new Map();\n\n        this.unk_token_id = undefined;\n        this.unk_token = undefined;\n        this.end_of_word_suffix = undefined;\n\n        /** @type {boolean} Whether to fuse unknown tokens when encoding. Defaults to false. */\n        this.fuse_unk = false;\n    }\n\n    /**\n     * Instantiates a new TokenizerModel instance based on the configuration object provided.\n     * @param {Object} config The configuration object for the TokenizerModel.\n     * @param {...*} args Optional arguments to pass to the specific TokenizerModel constructor.\n     * @returns {TokenizerModel} A new instance of a TokenizerModel.\n     * @throws Will throw an error if the TokenizerModel type in the config is not recognized.\n     */\n    static fromConfig(config, ...args) {\n        switch (config.type) {\n            case 'WordPiece':\n                return new WordPieceTokenizer(config);\n            case 'Unigram':\n                // @ts-ignore\n                return new Unigram(config, ...args);\n\n            case 'BPE':\n                // @ts-ignore\n                return new BPE(config, ...args);\n            default:\n                throw new Error(`Unknown TokenizerModel type: ${config.type}`);\n        }\n    }\n\n    /**\n     * Internal function to call the TokenizerModel instance.\n     * @param {string[]} tokens The tokens to encode.\n     * @returns {number[]} The encoded token IDs.\n     */\n    _call(tokens) {\n        return this.encode(tokens);\n    }\n\n    /**\n     * Encodes a list of tokens into a list of token IDs.\n     * @param {string[]} tokens The tokens to encode.\n     * @returns {number[]} The encoded token IDs.\n     * @throws Will throw an error if not implemented in a subclass.\n     */\n    encode(tokens) {\n        throw Error(\"encode should be implemented in subclass.\")\n    }\n\n    /**\n     * Converts a list of tokens into a list of token IDs.\n     * @param {string[]} tokens The tokens to convert.\n     * @returns {number[]} The converted token IDs.\n     */\n    convert_tokens_to_ids(tokens) {\n        let ids = tokens.map(t => this.tokens_to_ids.get(t) ?? this.unk_token_id);\n\n        if (this.fuse_unk) {\n            // Fuse unknown tokens\n            ids = fuse(ids, this.unk_token_id);\n        }\n        return ids;\n    }\n\n    /**\n     * Converts a list of token IDs into a list of tokens.\n     * @param {number[]} ids The token IDs to convert.\n     * @returns {string[]} The converted tokens.\n     */\n    convert_ids_to_tokens(ids) {\n        return ids.map(i => this.vocab[i] ?? this.unk_token);\n    }\n}\n\n/**\n * A subclass of TokenizerModel that uses WordPiece encoding to encode tokens.\n * @extends TokenizerModel\n */\nclass WordPieceTokenizer extends TokenizerModel {\n    /**\n     * @param {Object} config The configuration object.\n     * @param {Map<string, number>} config.vocab A mapping of tokens to ids.\n     * @param {string} config.unk_token The unknown token string.\n     * @param {string} config.continuing_subword_prefix The prefix to use for continuing subwords.\n     */\n    constructor(config) {\n        super(config);\n        /**\n         * A mapping of tokens to ids.\n         * @type {Map<string, number>}\n         */\n        this.tokens_to_ids = config.vocab;\n\n        /**\n         * The id of the unknown token.\n         * @type {number}\n         */\n        this.unk_token_id = this.tokens_to_ids.get(config.unk_token);\n\n        /**\n         * The unknown token string.\n         * @type {string}\n         */\n        this.unk_token = config.unk_token;\n\n        /**\n         * An array of tokens.\n         * @type {string[]}\n         */\n        this.vocab = new Array(this.tokens_to_ids.size);\n\n        for (const [key, value] of this.tokens_to_ids) {\n            this.vocab[value] = key;\n        }\n    }\n\n    /**\n     * Encodes an array of tokens using WordPiece encoding.\n     * @param {Array} tokens The tokens to encode.\n     * @returns {Array} An array of encoded tokens.\n     */\n    encode(tokens) {\n        let outputTokens = [];\n        for (let token of tokens) {\n            let chars = [...token];\n            // TODO add\n            // if len(chars) > self.max_input_chars_per_word:\n            //     output_tokens.append(self.unk_token)\n            //     continue\n\n            let isUnknown = false;\n            let start = 0;\n            let subTokens = [];\n\n            while (start < chars.length) {\n                let end = chars.length;\n                let currentSubstring = null;\n                while (start < end) {\n                    let substr = chars.slice(start, end).join('');\n\n                    if (start > 0) {\n                        substr = this.config.continuing_subword_prefix + substr;\n                    }\n                    if (this.tokens_to_ids.has(substr)) {\n                        currentSubstring = substr;\n                        break;\n                    }\n\n                    --end;\n                }\n                if (currentSubstring === null) {\n                    isUnknown = true;\n                    break;\n                }\n                subTokens.push(currentSubstring);\n                start = end;\n            }\n            if (isUnknown) {\n                outputTokens.push(this.unk_token);\n            } else {\n                outputTokens.push(...subTokens);\n            }\n        }\n\n        return outputTokens;\n    }\n\n}\n\n/**\n * Class representing a Unigram tokenizer model.\n * @extends TokenizerModel\n */\nclass Unigram extends TokenizerModel {\n    /**\n     * Create a new Unigram tokenizer model.\n     * @param {Object} config The configuration object for the Unigram model.\n     * @param {number} config.unk_id The ID of the unknown token\n     * @param {Map<string, number>} config.vocab A mapping of tokens to scores.\n     * @param {Object} moreConfig Additional configuration object for the Unigram model.\n     */\n    constructor(config, moreConfig) {\n        super(config);\n\n        this.vocab = new Array(config.vocab.size);\n        this.scores = new Array(config.vocab.size);\n        let count = 0;\n        config.vocab.forEach((value, key) => {\n            this.vocab[count] = key;\n            this.scores[count] = value;\n            ++count;\n        });\n\n        this.unk_token_id = config.unk_id;\n        this.unk_token = this.vocab[config.unk_id];\n\n        this.tokens_to_ids = new Map(this.vocab.map((x, i) => [x, i]));\n        this.bosToken = ' '; // beginning of a sentence token\n\n        this.bosTokenId = this.tokens_to_ids.get(this.bosToken); // NOTE: may be undefined\n        this.eosToken = moreConfig.eos_token;\n\n        this.eosTokenId = this.tokens_to_ids.get(this.eosToken);\n        this.unkToken = this.vocab[this.unk_token_id];\n\n        this.minScore = min(this.scores)[0];\n\n        this.unkScore = this.minScore - 10.0;\n        this.scores[this.unk_token_id] = this.unkScore;\n\n        this.trie = new CharTrie();\n        this.trie.extend(this.vocab);\n\n        // NOTE: `fuse_unk` is hardcoded to true for Unigram models\n        // See: https://github.com/huggingface/tokenizers/blob/b58227c7f1ccf8b73ee2268354336da56d91e492/tokenizers/src/models/unigram/model.rs#L119\n        this.fuse_unk = true;\n    }\n\n    /**\n     * Populates lattice nodes.\n     * @param {TokenLattice} lattice The token lattice to populate with nodes.\n     */\n    populateNodes(lattice) {\n        const sentence = lattice.sentence;\n        const len = sentence.length;\n        let beginPos = 0;\n        while (beginPos < len) {\n            const mblen = 1;\n            let hasSingleNode = false;\n            const tokens = [];\n\n            for (let token of this.trie.commonPrefixSearch(sentence.slice(beginPos))) {\n                tokens.push(token);\n                const tokenId = this.tokens_to_ids.get(token);\n                const tokenScore = this.scores[tokenId];\n                const n = token.length;\n                lattice.insert(beginPos, n, tokenScore, tokenId);\n                if (!hasSingleNode && n === mblen) {\n                    hasSingleNode = true;\n                }\n            }\n            if (!hasSingleNode) {\n                lattice.insert(beginPos, mblen, this.unkScore, this.unk_token_id);\n            }\n            beginPos += mblen;\n        }\n    }\n\n    /**\n     * Encodes an array of tokens into an array of subtokens using the unigram model.\n     *\n     * @param {string} normalized The normalized string.\n     * @returns {string[]} An array of subtokens obtained by encoding the input tokens using the unigram model.\n     */\n    tokenize(normalized) {\n        const lattice = new TokenLattice(normalized, this.bosTokenId, this.eosTokenId);\n        this.populateNodes(lattice);\n        return lattice.tokens();\n    }\n\n    /**\n     * Encodes an array of tokens using WordPiece encoding.\n     * @param {Array} tokens The tokens to encode.\n     * @returns {Array} An array of encoded tokens.\n     */\n    encode(tokens) {\n        let toReturn = [];\n        for (let token of tokens) {\n            const tokenized = this.tokenize(token);\n            toReturn.push(...tokenized);\n        }\n        return toReturn;\n    }\n\n}\n\n/**\n * Returns list of utf-8 byte and a mapping to unicode strings.\n * Specifically avoids mapping to whitespace/control characters the BPE code barfs on.\n * @returns {Object} Object with utf-8 byte keys and unicode string values.\n */\nconst BYTES_TO_UNICODE = (() => {\n    // Returns list of utf-8 byte and a mapping to unicode strings.\n    // We specifically avoids mapping to whitespace/control characters\n    // the bpe code barfs on.\n\n    const bs = [\n        ...Array.from({ length: \"~\".charCodeAt(0) - \"!\".charCodeAt(0) + 1 }, (_, i) => i + \"!\".charCodeAt(0)),\n        ...Array.from({ length: \"¬\".charCodeAt(0) - \"¡\".charCodeAt(0) + 1 }, (_, i) => i + \"¡\".charCodeAt(0)),\n        ...Array.from({ length: \"ÿ\".charCodeAt(0) - \"®\".charCodeAt(0) + 1 }, (_, i) => i + \"®\".charCodeAt(0)),\n    ];\n    let cs = bs.slice();\n    let n = 0;\n    for (let b = 0; b < 256; ++b) {\n        if (!bs.includes(b)) {\n            bs.push(b);\n            cs.push(256 + n);\n            n += 1;\n        }\n    }\n    let ccs = cs.map(n => String.fromCharCode(n));\n    return Object.fromEntries(bs.map((b, i) => [b, ccs[i]]));\n})();\n\nconst UNICODE_TO_BYTES = reverseDictionary(BYTES_TO_UNICODE);\n\n/**\n * BPE class for encoding text into Byte-Pair-Encoding (BPE) tokens.\n * @extends TokenizerModel\n */\nclass BPE extends TokenizerModel {\n    /**\n     * Create a BPE instance.\n     * @param {Object} config The configuration object for BPE.\n     * @param {Map<string, number>} config.vocab A mapping of tokens to ids.\n     * @param {string} config.unk_token The unknown token used for out of vocabulary words.\n     * @param {string} config.end_of_word_suffix The suffix to place at the end of each word.\n     * @param {Array} config.merges An array of BPE merges as strings.\n     */\n    constructor(config) {\n        super(config);\n\n        this.tokens_to_ids = config.vocab;\n\n        this.unk_token_id = this.tokens_to_ids.get(config.unk_token);\n        this.unk_token = config.unk_token;\n\n        this.vocab = new Array(this.tokens_to_ids.size);\n        for (const [key, value] of this.tokens_to_ids) {\n            this.vocab[value] = key;\n        }\n\n        this.bpe_ranks = Object.fromEntries(config.merges.map((x, i) => [x, i]));\n        this.merges = config.merges.map(x => x.split(/\\s+/))\n\n        this.end_of_word_suffix = config.end_of_word_suffix;\n\n        this.byte_fallback = this.config.byte_fallback ?? false;\n\n        if (this.byte_fallback) {\n            this.text_encoder = new TextEncoder();\n        }\n\n        this.cache = Object.create(null);\n\n        this.fuse_unk ??= this.config.fuse_unk;\n    }\n\n    /**\n     * Get all the possible pairs of characters in a word.\n     * @param {string[]} word The word to get pairs from.\n     * @returns {Array} An array of pairs.\n     */\n    get_pairs(word) {\n        let pairs = new Set();\n        let prev_char = word[0];\n        for (let i = 1; i < word.length; ++i) {\n            let char = word[i];\n            pairs.add(`${prev_char} ${char}`);\n            prev_char = char;\n        }\n        return Array.from(pairs);\n    }\n\n    /**\n     * Apply Byte-Pair-Encoding (BPE) to a given token.\n     * @param {string} token The token to encode.\n     * @returns {string} The BPE encoded token.\n     */\n    bpe(token) {\n        if (token in this.cache) {\n            return this.cache[token];\n        }\n        let word = Array.from(token);\n        if (this.end_of_word_suffix) {\n            word[word.length - 1] += this.end_of_word_suffix;\n        }\n        let pairs = this.get_pairs(word);\n\n        if (!pairs.length) {\n            if (this.end_of_word_suffix) {\n                token += this.end_of_word_suffix;\n            }\n            return token;\n        }\n\n        while (true) {\n            let bigram = pairs.reduce((a, b) => {\n                let c = this.bpe_ranks[a] ?? Infinity\n                let d = this.bpe_ranks[b] ?? Infinity\n                return c <= d ? a : b;\n            });\n            if (!(bigram in this.bpe_ranks)) {\n                break;\n            }\n            let [first, second] = bigram.split(/\\s+/g)\n            let new_word = [];\n            let i = 0;\n            let j = -1;\n\n            while (i < word.length) {\n                try {\n                    j = word.indexOf(first, i);\n                    if (j === -1) throw \"Error\";\n                } catch (e) {\n                    new_word.push(...word.slice(i));\n                    break;\n                }\n                new_word.push(...word.slice(i, j));\n                i = j;\n\n                if (word[i] === first && i < word.length - 1 && word[i + 1] === second) {\n                    new_word.push(first + second);\n                    i += 2;\n                } else {\n                    new_word.push(word[i]);\n                    i += 1;\n                }\n            }\n            word = new_word\n            if (word.length === 1) {\n                break;\n            } else {\n                pairs = this.get_pairs(word);\n            }\n        }\n        let final_word = word.join(\" \");\n        this.cache[token] = final_word;\n        return final_word;\n    }\n\n    /**\n     * Encodes the input sequence of tokens using the BPE algorithm and returns the resulting subword tokens.\n     * @param {Array} tokens The input sequence of tokens to encode.\n     * @returns {Array} The resulting subword tokens after applying the BPE algorithm to the input sequence of tokens.\n     */\n    encode(tokens) {\n        let outputTokens = [];\n\n        for (let token of tokens) {\n            let bpe_token_list = this.bpe(token).split(' ');\n\n            for (let t of bpe_token_list) {\n                if (this.tokens_to_ids.has(t)) {\n                    outputTokens.push(t);\n                } else {\n                    if (this.byte_fallback) {\n                        outputTokens.push(\n                            ...Array.from(this.text_encoder.encode(t))\n                                .map(x => `<0x${x.toString(16).toUpperCase().padStart(2, '0')}>`)\n                        );\n                    } else {\n                        outputTokens.push(this.unk_token);\n                    }\n                }\n            }\n        }\n\n        return outputTokens;\n    }\n\n}\n\n/**\n * A base class for text normalization.\n * @abstract\n */\nclass Normalizer extends Callable {\n    /**\n     * @param {Object} config The configuration object for the normalizer.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n    }\n\n    /**\n     * Factory method for creating normalizers from config objects.\n     * @static\n     * @param {Object} config The configuration object for the normalizer.\n     * @returns {Normalizer} A Normalizer object.\n     * @throws {Error} If an unknown Normalizer type is specified in the config.\n     */\n    static fromConfig(config) {\n        if (config === null) return null;\n        switch (config.type) {\n            case 'BertNormalizer':\n                return new BertNormalizer(config);\n            case 'Precompiled':\n                return new Precompiled(config);\n            case 'Sequence':\n                return new NormalizerSequence(config);\n            case 'Replace':\n                return new Replace(config);\n            case 'NFC':\n                return new NFC(config);\n            case 'NFKD':\n                return new NFKD(config);\n            case 'StripAccents':\n                return new StripAccents(config);\n            case 'Lowercase':\n                return new Lowercase(config);\n            case 'Prepend':\n                return new Prepend(config);\n            default:\n                throw new Error(`Unknown Normalizer type: ${config.type}`);\n        }\n    }\n\n    /**\n     * Normalize the input text.\n     * @abstract\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     * @throws {Error} If this method is not implemented in a subclass.\n     */\n    normalize(text) {\n        throw Error(\"normalize should be implemented in subclass.\")\n    }\n\n    /**\n     * Alias for {@link Normalizer#normalize}.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    _call(text) {\n        return this.normalize(text);\n    }\n\n}\n\n/**\n * Replace normalizer that replaces occurrences of a pattern with a given string or regular expression.\n * @extends Normalizer\n */\nclass Replace extends Normalizer {\n    /**\n     * Normalize the input text by replacing the pattern with the content.\n     * @param {string} text The input text to be normalized.\n     * @returns {string} The normalized text after replacing the pattern with the content.\n     */\n    normalize(text) {\n        let pattern = createPattern(this.config.pattern);\n        if (pattern === null) {\n            return text;\n        }\n\n        text = text.replaceAll(pattern, this.config.content)\n\n        return text;\n    }\n}\n\n/**\n * A normalizer that applies Unicode normalization form C (NFC) to the input text.\n * @extends Normalizer\n */\nclass NFC extends Normalizer {\n    /**\n     * Normalize the input text by applying Unicode normalization form C (NFC).\n     * @param {string} text The input text to be normalized.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = text.normalize('NFC')\n        return text;\n    }\n}\n\n/**\n * NFKD Normalizer.\n * @extends Normalizer\n */\nclass NFKD extends Normalizer {\n    /**\n     * Normalize text using NFKD normalization.\n     * @param {string} text The text to be normalized.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = text.normalize('NFKD')\n        return text;\n    }\n}\n\n/**\n * StripAccents normalizer removes all accents from the text.\n * @extends Normalizer\n */\nclass StripAccents extends Normalizer {\n    /**\n     * Remove all accents from the text.\n     * @param {string} text The input text.\n     * @returns {string} The normalized text without accents.\n     */\n    normalize(text) {\n        text = text.replace(/[\\u0300-\\u036f]/g, '');\n        return text;\n    }\n}\n\n/**\n * A Normalizer that lowercases the input string.\n * @extends Normalizer\n */\nclass Lowercase extends Normalizer {\n    /**\n     * Lowercases the input string.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = text.toLowerCase();\n        return text;\n    }\n}\n\n/**\n * A Normalizer that prepends a string to the input string.\n * @extends Normalizer\n */\nclass Prepend extends Normalizer {\n    /**\n     * Prepends the input string.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = this.config.prepend + text;\n        return text;\n    }\n}\n\n/**\n * A Normalizer that applies a sequence of Normalizers.\n * @extends Normalizer\n */\nclass NormalizerSequence extends Normalizer {\n    /**\n   * Create a new instance of NormalizerSequence.\n   * @param {Object} config The configuration object.\n   * @param {Object[]} config.normalizers An array of Normalizer configuration objects.\n   */\n    constructor(config) {\n        super(config);\n        this.normalizers = config.normalizers.map(x => Normalizer.fromConfig(x));\n    }\n    /**\n   * Apply a sequence of Normalizers to the input text.\n   * @param {string} text The text to normalize.\n   * @returns {string} The normalized text.\n   */\n    normalize(text) {\n        return this.normalizers.reduce((t, normalizer) => {\n            return normalizer.normalize(t);\n        }, text);\n    }\n}\n\n/**\n * A class representing a normalizer used in BERT tokenization.\n * @extends Normalizer\n */\nclass BertNormalizer extends Normalizer {\n    /**\n     * Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the input text.\n     *\n     * @param {string} text The input text to tokenize.\n     * @returns {string} The tokenized text with whitespace added around CJK characters.\n     */\n    _tokenize_chinese_chars(text) {\n        /* Adds whitespace around any CJK character. */\n        let output = [];\n        for (let i = 0; i < text.length; ++i) {\n            let char = text[i];\n            let cp = char.charCodeAt(0);\n            if (this._is_chinese_char(cp)) {\n                output.push(\" \");\n                output.push(char);\n                output.push(\" \");\n            } else {\n                output.push(char);\n            }\n        }\n        return output.join(\"\");\n    }\n\n    /**\n     * Checks whether the given Unicode codepoint represents a CJK (Chinese, Japanese, or Korean) character.\n     *\n     * A \"chinese character\" is defined as anything in the CJK Unicode block:\n     * https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n     *\n     * Note that the CJK Unicode block is NOT all Japanese and Korean characters, despite its name.\n     * The modern Korean Hangul alphabet is a different block, as is Japanese Hiragana and Katakana.\n     * Those alphabets are used to write space-separated words, so they are not treated specially\n     * and are handled like all other languages.\n     *\n     * @param {number} cp The Unicode codepoint to check.\n     * @returns {boolean} True if the codepoint represents a CJK character, false otherwise.\n     */\n    _is_chinese_char(cp) {\n        return (\n            (cp >= 0x4E00 && cp <= 0x9FFF)\n            || (cp >= 0x3400 && cp <= 0x4DBF)\n            || (cp >= 0x20000 && cp <= 0x2A6DF)\n            || (cp >= 0x2A700 && cp <= 0x2B73F)\n            || (cp >= 0x2B740 && cp <= 0x2B81F)\n            || (cp >= 0x2B820 && cp <= 0x2CEAF)\n            || (cp >= 0xF900 && cp <= 0xFAFF)\n            || (cp >= 0x2F800 && cp <= 0x2FA1F)\n        )\n    }\n    /**\n     * Strips accents from the given text.\n     * @param {string} text The text to strip accents from.\n     * @returns {string} The text with accents removed.\n     */\n    stripAccents(text) {\n        return text.normalize('NFD').replace(/[\\u0300-\\u036f]/g, '');\n    }\n\n    /**\n     * Normalizes the given text based on the configuration.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        // TODO use rest of config\n        // config.clean_text,\n        // config.handle_chinese_chars,\n        // config.strip_accents,\n        // config.lowercase,\n\n        if (this.config.handle_chinese_chars) {\n            text = this._tokenize_chinese_chars(text);\n        }\n\n        if (this.config.lowercase) {\n            text = text.toLowerCase();\n\n            if (this.config.strip_accents !== false) {\n                text = this.stripAccents(text);\n            }\n        } else if (this.config.strip_accents) {\n            text = this.stripAccents(text);\n        }\n\n        return text;\n    }\n}\n\n/**\n * A callable class representing a pre-tokenizer used in tokenization. Subclasses\n * should implement the `pre_tokenize_text` method to define the specific pre-tokenization logic.\n * @extends Callable\n */\nclass PreTokenizer extends Callable {\n    /**\n   * Factory method that returns an instance of a subclass of `PreTokenizer` based on the provided configuration.\n   *\n   * @static\n   * @param {Object} config A configuration object for the pre-tokenizer.\n   * @returns {PreTokenizer} An instance of a subclass of `PreTokenizer`.\n   * @throws {Error} If the provided configuration object does not correspond to any known pre-tokenizer.\n   */\n    static fromConfig(config) {\n        if (config === null) return null;\n\n        switch (config.type) {\n            case 'BertPreTokenizer':\n                return new BertPreTokenizer(config);\n            case 'Sequence':\n                return new PreTokenizerSequence(config);\n            case 'WhitespaceSplit':\n                return new WhitespaceSplit(config);\n            case 'Metaspace':\n                return new MetaspacePreTokenizer(config);\n\n            case 'ByteLevel':\n                return new ByteLevelPreTokenizer(config);\n            case 'Split':\n                return new SplitPreTokenizer(config);\n\n            default:\n                throw new Error(`Unknown PreTokenizer type: ${config.type}`);\n        }\n    }\n\n    /**\n   * Method that should be implemented by subclasses to define the specific pre-tokenization logic.\n   *\n   * @abstract\n   * @param {string} text The text to pre-tokenize.\n   * @returns {string[]} The pre-tokenized text.\n   * @throws {Error} If the method is not implemented in the subclass.\n   */\n    pre_tokenize_text(text) {\n        throw Error(\"pre_tokenize_text should be implemented in subclass.\")\n    }\n\n    /**\n     * Tokenizes the given text into pre-tokens.\n     * @param {string|string[]} text The text or array of texts to pre-tokenize.\n     * @returns {string[]} An array of pre-tokens.\n     */\n    pre_tokenize(text) {\n        let result = [];\n        if (Array.isArray(text)) {\n            result = text.map(x => this.pre_tokenize_text(x))\n        } else {\n            result = this.pre_tokenize_text(text);\n        }\n        return result.flat();\n    }\n\n    /**\n     * Alias for {@link PreTokenizer#pre_tokenize}.\n     * @param {string|string[]} text The text or array of texts to pre-tokenize.\n     * @returns {string[]} An array of pre-tokens.\n     */\n    _call(text) {\n        return this.pre_tokenize(text);\n    }\n}\n\n/**\n * @extends PreTokenizer\n */\nclass BertPreTokenizer extends PreTokenizer {\n    /**\n     * A PreTokenizer that splits text into wordpieces using a basic tokenization scheme\n     * similar to that used in the original implementation of BERT.\n     * \n     * @param {Object} config The configuration object.\n     */\n    constructor(config) {\n        super();\n        // TODO use config\n\n        // Construct a pattern which matches the rust implementation:\n        // https://github.com/huggingface/tokenizers/blob/b4fcc9ce6e4ad5806e82826f816acfdfdc4fcc67/tokenizers/src/pre_tokenizers/bert.rs#L11\n        // Equivalent to removing whitespace and splitting on punctuation (both \\p{P} and other ascii characters)\n        const punctuation = '\\\\p{P}\\\\u0021-\\\\u002F\\\\u003A-\\\\u0040\\\\u005B-\\\\u0060\\\\u007B-\\\\u007E'\n        this.pattern = new RegExp(`[^\\\\s${punctuation}]+|[${punctuation}]`, 'gu');\n    }\n    /**\n     * Tokenizes a single text using the BERT pre-tokenization scheme.\n     * \n     * @param {string} text The text to tokenize.\n     * @returns {Array<string>} An array of tokens.\n     */\n    pre_tokenize_text(text) {\n        return text.trim().match(this.pattern) || [];\n    }\n}\n\n/**\n * A pre-tokenizer that splits text into Byte-Pair-Encoding (BPE) subwords.\n * @extends PreTokenizer\n */\nclass ByteLevelPreTokenizer extends PreTokenizer {\n    /**\n     * Creates a new instance of the `ByteLevelPreTokenizer` class.\n     * @param {Object} config The configuration object.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n\n        /**\n         * @type {boolean} Whether to add a leading space to the first word.\n         * This allows to treat the leading word just as any other word.\n         */\n        this.add_prefix_space = this.config.add_prefix_space;\n\n        /**\n         * @type {boolean} Whether the post processing step should trim offsets\n         * to avoid including whitespaces.\n         * @todo Use this in the pretokenization step.\n         */\n        this.trim_offsets = this.config.trim_offsets;\n\n        /**\n         * @type {boolean} Whether to use the standard GPT2 regex for whitespace splitting.\n         * Set it to False if you want to use your own splitting. Defaults to true.\n         */\n        this.use_regex = this.config.use_regex ?? true;\n        this.pattern = /'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+/gu;\n\n        this.byte_encoder = BYTES_TO_UNICODE;\n        this.text_encoder = new TextEncoder();\n    }\n\n    /**\n     * Tokenizes a single piece of text using byte-level tokenization.\n     * @param {string} text The text to tokenize.\n     * @returns {string[]} An array of tokens.\n     */\n    pre_tokenize_text(text) {\n        // Split on whitespace and punctuation\n        let tokens = this.use_regex ? (text.match(this.pattern) || []) : [text];\n\n        return tokens.map(token => {\n            if (this.add_prefix_space && !token.startsWith(' ')) {\n                token = ' ' + token;\n            }\n\n            // Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n            token = Array.from(this.text_encoder.encode(token), byte => this.byte_encoder[byte]).join('');\n\n            return token;\n        });\n    }\n}\n\n/**\n * Splits text using a given pattern.\n * @extends PreTokenizer\n */\nclass SplitPreTokenizer extends PreTokenizer {\n    /**\n     * @param {Object} config The configuration options for the pre-tokenizer.\n     * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.\n     * @param {string|undefined} config.pattern.String The string to use for splitting. Only defined if the pattern is a string.\n     * @param {string|undefined} config.pattern.Regex The regex to use for splitting. Only defined if the pattern is a regex.\n     * @param {'isolated'|'removed'} config.behavior The behavior to use when splitting.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n    }\n\n    /**\n     * Tokenizes text by splitting it using the given pattern.\n     * @param {string} text The text to tokenize.\n     * @returns {string[]} An array of tokens.\n     */\n    pre_tokenize_text(text) {\n        let pattern = createPattern(this.config.pattern);\n        if (pattern === null) {\n            return [];\n        }\n\n        switch (this.config.behavior.toLowerCase()) {\n            // TODO add merged_with_previous, merged_with_next, contiguous\n            // TODO these should act slightly differently. Currently, we haven't found a tokenizer which produces different results.\n            case 'isolated':\n            case 'removed':\n                return text.match(pattern) || [];\n            default:\n                console.warn(`Unknown split behavior: \"${this.config.behavior}\"`)\n                return [];\n        }\n    }\n}\n\n/**\n * @extends Callable\n */\nclass PostProcessor extends Callable {\n\n    /**\n     * @param {Object} config The configuration for the post-processor.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n    }\n\n    /**\n     * Factory method to create a PostProcessor object from a configuration object.\n     *\n     * @param {Object} config Configuration object representing a PostProcessor.\n     * @returns {PostProcessor} A PostProcessor object created from the given configuration.\n     * @throws {Error} If an unknown PostProcessor type is encountered.\n     */\n    static fromConfig(config) {\n        switch (config.type) {\n            case 'TemplateProcessing':\n                return new TemplateProcessing(config);\n\n            case 'ByteLevel':\n                return new ByteLevelPostProcessor(config);\n\n            case 'RobertaProcessing':\n                return new RobertaProcessing(config);\n\n            default:\n                throw new Error(`Unknown PostProcessor type: ${config.type}`);\n        }\n    }\n\n    /**\n     * Method to be implemented in subclass to apply post-processing on the given tokens.\n     *\n     * @param {Array} tokens The input tokens to be post-processed.\n     * @param {...*} args Additional arguments required by the post-processing logic.\n     * @returns {Array} The post-processed tokens.\n     * @throws {Error} If the method is not implemented in subclass.\n     */\n    post_process(tokens, ...args) {\n        throw Error(\"post_process should be implemented in subclass.\")\n    }\n\n    /**\n     * Alias for {@link PostProcessor#post_process}.\n     * @param {Array} tokens The text or array of texts to post-process.\n     * @param {...*} args Additional arguments required by the post-processing logic.\n     * @returns {Array} An array of post-processed tokens.\n     */\n    _call(tokens, ...args) {\n        return this.post_process(tokens, ...args);\n    }\n}\n\n/**\n * A post-processor that adds special tokens to the beginning and end of the input.\n * @extends PostProcessor\n */\nclass RobertaProcessing extends PostProcessor {\n    /**\n     * @param {Object} config The configuration for the post-processor.\n     * @param {string[]} config.cls The special tokens to add to the beginning of the input.\n     * @param {string[]} config.sep The special tokens to add to the end of the input.\n     */\n    constructor(config) {\n        super(config);\n        // TODO use all of config: add_prefix_space, trim_offsets\n\n        this.cls = config.cls[0];\n        this.sep = config.sep[0];\n    }\n\n    /**\n     * Adds the special tokens to the beginning and end of the input.\n     * @param {string[]} tokens The input tokens.\n     * @param {string[]|null} tokens_pair An optional second set of input tokens.\n     * @returns {string[]} The input tokens with the special tokens added to the beginning and end.\n     */\n    post_process(tokens, tokens_pair = null) {\n        tokens = mergeArrays([this.cls], tokens, [this.sep]);\n\n        // NOTE: It is intended to add 2 EOS tokens after the first set of tokens\n        // https://github.com/huggingface/tokenizers/issues/983\n        if (tokens_pair !== null) {\n            tokens = mergeArrays(tokens, [this.sep], tokens_pair, [this.sep]);\n        }\n        return tokens;\n    }\n}\n\n/**\n * Post processor that replaces special tokens in a template with actual tokens.\n * @extends PostProcessor\n */\nclass TemplateProcessing extends PostProcessor {\n    /**\n     * Creates a new instance of `TemplateProcessing`.\n     * @param {Object} config The configuration options for the post processor.\n     * @param {Array} config.single The template for a single sequence of tokens.\n     * @param {Array} config.pair The template for a pair of sequences of tokens.\n     */\n    constructor(config) {\n        super(config);\n\n        this.single = config.single;\n        this.pair = config.pair;\n    }\n\n    /**\n     * Replaces special tokens in the template with actual tokens.\n     * @param {Array} tokens The list of tokens for the first sequence.\n     * @param {Array} [tokens_pair=null] The list of tokens for the second sequence (optional).\n     * @returns {Array} The list of tokens with the special tokens replaced with actual tokens.\n     */\n    post_process(tokens, tokens_pair = null) {\n        let type = tokens_pair === null ? this.single : this.pair\n\n        let toReturn = [];\n        for (let item of type) {\n            if ('SpecialToken' in item) {\n                toReturn.push(item.SpecialToken.id);\n\n            } else if ('Sequence' in item) {\n                if (item.Sequence.id === 'A') {\n                    toReturn = mergeArrays(toReturn, tokens);\n\n                } else if (item.Sequence.id === 'B') {\n                    toReturn = mergeArrays(toReturn, tokens_pair);\n                }\n            }\n        }\n        return toReturn;\n    }\n}\n\n/**\n * A PostProcessor that returns the given tokens as is.\n * @extends PostProcessor\n */\nclass ByteLevelPostProcessor extends PostProcessor {\n    /**\n     * Post process the given tokens.\n     * @param {string[]} tokens The tokens to be post processed.\n     * @returns {string[]} The post processed tokens.\n     */\n    post_process(tokens) {\n        return tokens;\n    }\n}\n\n/**\n * The base class for token decoders.\n * @extends Callable\n */\nclass Decoder extends Callable {\n\n    /**\n    * Creates an instance of `Decoder`.\n    *\n    * @param {Object} config The configuration object.\n    */\n    constructor(config) {\n        super();\n        this.config = config;\n\n        this.added_tokens = [];\n        this.end_of_word_suffix = null;\n        this.trim_offsets = config.trim_offsets;\n    }\n\n    /**\n   * Creates a decoder instance based on the provided configuration.\n   *\n   * @param {Object} config The configuration object.\n   * @returns {Decoder} A decoder instance.\n   * @throws {Error} If an unknown decoder type is provided.\n   */\n    static fromConfig(config) {\n        switch (config.type) {\n            case 'WordPiece':\n                return new WordPieceDecoder(config);\n            case 'Metaspace':\n                return new MetaspaceDecoder(config);\n            case 'ByteLevel':\n                return new ByteLevelDecoder(config);\n\n            case 'Replace':\n                return new ReplaceDecoder(config);\n            case 'ByteFallback':\n                return new ByteFallback(config);\n            case 'Fuse':\n                return new FuseDecoder(config);\n            case 'Strip':\n                return new StripDecoder(config);\n\n            case 'Sequence':\n                return new DecoderSequence(config);\n\n            default:\n                throw new Error(`Unknown Decoder type: ${config.type}`);\n        }\n    }\n\n    /**\n    * Calls the `decode` method.\n    *\n    * @param {string[]} tokens The list of tokens.\n    * @returns {string} The decoded string.\n    */\n    _call(tokens) {\n        return this.decode(tokens);\n    }\n\n    /**\n    * Decodes a list of tokens.\n    * @param {string[]} tokens The list of tokens.\n    * @returns {string} The decoded string.\n    */\n    decode(tokens) {\n        return this.decode_chain(tokens).join('');\n    }\n\n    /**\n     * Apply the decoder to a list of tokens.\n     * \n     * @param {string[]} tokens The list of tokens.\n     * @returns {string[]} The decoded list of tokens.\n     * @throws {Error} If the `decode_chain` method is not implemented in the subclass.\n     */\n    decode_chain(tokens) {\n        throw Error(\"`decode_chain` should be implemented in subclass.\")\n    }\n\n}\n\nclass ReplaceDecoder extends Decoder {\n    constructor(config) {\n        super(config);\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        let pattern = createPattern(this.config.pattern);\n        if (pattern === null) {\n            return tokens;\n        }\n\n        return tokens.map(token => token.replaceAll(pattern, this.config.content))\n    }\n}\n\n\nclass ByteFallback extends Decoder {\n    constructor(config) {\n        super(config);\n\n        this.text_decoder = new TextDecoder();\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n\n        let new_tokens = [];\n        let previous_byte_tokens = [];\n\n        for (let token of tokens) {\n            let bytes = null;\n            if (token.length === 6 && token.startsWith('<0x') && token.endsWith('>')) {\n                let byte = parseInt(token.slice(3, 5), 16);\n                if (!isNaN(byte)) {\n                    bytes = byte;\n                }\n            }\n            if (bytes !== null) {\n                previous_byte_tokens.push(bytes);\n            } else {\n                if (previous_byte_tokens.length > 0) {\n                    let string = this.text_decoder.decode(Uint8Array.from(previous_byte_tokens));\n                    new_tokens.push(string);\n                    previous_byte_tokens = [];\n                }\n                new_tokens.push(token);\n            }\n        }\n        if (previous_byte_tokens.length > 0) {\n            let string = this.text_decoder.decode(Uint8Array.from(previous_byte_tokens));\n            new_tokens.push(string);\n            previous_byte_tokens = [];\n        }\n\n        return new_tokens;\n    }\n}\n\n/**\n * Fuse simply fuses all tokens into one big string.\n * It's usually the last decoding step anyway, but this decoder\n * exists incase some decoders need to happen after that step\n */\nclass FuseDecoder extends Decoder {\n    constructor(config) {\n        super(config);\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return [tokens.join('')];\n    }\n}\n\nclass StripDecoder extends Decoder {\n    constructor(config) {\n        super(config);\n\n        this.content = this.config.content;\n        this.start = this.config.start;\n        this.stop = this.config.stop;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return tokens.map(token => {\n            let start_cut = 0;\n            for (let i = 0; i < this.start; ++i) {\n                if (token[i] === this.content) {\n                    start_cut = i + 1;\n                    continue;\n                } else {\n                    break;\n                }\n            }\n\n            let stop_cut = token.length;\n            for (let i = 0; i < this.stop; ++i) {\n                const index = token.length - i - 1;\n                if (token[index] === this.content) {\n                    stop_cut = index;\n                    continue;\n                } else {\n                    break;\n                }\n            }\n\n            return token.slice(start_cut, stop_cut)\n        });\n    }\n}\n\n/**\n * A decoder that decodes a list of WordPiece tokens into a single string.\n * @extends Decoder\n */\nclass WordPieceDecoder extends Decoder {\n\n    /**\n     * Creates a new instance of WordPieceDecoder.\n     * @param {Object} config The configuration object.\n     * @param {string} config.prefix The prefix used for WordPiece encoding.\n     * @param {boolean} config.cleanup Whether to cleanup the decoded string.\n     */\n    constructor(config) {\n        super(config);\n        this.cleanup = config.cleanup;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return tokens.map((token, i) => {\n            if (i !== 0) {\n                if (token.startsWith(this.config.prefix)) {\n                    // NOTE: .replace() is intended; only replace first occurrence\n                    token = token.replace(this.config.prefix, '');\n                } else {\n                    token = ' ' + token;\n                }\n            }\n            if (this.cleanup) {\n                token = clean_up_tokenization(token)\n            }\n\n            return token;\n        });\n    }\n}\n\n/**\n * Byte-level decoder for tokenization output. Inherits from the `Decoder` class.\n * @extends Decoder\n */\nclass ByteLevelDecoder extends Decoder {\n\n    /**\n     * Create a `ByteLevelDecoder` object.\n     * @param {Object} config Configuration object.\n     */\n    constructor(config) {\n        super(config);\n\n        this.byte_decoder = UNICODE_TO_BYTES;\n        this.text_decoder = new TextDecoder(\"utf-8\", {\n            fatal: false,\n            ignoreBOM: true,\n        });\n\n        this.end_of_word_suffix = null;\n    }\n\n    /**\n     * Convert an array of tokens to string by decoding each byte.\n     * @param {string[]} tokens Array of tokens to be decoded.\n     * @returns {string} The decoded string.\n     */\n    convert_tokens_to_string(tokens) {\n        let text = tokens.join('');\n\n        let byteArray = new Uint8Array([...text].map(c => this.byte_decoder[c]));\n        let decoded_text = this.text_decoder.decode(byteArray);\n        return decoded_text;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        // TODO move to base class (like HF)\n        // tokens === filtered_tokens\n\n        // To avoid mixing byte-level and unicode for byte-level BPT\n        // we need to build string separately for added tokens and byte-level tokens\n        // cf. https://github.com/huggingface/transformers/issues/1133\n        let sub_texts = [];\n        let current_sub_text = [];\n        for (let token of tokens) {\n            // tokens sent here are already filtered, so we don't need to do this\n            // if (skip_special_tokens && this.all_special_ids.includes(token)) {\n            //     continue;\n            // }\n\n            if (this.added_tokens.includes(token)) {\n                if (current_sub_text.length > 0) {\n                    sub_texts.push(this.convert_tokens_to_string(current_sub_text));\n                    current_sub_text = [];\n                }\n                sub_texts.push(token);\n            } else {\n                current_sub_text.push(token);\n            }\n        }\n        if (current_sub_text.length > 0) {\n            sub_texts.push(this.convert_tokens_to_string(current_sub_text));\n        }\n\n        // TODO add spaces_between_special_tokens and clean_up_tokenization_spaces options\n\n        return sub_texts;\n    }\n}\n\n\n/**\n * Apply a sequence of decoders.\n * @extends Decoder\n */\nclass DecoderSequence extends Decoder {\n\n    /**\n     * Creates a new instance of DecoderSequence.\n     * @param {Object} config The configuration object.\n     * @param {Decoder[]} config.decoders The list of decoders to apply.\n     */\n    constructor(config) {\n        super(config);\n        this.decoders = config.decoders.map(x => Decoder.fromConfig(x));\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        // Use reduce to apply each decoder to the tokens\n        return this.decoders.reduce((toks, decoder) => {\n            return decoder.decode_chain(toks);\n        }, tokens);\n    }\n\n}\n\n/**\n * This PreTokenizer replaces spaces with the given replacement character, adds a prefix space if requested,\n * and returns a list of tokens.\n * @extends PreTokenizer\n */\nclass MetaspacePreTokenizer extends PreTokenizer {\n    /**\n     * @param {Object} config The configuration object for the MetaspacePreTokenizer.\n     * @param {boolean} config.add_prefix_space Whether to add a prefix space to the first token.\n     * @param {string} config.replacement The character to replace spaces with.\n     * @param {string} [config.str_rep=config.replacement] An optional string representation of the replacement character.\n     */\n    constructor(config) {\n        super();\n\n        this.addPrefixSpace = config.add_prefix_space;\n        this.replacement = config.replacement;\n        this.strRep = config.str_rep || this.replacement;\n    }\n\n    /**\n     * This method takes a list of normalized tokens, replaces spaces with the replacement character,\n     * adds a prefix space if requested, and returns a new list of tokens.\n     * @param {string[]|string} normalizedTokens The list of normalized tokens to pre-tokenize.\n     * @returns {string[]} A new list of pre-tokenized tokens.\n     */\n    pre_tokenize(normalizedTokens) {\n        if (typeof normalizedTokens === 'string') {\n            // Metaspace acts on a list of tokens. If passing in a string, first split on whitespace\n            // NOTE: For some reason, metaspace includes trailing whitespace, so we only trim leading whitespace.\n            // See: https://github.com/huggingface/tokenizers/issues/1250\n            normalizedTokens = normalizedTokens.trimStart().split(/\\s+/);\n        }\n\n        const result = [];\n        for (let token of normalizedTokens) {\n            let normalized = token.replaceAll(' ', this.strRep);\n            if (this.addPrefixSpace && !normalized.startsWith(this.replacement)) {\n                normalized = this.strRep + normalized;\n            }\n            result.push(normalized);\n        }\n        return result;\n    }\n}\n\n/**\n * MetaspaceDecoder class extends the Decoder class and decodes Metaspace tokenization.\n * @extends Decoder\n */\nclass MetaspaceDecoder extends Decoder {\n    /**\n     * Constructs a new MetaspaceDecoder object.\n     * @param {Object} config The configuration object for the MetaspaceDecoder.\n     * @param {boolean} config.add_prefix_space Whether to add a prefix space to the decoded string.\n     * @param {string} config.replacement The string to replace spaces with.\n     */\n    constructor(config) {\n        super(config);\n\n        this.addPrefixSpace = config.add_prefix_space;\n        this.replacement = config.replacement;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        let result = [];\n        for (let i = 0; i < tokens.length; ++i) {\n            let normalized = tokens[i].replaceAll(this.replacement, ' ');\n            if (this.addPrefixSpace && i == 0 && normalized.startsWith(' ')) {\n                normalized = normalized.substring(1);\n            }\n            result.push(normalized);\n        }\n        return result;\n    }\n}\n\n/**\n * A normalizer that applies a precompiled charsmap.\n * This is useful for applying complex normalizations in C++ and exposing them to JavaScript.\n * @extends Normalizer\n * @param {Object} config The configuration object for the Precompiled normalizer.\n * @param {Object} config.precompiled_charsmap The precompiled charsmap object.\n */\nclass Precompiled extends Normalizer {\n    /**\n     * Create a new instance of Precompiled normalizer.\n     * @param {Object} config The configuration object.\n     * @param {any} config.precompiled_charsmap Precompiled chars mapping.\n     */\n    constructor(config) {\n        super(config);\n        this.charsmap = config.precompiled_charsmap;\n    }\n\n    /**\n     * Normalizes the given text by applying the precompiled charsmap.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        // TODO use this.charsmap\n        return text;\n    }\n}\n\n/**\n * A pre-tokenizer that applies a sequence of pre-tokenizers to the input text.\n * @extends PreTokenizer\n */\nclass PreTokenizerSequence extends PreTokenizer {\n    /**\n     * Creates an instance of PreTokenizerSequence.\n     * @param {Object} config The configuration object for the pre-tokenizer sequence.\n     * @param {Object[]} config.pretokenizers An array of pre-tokenizer configurations.\n     */\n    constructor(config) {\n        super();\n        this.tokenizers = config.pretokenizers.map(x => PreTokenizer.fromConfig(x));\n    }\n\n    /**\n     * Applies each pre-tokenizer in the sequence to the input text in turn.\n     * @param {string|string[]} text The text(s) to pre-tokenize.\n     * @returns {string[]} The pre-tokenized text.\n     */\n    pre_tokenize_text(text) {\n        if (typeof text === 'string') {\n            text = [text];\n        }\n        // Use reduce to apply each tokenizer to the text\n        return this.tokenizers.reduce((preTokenizedText, tokenizer) => {\n            return tokenizer.pre_tokenize(preTokenizedText);\n        }, text);\n    }\n}\n\n/**\n * Splits a string of text by whitespace characters into individual tokens.\n * @extends PreTokenizer\n */\nclass WhitespaceSplit extends PreTokenizer {\n    /**\n     * Creates an instance of WhitespaceSplit.\n     * @param {Object} config The configuration object for the pre-tokenizer sequence.\n     */\n    constructor(config) {\n        super();\n    }\n    /**\n     * Pre-tokenizes the input text by splitting it on whitespace characters.\n     * @param {string} text The text to be pre-tokenized.\n     * @returns {string[]} An array of tokens produced by splitting the input text on whitespace.\n     */\n    pre_tokenize_text(text) {\n        return whitespace_split(text);\n    }\n}\n\nexport class PreTrainedTokenizer extends Callable {\n    /**\n     * Create a new PreTrainedTokenizer instance.\n     * @param {Object} tokenizerJSON The JSON of the tokenizer.\n     * @param {Object} tokenizerConfig The config of the tokenizer.\n     */\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super();\n\n        // Construct parts of the tokenizer from the JSON\n        this.normalizer = Normalizer.fromConfig(tokenizerJSON.normalizer);\n        this.pre_tokenizer = PreTokenizer.fromConfig(tokenizerJSON.pre_tokenizer);\n\n        // Convert the vocabulary to a map, if it exists\n        if (tokenizerJSON.model.vocab) {\n            if (!Array.isArray(tokenizerJSON.model.vocab)) {\n                tokenizerJSON.model.vocab = Object.entries(tokenizerJSON.model.vocab);\n            }\n            tokenizerJSON.model.vocab = new Map(tokenizerJSON.model.vocab);\n        }\n        this.model = TokenizerModel.fromConfig(tokenizerJSON.model, tokenizerConfig);\n        this.post_processor = PostProcessor.fromConfig(tokenizerJSON.post_processor);\n\n        // TODO: maybe, allow this to be null; in which case, we use model as decoder too?\n        this.decoder = Decoder.fromConfig(tokenizerJSON.decoder);\n\n\n        // Another slight hack to add `end_of_word_suffix` (if present) to the decoder\n        // This is needed for cases where BPE model and ByteLevel decoder are used\n        // For more information, see https://github.com/xenova/transformers.js/issues/74\n        // TODO: save this to the decoder when exporting?\n        this.decoder.end_of_word_suffix = this.model.end_of_word_suffix;\n\n        // Add added_tokens to model\n        this.special_tokens = [];\n        this.all_special_ids = [];\n        this.added_tokens = [];\n        for (let addedToken of tokenizerJSON.added_tokens) {\n            let id = addedToken.id;\n            let content = addedToken.content;\n\n            this.added_tokens.push(content);\n\n            this.model.tokens_to_ids.set(content, id);\n            this.model.vocab[id] = content;\n\n            if (addedToken.special) {\n                this.special_tokens.push(content);\n                this.all_special_ids.push(id);\n            }\n        }\n\n        // Slight hack, but it prevents code duplication:\n        this.decoder.added_tokens = this.added_tokens;\n\n        this.added_tokens_regex = new RegExp(\n            '(' + this.added_tokens.map(escapeRegExp).join('|') + ')'\n        );\n\n        // Set mask token if present (otherwise will be undefined, which is fine)\n        this.mask_token = this.getToken(tokenizerConfig, 'mask_token');\n        this.mask_token_id = this.model.tokens_to_ids.get(this.mask_token);\n\n        this.pad_token = this.getToken(tokenizerConfig, 'pad_token', 'eos_token');\n        this.pad_token_id = this.model.tokens_to_ids.get(this.pad_token);\n\n        this.sep_token = this.getToken(tokenizerConfig, 'sep_token');\n        this.sep_token_id = this.model.tokens_to_ids.get(this.sep_token);\n\n        this.model_max_length = tokenizerConfig.model_max_length;\n\n        /** @type {boolean} Whether or not to strip the text when tokenizing (removing excess spaces before and after the string). */\n        this.remove_space = tokenizerConfig.remove_space;\n\n        this.clean_up_tokenization_spaces = tokenizerConfig.clean_up_tokenization_spaces ?? true;\n\n        // TODO allow user to change this\n        this.padding_side = 'right';\n    }\n\n    /**\n     * Returns the value of the first matching key in the tokenizer config object.\n     * @param {...string} keys One or more keys to search for in the tokenizer config object.\n     * @returns {string|null} The value associated with the first matching key, or null if no match is found.\n     * @throws {Error} If an object is found for a matching key and its __type property is not \"AddedToken\".\n     */\n    getToken(tokenizerConfig, ...keys) {\n        for (let key of keys) {\n            let item = tokenizerConfig[key];\n\n            if (!item) continue;\n\n            if (typeof item === 'object') {\n                if (item.__type === 'AddedToken') {\n                    return item.content;\n                } else {\n                    throw Error(`Unknown token: ${item}`);\n                }\n            } else {\n                return item;\n            }\n        }\n        return null;\n    }\n\n    /**\n     * Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`. \n     * \n     * @param {string} pretrained_model_name_or_path The path to the pre-trained tokenizer.\n     * @param {PretrainedOptions} options Additional options for loading the tokenizer.\n     * \n     * @throws {Error} Throws an error if the tokenizer.json or tokenizer_config.json files are not found in the `pretrained_model_name_or_path`.\n     * @returns {Promise<PreTrainedTokenizer>} A new instance of the `PreTrainedTokenizer` class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n    } = {}) {\n\n        let info = await loadTokenizer(pretrained_model_name_or_path, {\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n        })\n\n        // @ts-ignore\n        return new this(...info);\n    }\n\n    /**\n     * This function can be overridden by a subclass to apply additional preprocessing\n     * to a model's input data.\n     * @param {Object} inputs An object containing input data as properties.\n     * @returns {Object} The modified inputs object.\n     */\n    prepare_model_inputs(inputs) {\n        return inputs;\n    }\n\n    /**\n     * Encode/tokenize the given text(s).\n     * @param {string|string[]} text The text to tokenize.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {string|string[]} [options.text_pair=null] Optional second sequence to be encoded. If set, must be the same type as text.\n     * @param {boolean} [options.padding=false] Whether to pad the input sequences.\n     * @param {boolean} [options.truncation=null] Whether to truncate the input sequences.\n     * @param {number} [options.max_length=null] Maximum length of the returned list and optionally padding length.\n     * @param {boolean} [options.return_tensor=true] Whether to return the results as Tensors or arrays.\n     * @returns {{ input_ids: number[]|number[][]|Tensor, attention_mask: any[]|Tensor }} Object to be passed to the model.\n     */\n    _call(\n        // Required positional arguments\n        text,\n\n        // Optional keyword arguments\n        {\n            text_pair = null,\n            // add_special_tokens = true, // TODO\n            padding = false,\n            truncation = null,\n            max_length = null,\n            return_tensor = true, // Different to HF\n        } = {},\n    ) {\n\n        /** @type {number[]|number[][]|Tensor} */\n        let tokens;\n\n        if (Array.isArray(text)) {\n            if (text.length === 0) {\n                throw Error('text array must be non-empty')\n            }\n\n            if (text_pair !== null) {\n                if (!Array.isArray(text_pair)) {\n                    throw Error('text_pair must also be an array')\n\n                } else if (text.length !== text_pair.length) {\n                    throw Error('text and text_pair must have the same length')\n                }\n\n                tokens = text.map(\n                    (t, i) => this.encode(t, text_pair[i])\n                )\n\n            } else {\n                tokens = text.map(x => this.encode(x));\n            }\n\n        } else {\n            if (text === null) {\n                throw Error('text may not be null')\n            }\n\n            if (Array.isArray(text_pair)) {\n                throw Error('When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).')\n            }\n\n            // For single input, we just wrap in an array, and then unwrap later.\n            tokens = [this.encode(text, text_pair)];\n        }\n        // At this point, tokens is batched: [batch_size, tokens]\n        // However, array may be jagged. So, we pad to max_length\n\n        let maxLengthOfBatch = max(tokens.map(x => x.length))[0];\n\n        // If null, we calculate max length from sequences\n        if (max_length === null) {\n            max_length = maxLengthOfBatch;\n        }\n\n        // Ensure it is less than model max length\n        max_length = Math.min(max_length, this.model_max_length)\n\n        /** @type {any[]|Tensor} */\n        let attention_mask = [];\n        if (padding || truncation) {\n            // Perform padding and/or truncation\n            for (let i = 0; i < tokens.length; ++i) {\n                if (tokens[i].length === max_length) {\n                    attention_mask.push(new Array(tokens[i].length).fill(1))\n                    continue;\n\n                } else if (tokens[i].length > max_length) {\n                    // possibly truncate\n                    if (truncation) {\n                        tokens[i] = tokens[i].slice(0, max_length);\n                    }\n                    attention_mask.push(new Array(tokens[i].length).fill(1))\n\n                } else { // t.length < max_length\n                    if (padding) {\n                        let diff = max_length - tokens[i].length;\n\n                        if (this.padding_side === 'right') {\n                            attention_mask.push(\n                                (new Array(tokens[i].length).fill(1)).concat(new Array(diff).fill(0))\n                            )\n                            tokens[i].push(...new Array(diff).fill(this.pad_token_id))\n                        } else { // left\n                            attention_mask.push(\n                                (new Array(diff).fill(0)).concat(new Array(tokens[i].length).fill(1))\n                            )\n                            tokens[i].unshift(...new Array(diff).fill(this.pad_token_id))\n                        }\n\n                    } else {\n                        attention_mask.push(new Array(tokens[i].length).fill(1))\n                    }\n                }\n            }\n        } else {\n            attention_mask = tokens.map(x => new Array(x.length).fill(1))\n        }\n\n        if (return_tensor) {\n            if (!(padding && truncation)) {\n                // Not, guaranteed that all items have same length, so\n                // we perform additional check\n\n                if (tokens.some(x => x.length !== tokens[0].length)) {\n                    throw Error(\n                        \"Unable to create tensor, you should probably activate truncation and/or padding \" +\n                        \"with 'padding=true' and 'truncation=true' to have batched tensors with the same length.\"\n                    )\n                }\n            }\n\n            // Now we actually convert to tensor\n            // NOTE: In the same way as the python library, we return a batched tensor, regardless of\n            // whether we have a single input or multiple inputs.\n            let dims = [tokens.length, tokens[0].length];\n\n            tokens = new Tensor('int64',\n                BigInt64Array.from(tokens.flat().map(BigInt)),\n                dims\n            );\n\n            attention_mask = new Tensor(\n                'int64',\n                BigInt64Array.from(attention_mask.flat().map(BigInt)),\n                dims\n            )\n        } else {\n            // If not returning a tensor, we match the input type\n            if (!Array.isArray(text)) {\n                // Input was not batched, so we unwrap\n                tokens = tokens[0];\n                attention_mask = attention_mask[0];\n            }\n        }\n\n\n        // Finally, add attention mask, and possibly model-specific parameters\n        let modelInputs = {\n            input_ids: tokens,\n            attention_mask: attention_mask\n        }\n\n        // Optional post-processing\n        modelInputs = this.prepare_model_inputs(modelInputs);\n\n        return modelInputs\n    }\n\n    /**\n     * Encodes a single text using the preprocessor pipeline of the tokenizer.\n     *\n     * @param {string|null} text The text to encode.\n     * @returns {Array} The encoded tokens.\n     */\n    _encode_text(text) {\n        if (text === null) return null;\n\n        // Actual function which does encoding, for a single text\n        // First, we take care of special tokens. Needed to avoid issues arising from\n        // normalization and/or pretokenization (which may not preserve special tokens)\n        const sections = text.split(this.added_tokens_regex).filter(x => x);\n\n        let tokens = sections.map(x => {\n            if (this.added_tokens.includes(x)) {\n                // Ignore added tokens\n                return x\n            } else {\n                if (this.remove_space === true) {\n                    x = x.trim().split(/\\s+/).join(' ');\n                }\n\n                if (this.normalizer !== null) {\n                    x = this.normalizer(x);\n                }\n\n                let sectionTokens = (this.pre_tokenizer !== null) ? this.pre_tokenizer(x) : [x];\n\n                let tokens = this.model(sectionTokens);\n\n                return tokens;\n            }\n        }).flat();\n\n        return tokens;\n    }\n\n    /**\n     * Encodes a single text or a pair of texts using the model's tokenizer.\n     *\n     * @param {string} text The text to encode.\n     * @param {string|null} text_pair The optional second text to encode.\n     * @returns {number[]} An array of token IDs representing the encoded text(s).\n     */\n    encode(text, text_pair = null) {\n        // Function called by users to encode possibly multiple texts\n        let tokens = this._encode_text(text);\n        let tokens2 = this._encode_text(text_pair);\n\n        let combinedTokens = this.post_processor(tokens, tokens2);\n        let ids = this.model.convert_tokens_to_ids(combinedTokens);\n\n        return ids\n    }\n\n    /**\n     * Decode a batch of tokenized sequences.\n     * @param {number[][]} batch List of tokenized input sequences.\n     * @param {Object} decode_args (Optional) Object with decoding arguments.\n     * @returns {string[]} List of decoded sequences.\n     */\n    batch_decode(batch, decode_args = {}) {\n        return batch.map(x => this.decode(x, decode_args));\n    }\n\n    /**\n     * Decodes a sequence of token IDs back to a string.\n     *\n     * @param {number[]} token_ids List of token IDs to decode.\n     * @param {Object} [decode_args={}]\n     * @param {boolean} [decode_args.skip_special_tokens=false] If true, special tokens are removed from the output string.\n     * @param {boolean} [decode_args.clean_up_tokenization_spaces=true] If true, spaces before punctuations and abbreviated forms are removed.\n     *\n     * @returns {string} The decoded string.\n     * @throws {Error} If `token_ids` is not a non-empty array of integers.\n     */\n    decode(\n        token_ids,\n        decode_args = {},\n    ) {\n        if (!Array.isArray(token_ids) || token_ids.length === 0 || !isIntegralNumber(token_ids[0])) {\n            throw Error(\"token_ids must be a non-empty array of integers.\");\n        }\n\n        return this.decode_single(token_ids, decode_args)\n    }\n\n    /**\n     * Decode a single list of token ids to a string.\n     * @param {number[]} token_ids List of token ids to decode\n     * @param {Object} decode_args Optional arguments for decoding\n     * @param {boolean} [decode_args.skip_special_tokens=false] Whether to skip special tokens during decoding\n     * @param {boolean} [decode_args.clean_up_tokenization_spaces=null] Whether to clean up tokenization spaces during decoding.\n     * If null, the value is set to `this.decoder.cleanup` if it exists, falling back to `this.clean_up_tokenization_spaces` if it exists, falling back to `true`.\n     * @returns {string} The decoded string\n     */\n    decode_single(\n        token_ids,\n        {\n            skip_special_tokens = false,\n            clean_up_tokenization_spaces = null,\n        }\n    ) {\n        let tokens = this.model.convert_ids_to_tokens(token_ids);\n        if (skip_special_tokens) {\n            tokens = tokens.filter(x => !this.special_tokens.includes(x));\n        }\n\n        /** @type {string} */\n        let decoded = this.decoder(tokens);\n\n\n        // Slight hack, but prevents having to pass `skip_special_tokens` to\n        // each call to `decode`, which would lead to code duplication.\n        if (this.decoder.end_of_word_suffix) {\n            decoded = decoded.replaceAll(this.decoder.end_of_word_suffix, ' ');\n            if (skip_special_tokens) {\n                decoded = decoded.trim();\n            }\n        }\n\n        if (clean_up_tokenization_spaces ?? this.clean_up_tokenization_spaces) {\n            decoded = clean_up_tokenization(decoded);\n        }\n\n        return decoded;\n    }\n\n}\n\n/**\n* Helper method for added `token_type_ids` to model inputs\n* @param {Object} inputs An object containing the input ids and attention mask.\n* @returns {Object} The prepared inputs object.\n*/\nfunction add_token_types(inputs) {\n    if (inputs.input_ids instanceof Tensor) {\n        inputs.token_type_ids = new Tensor(\n            'int64',\n            new BigInt64Array(inputs.input_ids.data.length),\n            inputs.input_ids.dims\n        )\n    } else if (Array.isArray(inputs.input_ids)) {\n\n        if (Array.isArray(inputs.input_ids[0])) {\n            // This means input is batched, so we need to batch the token_type_ids as well\n            inputs.token_type_ids = inputs.input_ids.map(\n                x => new Array(x.length).fill(0)\n            )\n        } else {\n            inputs.token_type_ids = new Array(inputs.input_ids.length).fill(0);\n        }\n    } else {\n        throw new Error('Input ids must be a Tensor or an Array')\n    }\n\n    return inputs;\n}\n\n/**\n * BertTokenizer is a class used to tokenize text for BERT models.\n * @extends PreTrainedTokenizer\n */\nexport class BertTokenizer extends PreTrainedTokenizer {\n    /** @see {@link add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\n/**\n * Albert tokenizer\n * @extends PreTrainedTokenizer\n */\nexport class AlbertTokenizer extends PreTrainedTokenizer {\n    /** @see {@link add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class MobileBertTokenizer extends PreTrainedTokenizer {\n    /** @see {@link add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class SqueezeBertTokenizer extends PreTrainedTokenizer {\n    /** @see {@link add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class DistilBertTokenizer extends PreTrainedTokenizer { }\nexport class T5Tokenizer extends PreTrainedTokenizer { }\nexport class GPT2Tokenizer extends PreTrainedTokenizer { }\nexport class BartTokenizer extends PreTrainedTokenizer { }\nexport class RobertaTokenizer extends PreTrainedTokenizer { }\n\nexport class BloomTokenizer extends PreTrainedTokenizer { }\nexport class LlamaTokenizer extends PreTrainedTokenizer {\n    /** @see {@link add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\n/**\n * The NllbTokenizer class is used to tokenize text for NLLB (\"No Language Left Behind\") models.\n * \n * No Language Left Behind (NLLB) is a first-of-its-kind, AI breakthrough project\n * that open-sources models capable of delivering high-quality translations directly\n * between any pair of 200+ languages — including low-resource languages like Asturian,\n * Luganda, Urdu and more. It aims to help people communicate with anyone, anywhere,\n * regardless of their language preferences. For more information, check out their\n * [paper](https://arxiv.org/abs/2207.04672).\n * \n * For a list of supported languages (along with their language codes),\n * @see {@link https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200}\n */\nexport class NllbTokenizer extends PreTrainedTokenizer {\n\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n\n        this.languageRegex = /^[a-z]{3}_[A-Z][a-z]{3}$/;\n        this.language_codes = this.special_tokens.filter(x => this.languageRegex.test(x));\n    }\n\n    /**\n     * Helper function to build translation inputs for an `NllbTokenizer`.\n     * @param {string|string[]} raw_inputs The text to tokenize.\n     * @param {Object} tokenizer_options Options to be sent to the tokenizer\n     * @param {Object} generate_kwargs Generation options.\n     * @returns {Object} Object to be passed to the model.\n     */\n    _build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs) {\n\n\n        // Check that the target language is valid:\n        if (!this.language_codes.includes(generate_kwargs.tgt_lang)) {\n            throw new Error(`Target language code \"${generate_kwargs.tgt_lang}\" is not valid. Must be one of: {${this.language_codes.join(', ')}}`);\n        }\n\n        // Allow `src_lang` to be optional. If not set, we'll use the tokenizer's default.\n        if (generate_kwargs.src_lang !== undefined) {\n            // Check that the source language is valid:\n            if (!this.language_codes.includes(generate_kwargs.src_lang)) {\n                throw new Error(`Source language code \"${generate_kwargs.src_lang}\" is not valid. Must be one of: {${this.language_codes.join(', ')}}`);\n            }\n\n            // In the same way as the Python library, we override the post-processor\n            // to force the source language to be first:\n            for (let item of this.post_processor.config.single) {\n                if ('SpecialToken' in item && this.languageRegex.test(item.SpecialToken.id)) {\n                    item.SpecialToken.id = generate_kwargs.src_lang;\n                    break;\n                }\n            }\n        }\n\n        // Override the `forced_bos_token_id` to force the correct language\n        generate_kwargs.forced_bos_token_id = this.model.convert_tokens_to_ids([generate_kwargs.tgt_lang])[0];\n\n        return this._call(raw_inputs, tokenizer_options);\n    }\n}\n\n\nconst WHISPER_LANGUAGES = [\n    [\"en\", \"english\"],\n    [\"zh\", \"chinese\"],\n    [\"de\", \"german\"],\n    [\"es\", \"spanish\"],\n    [\"ru\", \"russian\"],\n    [\"ko\", \"korean\"],\n    [\"fr\", \"french\"],\n    [\"ja\", \"japanese\"],\n    [\"pt\", \"portuguese\"],\n    [\"tr\", \"turkish\"],\n    [\"pl\", \"polish\"],\n    [\"ca\", \"catalan\"],\n    [\"nl\", \"dutch\"],\n    [\"ar\", \"arabic\"],\n    [\"sv\", \"swedish\"],\n    [\"it\", \"italian\"],\n    [\"id\", \"indonesian\"],\n    [\"hi\", \"hindi\"],\n    [\"fi\", \"finnish\"],\n    [\"vi\", \"vietnamese\"],\n    [\"he\", \"hebrew\"],\n    [\"uk\", \"ukrainian\"],\n    [\"el\", \"greek\"],\n    [\"ms\", \"malay\"],\n    [\"cs\", \"czech\"],\n    [\"ro\", \"romanian\"],\n    [\"da\", \"danish\"],\n    [\"hu\", \"hungarian\"],\n    [\"ta\", \"tamil\"],\n    [\"no\", \"norwegian\"],\n    [\"th\", \"thai\"],\n    [\"ur\", \"urdu\"],\n    [\"hr\", \"croatian\"],\n    [\"bg\", \"bulgarian\"],\n    [\"lt\", \"lithuanian\"],\n    [\"la\", \"latin\"],\n    [\"mi\", \"maori\"],\n    [\"ml\", \"malayalam\"],\n    [\"cy\", \"welsh\"],\n    [\"sk\", \"slovak\"],\n    [\"te\", \"telugu\"],\n    [\"fa\", \"persian\"],\n    [\"lv\", \"latvian\"],\n    [\"bn\", \"bengali\"],\n    [\"sr\", \"serbian\"],\n    [\"az\", \"azerbaijani\"],\n    [\"sl\", \"slovenian\"],\n    [\"kn\", \"kannada\"],\n    [\"et\", \"estonian\"],\n    [\"mk\", \"macedonian\"],\n    [\"br\", \"breton\"],\n    [\"eu\", \"basque\"],\n    [\"is\", \"icelandic\"],\n    [\"hy\", \"armenian\"],\n    [\"ne\", \"nepali\"],\n    [\"mn\", \"mongolian\"],\n    [\"bs\", \"bosnian\"],\n    [\"kk\", \"kazakh\"],\n    [\"sq\", \"albanian\"],\n    [\"sw\", \"swahili\"],\n    [\"gl\", \"galician\"],\n    [\"mr\", \"marathi\"],\n    [\"pa\", \"punjabi\"],\n    [\"si\", \"sinhala\"],\n    [\"km\", \"khmer\"],\n    [\"sn\", \"shona\"],\n    [\"yo\", \"yoruba\"],\n    [\"so\", \"somali\"],\n    [\"af\", \"afrikaans\"],\n    [\"oc\", \"occitan\"],\n    [\"ka\", \"georgian\"],\n    [\"be\", \"belarusian\"],\n    [\"tg\", \"tajik\"],\n    [\"sd\", \"sindhi\"],\n    [\"gu\", \"gujarati\"],\n    [\"am\", \"amharic\"],\n    [\"yi\", \"yiddish\"],\n    [\"lo\", \"lao\"],\n    [\"uz\", \"uzbek\"],\n    [\"fo\", \"faroese\"],\n    [\"ht\", \"haitian creole\"],\n    [\"ps\", \"pashto\"],\n    [\"tk\", \"turkmen\"],\n    [\"nn\", \"nynorsk\"],\n    [\"mt\", \"maltese\"],\n    [\"sa\", \"sanskrit\"],\n    [\"lb\", \"luxembourgish\"],\n    [\"my\", \"myanmar\"],\n    [\"bo\", \"tibetan\"],\n    [\"tl\", \"tagalog\"],\n    [\"mg\", \"malagasy\"],\n    [\"as\", \"assamese\"],\n    [\"tt\", \"tatar\"],\n    [\"haw\", \"hawaiian\"],\n    [\"ln\", \"lingala\"],\n    [\"ha\", \"hausa\"],\n    [\"ba\", \"bashkir\"],\n    [\"jw\", \"javanese\"],\n    [\"su\", \"sundanese\"],\n]\n\n// @ts-ignore\nconst WHISPER_LANGUAGE_MAPPING = new Map(WHISPER_LANGUAGES);\n// @ts-ignore\nconst WHISPER_TO_LANGUAGE_CODE_MAPPING = new Map([\n    ...WHISPER_LANGUAGES.map(([k, v]) => [v, k]),\n    ...[\n        [\"burmese\", \"my\"],\n        [\"valencian\", \"ca\"],\n        [\"flemish\", \"nl\"],\n        [\"haitian\", \"ht\"],\n        [\"letzeburgesch\", \"lb\"],\n        [\"pushto\", \"ps\"],\n        [\"panjabi\", \"pa\"],\n        [\"moldavian\", \"ro\"],\n        [\"moldovan\", \"ro\"],\n        [\"sinhalese\", \"si\"],\n        [\"castilian\", \"es\"],\n    ]\n]);\n\n/**\n * WhisperTokenizer tokenizer\n * @extends PreTrainedTokenizer\n */\nexport class WhisperTokenizer extends PreTrainedTokenizer {\n\n    /**\n     * Decodes automatic speech recognition (ASR) sequences.\n     * @param {Array<{tokens: number[], stride: number[]}>} sequences The sequences to decode.\n     * @param {Object} options The options to use for decoding.\n     * @returns {Array<string|{chunks?: undefined|Array<{language: string|null, timestamp: Array<number|null>, text: string}>}>} The decoded sequences.\n     */\n    _decode_asr(sequences, {\n        return_timestamps = false,\n        return_language = false,\n        time_precision = null,\n        force_full_sequences = true\n    } = {}) {\n        // Set force_full_sequences=false if you want streaming\n        // TODO add support for `return_language`\n\n        // Internal method meant to only be used by asr pipeline.\n        // Handles all the little quirks specific to whisper to handle\n        // the various options not allowed in other seq2seq models\n\n        // =========== Overview ============\n        // - iterate over all outputs\n        // - all tokens within output\n        // - Each token can be\n        //   - language token\n        //   - special token\n        //   - timestamp token\n        //   - text token\n        // - We accumulate the text tokens.\n        // - We split on end timestamps\n        // - Lots of complexity comes from stride and timestamps\n\n        if (time_precision === null) {\n            throw Error(\"Must specify time_precision\")\n        }\n        let last_language = null;\n\n        function new_chunk() {\n            return { \"language\": last_language, \"timestamp\": [null, null], \"text\": \"\" };\n        }\n\n        // Welcome to the state machine!\n        const chunks = [];\n        let chunk = new_chunk();\n        let time_offset = 0.0;\n        const timestamp_begin = this.model.convert_tokens_to_ids([\"<|notimestamps|>\"])[0] + 1;\n\n        let previous_tokens = [];\n        let skip = false;\n        let right_stride_start = null;\n\n\n        const all_special_ids = new Set(this.all_special_ids);\n\n        for (let output of sequences) {\n            // NOTE: python version has batches, so it uses [0]\n            const token_ids = output.tokens;\n\n            // These keep track of timestamps within strides, which need\n            // to be skipped and resolve all tokens in a single chunk.\n            let last_timestamp = null;\n            let first_timestamp = timestamp_begin;\n\n            if (\"stride\" in output) {\n                const [chunk_len, stride_left, stride_right] = output.stride;\n\n                // Offset the timings to account for the other `model_outputs`.\n                time_offset -= stride_left;\n                right_stride_start = chunk_len - stride_right;\n\n                // Keeping track of timestamps within strides\n                // We're going to NOT split on those, and delay until we're\n                // out of BOTH stride. Otherwise lots of issues occur and\n                // corner cases\n                if (stride_left) {\n                    first_timestamp = stride_left / time_precision + timestamp_begin;\n                }\n\n                if (stride_right) {\n                    for (let i = token_ids.length - 1; i >= 0; --i) {\n                        const token = token_ids[i];\n                        if (token >= timestamp_begin) {\n                            // There can be several token in the right stride\n                            // But the last one is ALWAYS going to be skipped\n                            if (last_timestamp !== null && (token - timestamp_begin) * time_precision < right_stride_start) {\n                                break;\n                            }\n                            last_timestamp = token;\n                        }\n                    }\n                }\n            }\n\n            let current_tokens = [];\n\n            // - all tokens within output\n            for (const token of token_ids) {\n                // 4 possible states for each token\n                // - 1/ Language code\n                // - 2/ all other special tokens (which we ignore)\n                // - 3/ Timestamp\n                // - 4/ Regular text\n\n                if (all_special_ids.has(token)) {\n                    const text = this.decode([token]);\n                    if (text[0] === \"[\" && text[text.length - 1] === \"]\") {\n                        const language = WHISPER_LANGUAGE_MAPPING.get(text.slice(1, -1));\n\n                        if (language !== undefined) {\n                            // 1/ Indeed some language\n                            // TODO Handle when language is different from the previous\n                            // one, and we cannot use timestamped tokens to create chunks\n                            if (last_language !== null && language !== last_language && !return_timestamps) {\n                                previous_tokens.push(current_tokens);\n                                const resolved_tokens = this.findLongestCommonSequence(previous_tokens);\n                                const resolved_text = this.decode(resolved_tokens);\n                                chunk.text = resolved_text;\n                                chunks.push(chunk);\n\n                                // Flush all our temporary context\n                                previous_tokens = [];\n                                current_tokens = [];\n                                chunk = new_chunk();\n                            }\n\n                            last_language = chunk.language = language;\n                        } else {\n                            // 2/ This is a regular special token, ignoring it\n                        }\n                    }\n                } else if (token >= timestamp_begin) {\n                    // 3/ Timestamp token\n                    const time = (token - timestamp_begin) * time_precision + time_offset;\n                    const rounded_time = Math.round(time * 100) / 100;\n\n                    if (last_timestamp !== null && token >= last_timestamp) {\n                        // Whisper outputted a timestamp token, but it falls within\n                        // our stride, so we're going to skip it for the time being\n                        // and resolve this later\n                        // Skip is necessary because timestamp tokens always come\n                        // by pair, so we need to skip the next one too (which would mark the start of another chunk).\n                        skip = true;\n                    } else if (skip || (previous_tokens.length > 0 && token < first_timestamp)) {\n                        skip = false;\n                    } else if (chunk.timestamp[0] === null) {\n                        chunk.timestamp[0] = rounded_time;\n                    } else {\n                        // This is the end of the timestamp chunk\n                        if (rounded_time === chunk.timestamp[0]) {\n                            // This is a bug in timestamp token output\n                            // where we're taking the duplicate token\n                            // as a stop where it should be a start.\n                            // This is an issue in the underlying model output\n                            // Let's just skip it so it becomes\n                        } else {\n                            chunk.timestamp[1] = rounded_time;\n\n                            // Handling merges\n                            previous_tokens.push(current_tokens)\n                            const resolved_tokens = this.findLongestCommonSequence(previous_tokens)\n                            const resolved_text = this.decode(resolved_tokens)\n                            chunk.text = resolved_text\n                            chunks.push(chunk)\n\n                            // Flush all our temporary context\n                            previous_tokens = []\n                            current_tokens = []\n                            chunk = new_chunk()\n                        }\n                    }\n\n                } else {\n                    // 4/ Regular token\n                    // We just append to the list of all tokens so we can handle\n                    // merges later and decode into text.\n                    current_tokens.push(token)\n\n                }\n            }\n\n            if ('stride' in output) {\n                const [chunk_len, stride_left, stride_right] = output.stride;\n                time_offset += chunk_len - stride_right\n            }\n\n            // Leftover tokens\n            if (current_tokens.length > 0) {\n                previous_tokens.push(current_tokens)\n            } else if (previous_tokens.every(p => p.length === 0)) {\n                // Flushing previous tokens (END)\"\n                chunk = new_chunk()\n                previous_tokens = []\n                current_tokens = []\n            }\n\n        }\n\n        if (previous_tokens.length > 0) {\n            if (force_full_sequences && return_timestamps) {\n                // Last token should always be timestamps, so there shouldn't be\n                // leftover\n                throw new Error(\"There was an error while processing timestamps, we haven't found a timestamp as last token.\");\n            }\n\n            // Happens when we don't use timestamps\n            const resolved_tokens = this.findLongestCommonSequence(previous_tokens);\n\n            // Flushing previous tokens (FINAL)\n            const resolved_text = this.decode(resolved_tokens);\n            chunk.text = resolved_text;\n            chunks.push(chunk);\n        }\n\n        let optional = Object.create(null);\n\n        // Preparing and cleaning up the pipeline output\n        const full_text = chunks.map(chunk => chunk.text).join('');\n        if (return_timestamps || return_language) {\n            for (let i = 0; i < chunks.length; ++i) {\n                const chunk = chunks[i];\n                if (!return_timestamps) {\n                    delete chunk[\"timestamp\"];\n                }\n\n                if (!return_language) {\n                    delete chunk[\"language\"];\n                }\n            }\n            optional = { \"chunks\": chunks };\n        }\n        return [full_text, optional];\n\n    }\n\n    /**\n     * Finds the longest common sequence among the provided sequences.\n     * @param {number[][]} sequences An array of sequences of token ids to compare.\n     * @returns {number[]} The longest common sequence found.\n     * @throws {Error} If there is a bug within the function.\n     */\n    findLongestCommonSequence(sequences) {\n        // It would be much harder to do O(n) because of fault tolerance.\n        // We actually have a really good property which is that the total sequence\n        // MUST be those subsequences in order.\n        let leftSequence = sequences[0];\n        let leftLength = leftSequence.length;\n        let totalSequence = [];\n        for (let i = 1; i < sequences.length; ++i) {\n            const rightSequence = sequences[i];\n            let max = 0.0;\n            let maxIndices = [leftLength, leftLength, 0, 0];\n            // Here we're sliding matches\n            // [a, b, c, d]\n            //          [c, d, f]\n            // =        [c] == [d]\n\n            // [a, b, c, d]\n            //       [c, d, f]\n            // =     [c, d] == [c, d]\n\n\n            // [a, b, c, d]\n            //    [c, d, f]\n\n            // =  [b, c, d] == [c, d, f]\n\n            // [a, b, c, d]\n            // [c, d, f]\n\n            // [a, b, c] == [c, d, f]\n\n            // [a, b, c, d]\n            // [d, f]\n\n            // [a, b] == [d, f]\n\n            // [a, b, c, d]\n            // [f]\n\n            // [a] == [f]\n\n            const rightLength = rightSequence.length;\n            for (let j = 1; j < leftLength + rightLength; ++j) {\n                const eps = j / 10000.0;\n                const leftStart = Math.max(0, leftLength - j);\n                const leftStop = Math.min(leftLength, leftLength + rightLength - j);\n                const left = leftSequence.slice(leftStart, leftStop);\n                const rightStart = Math.max(0, j - leftLength);\n                const rightStop = Math.min(rightLength, j);\n                const right = rightSequence.slice(rightStart, rightStop);\n                if (left.length !== right.length) {\n                    throw new Error(\"There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.\");\n                }\n                const matches = left.filter((elem, idx) => elem === right[idx]).length;\n                const matching = matches / j + eps;\n                if (matches > 1 && matching > max) {\n                    max = matching;\n                    maxIndices = [leftStart, leftStop, rightStart, rightStop];\n                }\n            }\n            const [leftStart, leftStop, rightStart, rightStop] = maxIndices;\n            const leftMid = Math.floor((leftStop + leftStart) / 2);\n            const rightMid = Math.floor((rightStop + rightStart) / 2);\n            totalSequence.push(...leftSequence.slice(0, leftMid));\n            leftSequence = rightSequence.slice(rightMid);\n            leftLength = leftSequence.length;\n        }\n        totalSequence.push(...leftSequence);\n        return totalSequence;\n    }\n\n    /**\n     * Helper function to build translation inputs for a `WhisperTokenizer`,\n     * depending on the language, task, and whether to predict timestamp tokens.\n     * \n     * Used to override the prefix tokens appended to the start of the label sequence.\n     * \n     * **Example: Get ids for a language**\n     * ```javascript\n     * // instantiate the tokenizer and set the prefix token to Spanish\n     * let tokenizer = await WhisperTokenizer.from_pretrained('Xenova/whisper-tiny');\n     * let forced_decoder_ids = tokenizer.get_decoder_prompt_ids({ language: 'spanish' });\n     * // [(1, 50262), (2, 50363)]\n     * ```\n     * \n     * @param {Object} options Options to generate the decoder prompt.\n     * @param {string} [options.language] The language of the transcription text.\n     * The corresponding language id token is appended to the start of the sequence for multilingual\n     * speech recognition and speech translation tasks, e.g. for \"Spanish\" the token \"<|es|>\" is appended\n     * to the start of sequence.\n     * @param {string} [options.task] Task identifier to append at the start of sequence (if any).\n     * This should be used for mulitlingual fine-tuning, with \"transcribe\" for speech recognition and\n     * \"translate\" for speech translation.\n     * @param {boolean} [options.no_timestamps] Whether to add the <|notimestamps|> token at the start of the sequence.\n     * @returns {number[][]} The decoder prompt ids.\n     */\n    get_decoder_prompt_ids({\n        language = null,\n        task = null,\n        no_timestamps = true,\n    } = {}) {\n\n        // <|lang_id|> <|task|> <|notimestamps|>\n\n        let forced_decoder_ids = [];\n\n        if (language) {\n            // User wishes to specify the language\n            language = language.toLowerCase();\n\n            // Map to code from user-friendly name (e.g., \"english\" -> \"en\")\n            let language_code = WHISPER_TO_LANGUAGE_CODE_MAPPING.get(language);\n\n            if (language_code === undefined) {\n                // User provided something that is not a language name\n\n                if (WHISPER_LANGUAGE_MAPPING.has(language)) {\n                    // User provided the language code directly (e.g., \"en\")\n                    language_code = language;\n\n                } else {\n                    // User provided something that is not a language code or name\n                    const is_language_code = language.length === 2;\n                    const langs = is_language_code ? WHISPER_LANGUAGE_MAPPING.keys() : WHISPER_LANGUAGE_MAPPING.values();\n\n                    throw new Error(`Language \"${language}\" is not supported. Must be one of: ${JSON.stringify(langs)}`);\n                }\n            }\n\n            let language_token_id = this.model.tokens_to_ids.get(`<|${language_code}|>`);\n            if (language_token_id === undefined) {\n                throw new Error(`Unable to find language \"${language_code}\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.`)\n            }\n\n            forced_decoder_ids.push(language_token_id);\n        } else {\n            // No token will be forced, which leaves the model to predict the language\n            forced_decoder_ids.push(null);\n        }\n\n        if (task) {\n            task = task.toLowerCase();\n            if (task !== 'transcribe' && task !== 'translate') {\n                throw new Error(`Task \"${task}\" is not supported. Must be one of: [\"transcribe\", \"translate\"]`);\n            }\n\n            let task_token_id = this.model.tokens_to_ids.get(`<|${task}|>`);\n            if (task_token_id === undefined) {\n                throw new Error(`Unable to find task \"${task}\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.`)\n            }\n\n            forced_decoder_ids.push(task_token_id);\n        } else {\n            // No token will be forced, which leaves the model to predict the task\n            forced_decoder_ids.push(null);\n        }\n\n        if (no_timestamps) {\n            let no_timestamps_id = this.model.tokens_to_ids.get(`<|notimestamps|>`);\n            if (no_timestamps_id === undefined) {\n                throw new Error('Unable to find \"<|notimestamps|>\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.')\n            }\n\n            forced_decoder_ids.push(no_timestamps_id);\n        }\n\n        return forced_decoder_ids.map((x, i) => [i + 1, x]).filter(x => x[1] !== null);\n\n    }\n}\nexport class CodeGenTokenizer extends PreTrainedTokenizer { }\nexport class CLIPTokenizer extends PreTrainedTokenizer { }\n\n\n/**\n * @todo This model is not yet supported by Hugging Face's \"fast\" tokenizers library (https://github.com/huggingface/tokenizers).\n * Therefore, this implementation (which is based on fast tokenizers) may produce slightly inaccurate results.\n */\nexport class MarianTokenizer extends PreTrainedTokenizer {\n    /**\n     * Create a new MarianTokenizer instance.\n     * @param {Object} tokenizerJSON The JSON of the tokenizer.\n     * @param {Object} tokenizerConfig The config of the tokenizer.\n     */\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n\n        this.languageRegex = /^(>>\\w+<<)\\s*/g;\n\n        this.supported_language_codes = this.model.vocab.filter(\n            x => this.languageRegex.test(x)\n        );\n\n        console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\\'s \"fast\" tokenizers library. Therefore, you may experience slightly inaccurate results.')\n    }\n\n    /**\n     * Encodes a single text. Overriding this method is necessary since the language codes\n     * must be removed before encoding with sentencepiece model.\n     * @see https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213\n     *\n     * @param {string|null} text The text to encode.\n     * @returns {Array} The encoded tokens.\n     */\n    _encode_text(text) {\n        if (text === null) return null;\n\n        // Check if text starts with language code:\n        let [matchInfo, ...remainder] = text.trim().split(this.languageRegex);\n\n        if (remainder.length === 0) {\n            // No language code, encode normally\n            return super._encode_text(matchInfo);\n\n        } else if (remainder.length === 2) {\n            // Text starts with language code, so we do not encode it with sentencepiece.\n            let [language, text] = remainder;\n\n            if (!this.supported_language_codes.includes(language)) {\n                console.warn(`Unsupported language code \"${language}\" detected, which may lead to unexpected behavior. Should be one of: ${JSON.stringify(this.supported_language_codes)}`)\n            }\n            return mergeArrays([language], super._encode_text(text));\n        }\n    }\n\n}\n\n/**\n * A trie structure to efficiently store and search for strings.\n */\nclass CharTrie {\n    constructor() {\n        this.root = CharTrieNode.default();\n    }\n\n    /**\n     * Adds one or more `texts` to the trie.\n     * @param {string[]} texts The strings to add to the trie.\n     */\n    extend(texts) {\n        for (let text of texts) {\n            this.push(text);\n        }\n    }\n\n    /**\n     * Adds one or more `texts` to the trie.\n     * @param {*} text The strings to add to the trie.\n     */\n    push(text) {\n        let node = this.root;\n        for (let ch of text) {\n            let child = node.children.get(ch);\n            if (child === undefined) {\n                child = CharTrieNode.default();\n                node.children.set(ch, child);\n            }\n            node = child;\n        }\n        node.isLeaf = true;\n    }\n\n    /**\n     * Searches the trie for all strings with a common prefix of `text`.\n     * @param {string} text The common prefix to search for.\n     * @yields {string} Each string in the trie that has `text` as a prefix.\n     */\n    *commonPrefixSearch(text) {\n        let node = this.root;\n        let prefix = \"\";\n        for (let i = 0; i < text.length && node !== undefined; ++i) {\n            const ch = text[i];\n            prefix += ch;\n            node = node.children.get(ch);\n            if (node !== undefined && node.isLeaf) {\n                yield prefix;\n            }\n        }\n    }\n}\n\n/**\n * Represents a node in a character trie.\n * @param {boolean} isLeaf Whether the node is a leaf node or not.\n * @param {Map<string, CharTrieNode>} children A map containing the node's children, where the key is a character and the value is a `CharTrieNode`.\n */\nclass CharTrieNode {\n    constructor(isLeaf, children) {\n        this.isLeaf = isLeaf;\n        this.children = children;\n    }\n\n    /**\n     * Returns a new `CharTrieNode` instance with default values.\n     * @returns {CharTrieNode} A new `CharTrieNode` instance with `isLeaf` set to `false` and an empty `children` map.\n     */\n    static default() {\n        return new CharTrieNode(false, new Map());\n    }\n}\n\nclass TokenLattice {\n    /**\n     * Creates a new TokenLattice instance.\n     *\n     * @param {string} sentence The input sentence to be tokenized.\n     * @param {number} bosTokenId The beginning-of-sequence token ID.\n     * @param {number} eosTokenId The end-of-sequence token ID.\n     */\n    constructor(sentence, bosTokenId, eosTokenId) {\n        this.sentence = sentence;\n        this.len = sentence.length;\n        this.bosTokenId = bosTokenId;\n        this.eosTokenId = eosTokenId;\n        this.nodes = [];\n        this.beginNodes = new Array(this.len + 1);\n        this.endNodes = new Array(this.len + 1);\n        for (let i = 0; i < this.len + 1; ++i) {\n            this.beginNodes[i] = [];\n            this.endNodes[i] = [];\n        }\n        const bos = new TokenLatticeNode(this.bosTokenId, 0, 0, 0, 0.0);\n        const eos = new TokenLatticeNode(this.eosTokenId, 1, this.len, 0, 0.0);\n        this.nodes.push(bos.clone());\n        this.nodes.push(eos.clone());\n        this.beginNodes[this.len].push(eos);\n        this.endNodes[0].push(bos);\n    }\n\n    /**\n     * Inserts a new token node into the token lattice.\n     *\n     * @param {number} pos The starting position of the token.\n     * @param {number} length The length of the token.\n     * @param {number} score The score of the token.\n     * @param {number} tokenId The token ID of the token.\n     */\n    insert(pos, length, score, tokenId) {\n        const nodeId = this.nodes.length;\n        const node = new TokenLatticeNode(tokenId, nodeId, pos, length, score);\n        this.beginNodes[pos].push(node);\n        this.endNodes[pos + length].push(node);\n        this.nodes.push(node);\n    }\n\n    /**\n     * Implements the Viterbi algorithm to compute the most likely sequence of tokens.\n     *\n     * @returns {TokenLatticeNode[]} The array of nodes representing the most likely sequence of tokens.\n     */\n    viterbi() {\n        const len = this.len;\n        let pos = 0;\n        while (pos <= len) {\n            if (this.beginNodes[pos].length == 0) {\n                return [];\n            }\n            for (let rnode of this.beginNodes[pos]) {\n                rnode.prev = null;\n                let bestScore = 0.0;\n                let bestNode = null;\n                for (let lnode of this.endNodes[pos]) {\n                    const score = lnode.backtraceScore + rnode.score;\n                    if (bestNode === null || score > bestScore) {\n                        bestNode = lnode.clone();\n                        bestScore = score;\n                    }\n                }\n\n                if (bestNode !== null) {\n                    rnode.prev = bestNode;\n                    rnode.backtraceScore = bestScore;\n                } else {\n                    return [];\n                }\n            }\n            ++pos;\n        }\n\n        const results = [];\n        const root = this.beginNodes[len][0];\n        const prev = root.prev;\n        if (prev === null) {\n            return [];\n        }\n\n        let node = prev.clone();\n        while (node.prev !== null) {\n            results.push(node.clone());\n            const n = node.clone();\n            node = n.prev.clone();\n        }\n\n        results.reverse();\n        return results;\n    }\n\n    /**\n     * @param {TokenLatticeNode} node\n     * @returns {string} The array of nodes representing the most likely sequence of tokens.\n     */\n    piece(node) {\n        return this.sentence.slice(node.pos, node.pos + node.length);\n    }\n\n    /**\n     * @returns {Array} The array of nodes representing the most likely sequence of tokens.\n     */\n    tokens() {\n        const nodes = this.viterbi();\n        return nodes.map(x => this.piece(x));\n    }\n\n    /**\n     * @returns {Array} The array of nodes representing the most likely sequence of tokens.\n     */\n    tokenIds() {\n        const nodes = this.viterbi();\n        return nodes.map(x => x.tokenId);\n    }\n}\nclass TokenLatticeNode {\n    /**\n     * Represents a node in a token lattice for a given sentence.\n     * @param {number} tokenId The ID of the token associated with this node.\n     * @param {number} nodeId The ID of this node.\n     * @param {number} pos The starting position of the token in the sentence.\n     * @param {number} length The length of the token.\n     * @param {number} score The score associated with the token.\n     */\n    constructor(tokenId, nodeId, pos, length, score) {\n        this.tokenId = tokenId;\n        this.nodeId = nodeId;\n        this.pos = pos;\n        this.length = length;\n        this.score = score;\n        this.prev = null;\n        this.backtraceScore = 0.0;\n    }\n\n    /**\n     * Returns a clone of this node.\n     * @returns {TokenLatticeNode} A clone of this node.\n     */\n    clone() {\n        const n = new TokenLatticeNode(this.tokenId, this.nodeId, this.pos, this.length, this.score);\n        n.prev = this.prev;\n        n.backtraceScore = this.backtraceScore;\n        return n;\n    }\n}\n\n/**\n * Helper class which is used to instantiate pretrained tokenizers with the `from_pretrained` function.\n * The chosen tokenizer class is determined by the type specified in the tokenizer config.\n * \n * @example\n * let tokenizer = await AutoTokenizer.from_pretrained('bert-base-uncased');\n */\nexport class AutoTokenizer {\n    static TOKENIZER_CLASS_MAPPING = {\n        'T5Tokenizer': T5Tokenizer,\n        'DistilBertTokenizer': DistilBertTokenizer,\n        'BertTokenizer': BertTokenizer,\n        'MobileBertTokenizer': MobileBertTokenizer,\n        'SqueezeBertTokenizer': SqueezeBertTokenizer,\n        'AlbertTokenizer': AlbertTokenizer,\n        'GPT2Tokenizer': GPT2Tokenizer,\n        'BartTokenizer': BartTokenizer,\n        'RobertaTokenizer': RobertaTokenizer,\n        'WhisperTokenizer': WhisperTokenizer,\n        'CodeGenTokenizer': CodeGenTokenizer,\n        'CLIPTokenizer': CLIPTokenizer,\n        'MarianTokenizer': MarianTokenizer,\n\n        'BloomTokenizer': BloomTokenizer,\n        'NllbTokenizer': NllbTokenizer,\n        'LlamaTokenizer': LlamaTokenizer,\n    }\n\n\n    /**\n     * Instantiate one of the tokenizer classes of the library from a pretrained model.\n     * \n     * The tokenizer class to instantiate is selected based on the `tokenizer_class` property of the config object\n     * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n     * \n     * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n     * - A string, the *model id* of a pretrained tokenizer hosted inside a model repo on huggingface.co.\n     *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n     *   user or organization name, like `dbmdz/bert-base-german-cased`.\n     * - A path to a *directory* containing tokenizer files, e.g., `./my_model_directory/`.\n     * @param {PretrainedOptions} options Additional options for loading the tokenizer.\n     * \n     * @returns {Promise<PreTrainedTokenizer>} A new instance of the PreTrainedTokenizer class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n    } = {}) {\n\n        let [tokenizerJSON, tokenizerConfig] = await loadTokenizer(pretrained_model_name_or_path, {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n        })\n\n        // Some tokenizers are saved with the \"Fast\" suffix, so we remove that if present.\n        let tokenizerName = tokenizerConfig.tokenizer_class.replace(/Fast$/, '');\n\n        let cls = this.TOKENIZER_CLASS_MAPPING[tokenizerName];\n        if (!cls) {\n            console.warn(`Unknown tokenizer class \"${tokenizerName}\", attempting to construct from base class.`);\n            cls = PreTrainedTokenizer;\n        }\n        return new cls(tokenizerJSON, tokenizerConfig);\n    }\n}\n"],"mappings":"AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SACIA,QAAQ,EACRC,iBAAiB,EACjBC,YAAY,EACZC,gBAAgB,EAChBC,WAAW,QACR,iBAAiB;AAExB,SACIC,YAAY,QACT,gBAAgB;AAEvB,SAASC,GAAG,EAAEC,GAAG,QAAQ,kBAAkB;AAC3C,SAASC,MAAM,QAAQ,mBAAmB;;AAE1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAeC,aAAaA,CAACC,6BAA6B,EAAEC,OAAO,EAAE;EAEjE,IAAIC,IAAI,GAAG,MAAMC,OAAO,CAACC,GAAG,CAAC,CACzBT,YAAY,CAACK,6BAA6B,EAAE,gBAAgB,EAAE,IAAI,EAAEC,OAAO,CAAC,EAC5EN,YAAY,CAACK,6BAA6B,EAAE,uBAAuB,EAAE,IAAI,EAAEC,OAAO,CAAC,CACtF,CAAC;EACF,OAAOC,IAAI;AACf;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASG,aAAaA,CAACC,OAAO,EAAE;EAC5B,IAAIA,OAAO,CAACC,KAAK,EAAE;IACf,OAAO,IAAIC,MAAM,CAACF,OAAO,CAACC,KAAK,EAAE,IAAI,CAAC;EAE1C,CAAC,MAAM,IAAID,OAAO,CAACG,MAAM,EAAE;IACvB,OAAOH,OAAO,CAACG,MAAM;EAEzB,CAAC,MAAM;IACHC,OAAO,CAACC,IAAI,CAAC,uBAAuB,EAAEL,OAAO,CAAC;IAC9C,OAAO,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASM,qBAAqBA,CAACC,IAAI,EAAE;EACjC;EACA;EACA,OAAOA,IAAI,CAACC,OAAO,CAAC,MAAM,EAAE,GAAG,CAAC,CAC3BA,OAAO,CAAC,MAAM,EAAE,GAAG,CAAC,CACpBA,OAAO,CAAC,MAAM,EAAE,GAAG,CAAC,CACpBA,OAAO,CAAC,KAAK,EAAE,GAAG,CAAC,CACnBA,OAAO,CAAC,OAAO,EAAE,GAAG,CAAC,CACrBA,OAAO,CAAC,QAAQ,EAAE,KAAK,CAAC,CACxBA,OAAO,CAAC,OAAO,EAAE,IAAI,CAAC,CACtBA,OAAO,CAAC,OAAO,EAAE,IAAI,CAAC,CACtBA,OAAO,CAAC,QAAQ,EAAE,KAAK,CAAC,CACxBA,OAAO,CAAC,QAAQ,EAAE,KAAK,CAAC;AACjC;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASC,IAAIA,CAACC,GAAG,EAAEC,KAAK,EAAE;EACtB,IAAIC,KAAK,GAAG,EAAE;EACd,IAAIC,CAAC,GAAG,CAAC;EACT,OAAOA,CAAC,GAAGH,GAAG,CAACI,MAAM,EAAE;IACnBF,KAAK,CAACG,IAAI,CAACL,GAAG,CAACG,CAAC,CAAC,CAAC;IAClB,IAAIH,GAAG,CAACG,CAAC,CAAC,KAAKF,KAAK,EAAE;MAClB,EAAEE,CAAC;MACH;IACJ;IAEA,OAAOA,CAAC,GAAGH,GAAG,CAACI,MAAM,IAAIJ,GAAG,CAACG,CAAC,CAAC,KAAKF,KAAK,EAAE;MACvC,EAAEE,CAAC;IACP;EACJ;EAEA,OAAOD,KAAK;AAChB;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASI,gBAAgBA,CAACT,IAAI,EAAE;EAC5B,OAAOA,IAAI,CAACU,KAAK,CAAC,MAAM,CAAC,IAAI,EAAE;AACnC;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,cAAc,SAASlC,QAAQ,CAAC;EACzC;AACJ;AACA;AACA;EACImC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;;IAEpB;IACA,IAAI,CAACC,KAAK,GAAG,EAAE;;IAEf;AACR;AACA;AACA;IACQ,IAAI,CAACC,aAAa,GAAG,IAAIC,GAAG,CAAC,CAAC;IAE9B,IAAI,CAACC,YAAY,GAAGC,SAAS;IAC7B,IAAI,CAACC,SAAS,GAAGD,SAAS;IAC1B,IAAI,CAACE,kBAAkB,GAAGF,SAAS;;IAEnC;IACA,IAAI,CAACG,QAAQ,GAAG,KAAK;EACzB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,OAAOC,UAAUA,CAACT,MAAM,EAAW;IAAA,SAAAU,IAAA,GAAAC,SAAA,CAAAjB,MAAA,EAANkB,IAAI,OAAAC,KAAA,CAAAH,IAAA,OAAAA,IAAA,WAAAI,IAAA,MAAAA,IAAA,GAAAJ,IAAA,EAAAI,IAAA;MAAJF,IAAI,CAAAE,IAAA,QAAAH,SAAA,CAAAG,IAAA;IAAA;IAC7B,QAAQd,MAAM,CAACe,IAAI;MACf,KAAK,WAAW;QACZ,OAAO,IAAIC,kBAAkB,CAAChB,MAAM,CAAC;MACzC,KAAK,SAAS;QACV;QACA,OAAO,IAAIiB,OAAO,CAACjB,MAAM,EAAE,GAAGY,IAAI,CAAC;MAEvC,KAAK,KAAK;QACN;QACA,OAAO,IAAIM,GAAG,CAAClB,MAAM,EAAE,GAAGY,IAAI,CAAC;MACnC;QACI,MAAM,IAAIO,KAAK,CAAE,gCAA+BnB,MAAM,CAACe,IAAK,EAAC,CAAC;IACtE;EACJ;;EAEA;AACJ;AACA;AACA;AACA;EACIK,KAAKA,CAACC,MAAM,EAAE;IACV,OAAO,IAAI,CAACC,MAAM,CAACD,MAAM,CAAC;EAC9B;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIC,MAAMA,CAACD,MAAM,EAAE;IACX,MAAMF,KAAK,CAAC,2CAA2C,CAAC;EAC5D;;EAEA;AACJ;AACA;AACA;AACA;EACII,qBAAqBA,CAACF,MAAM,EAAE;IAC1B,IAAIG,GAAG,GAAGH,MAAM,CAACI,GAAG,CAACC,CAAC,IAAI,IAAI,CAACxB,aAAa,CAACyB,GAAG,CAACD,CAAC,CAAC,IAAI,IAAI,CAACtB,YAAY,CAAC;IAEzE,IAAI,IAAI,CAACI,QAAQ,EAAE;MACf;MACAgB,GAAG,GAAGnC,IAAI,CAACmC,GAAG,EAAE,IAAI,CAACpB,YAAY,CAAC;IACtC;IACA,OAAOoB,GAAG;EACd;;EAEA;AACJ;AACA;AACA;AACA;EACII,qBAAqBA,CAACJ,GAAG,EAAE;IACvB,OAAOA,GAAG,CAACC,GAAG,CAAChC,CAAC,IAAI,IAAI,CAACQ,KAAK,CAACR,CAAC,CAAC,IAAI,IAAI,CAACa,SAAS,CAAC;EACxD;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMU,kBAAkB,SAASlB,cAAc,CAAC;EAC5C;AACJ;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb;AACR;AACA;AACA;IACQ,IAAI,CAACE,aAAa,GAAGF,MAAM,CAACC,KAAK;;IAEjC;AACR;AACA;AACA;IACQ,IAAI,CAACG,YAAY,GAAG,IAAI,CAACF,aAAa,CAACyB,GAAG,CAAC3B,MAAM,CAACM,SAAS,CAAC;;IAE5D;AACR;AACA;AACA;IACQ,IAAI,CAACA,SAAS,GAAGN,MAAM,CAACM,SAAS;;IAEjC;AACR;AACA;AACA;IACQ,IAAI,CAACL,KAAK,GAAG,IAAIY,KAAK,CAAC,IAAI,CAACX,aAAa,CAAC2B,IAAI,CAAC;IAE/C,KAAK,MAAM,CAACC,GAAG,EAAEvC,KAAK,CAAC,IAAI,IAAI,CAACW,aAAa,EAAE;MAC3C,IAAI,CAACD,KAAK,CAACV,KAAK,CAAC,GAAGuC,GAAG;IAC3B;EACJ;;EAEA;AACJ;AACA;AACA;AACA;EACIR,MAAMA,CAACD,MAAM,EAAE;IACX,IAAIU,YAAY,GAAG,EAAE;IACrB,KAAK,IAAIC,KAAK,IAAIX,MAAM,EAAE;MACtB,IAAIY,KAAK,GAAG,CAAC,GAAGD,KAAK,CAAC;MACtB;MACA;MACA;MACA;;MAEA,IAAIE,SAAS,GAAG,KAAK;MACrB,IAAIC,KAAK,GAAG,CAAC;MACb,IAAIC,SAAS,GAAG,EAAE;MAElB,OAAOD,KAAK,GAAGF,KAAK,CAACvC,MAAM,EAAE;QACzB,IAAI2C,GAAG,GAAGJ,KAAK,CAACvC,MAAM;QACtB,IAAI4C,gBAAgB,GAAG,IAAI;QAC3B,OAAOH,KAAK,GAAGE,GAAG,EAAE;UAChB,IAAIE,MAAM,GAAGN,KAAK,CAACO,KAAK,CAACL,KAAK,EAAEE,GAAG,CAAC,CAACI,IAAI,CAAC,EAAE,CAAC;UAE7C,IAAIN,KAAK,GAAG,CAAC,EAAE;YACXI,MAAM,GAAG,IAAI,CAACvC,MAAM,CAAC0C,yBAAyB,GAAGH,MAAM;UAC3D;UACA,IAAI,IAAI,CAACrC,aAAa,CAACyC,GAAG,CAACJ,MAAM,CAAC,EAAE;YAChCD,gBAAgB,GAAGC,MAAM;YACzB;UACJ;UAEA,EAAEF,GAAG;QACT;QACA,IAAIC,gBAAgB,KAAK,IAAI,EAAE;UAC3BJ,SAAS,GAAG,IAAI;UAChB;QACJ;QACAE,SAAS,CAACzC,IAAI,CAAC2C,gBAAgB,CAAC;QAChCH,KAAK,GAAGE,GAAG;MACf;MACA,IAAIH,SAAS,EAAE;QACXH,YAAY,CAACpC,IAAI,CAAC,IAAI,CAACW,SAAS,CAAC;MACrC,CAAC,MAAM;QACHyB,YAAY,CAACpC,IAAI,CAAC,GAAGyC,SAAS,CAAC;MACnC;IACJ;IAEA,OAAOL,YAAY;EACvB;AAEJ;;AAEA;AACA;AACA;AACA;AACA,MAAMd,OAAO,SAASnB,cAAc,CAAC;EACjC;AACJ;AACA;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAACC,MAAM,EAAE4C,UAAU,EAAE;IAC5B,KAAK,CAAC5C,MAAM,CAAC;IAEb,IAAI,CAACC,KAAK,GAAG,IAAIY,KAAK,CAACb,MAAM,CAACC,KAAK,CAAC4B,IAAI,CAAC;IACzC,IAAI,CAACgB,MAAM,GAAG,IAAIhC,KAAK,CAACb,MAAM,CAACC,KAAK,CAAC4B,IAAI,CAAC;IAC1C,IAAIiB,KAAK,GAAG,CAAC;IACb9C,MAAM,CAACC,KAAK,CAAC8C,OAAO,CAAC,CAACxD,KAAK,EAAEuC,GAAG,KAAK;MACjC,IAAI,CAAC7B,KAAK,CAAC6C,KAAK,CAAC,GAAGhB,GAAG;MACvB,IAAI,CAACe,MAAM,CAACC,KAAK,CAAC,GAAGvD,KAAK;MAC1B,EAAEuD,KAAK;IACX,CAAC,CAAC;IAEF,IAAI,CAAC1C,YAAY,GAAGJ,MAAM,CAACgD,MAAM;IACjC,IAAI,CAAC1C,SAAS,GAAG,IAAI,CAACL,KAAK,CAACD,MAAM,CAACgD,MAAM,CAAC;IAE1C,IAAI,CAAC9C,aAAa,GAAG,IAAIC,GAAG,CAAC,IAAI,CAACF,KAAK,CAACwB,GAAG,CAAC,CAACwB,CAAC,EAAExD,CAAC,KAAK,CAACwD,CAAC,EAAExD,CAAC,CAAC,CAAC,CAAC;IAC9D,IAAI,CAACyD,QAAQ,GAAG,GAAG,CAAC,CAAC;;IAErB,IAAI,CAACC,UAAU,GAAG,IAAI,CAACjD,aAAa,CAACyB,GAAG,CAAC,IAAI,CAACuB,QAAQ,CAAC,CAAC,CAAC;IACzD,IAAI,CAACE,QAAQ,GAAGR,UAAU,CAACS,SAAS;IAEpC,IAAI,CAACC,UAAU,GAAG,IAAI,CAACpD,aAAa,CAACyB,GAAG,CAAC,IAAI,CAACyB,QAAQ,CAAC;IACvD,IAAI,CAACG,QAAQ,GAAG,IAAI,CAACtD,KAAK,CAAC,IAAI,CAACG,YAAY,CAAC;IAE7C,IAAI,CAACoD,QAAQ,GAAGrF,GAAG,CAAC,IAAI,CAAC0E,MAAM,CAAC,CAAC,CAAC,CAAC;IAEnC,IAAI,CAACY,QAAQ,GAAG,IAAI,CAACD,QAAQ,GAAG,IAAI;IACpC,IAAI,CAACX,MAAM,CAAC,IAAI,CAACzC,YAAY,CAAC,GAAG,IAAI,CAACqD,QAAQ;IAE9C,IAAI,CAACC,IAAI,GAAG,IAAIC,QAAQ,CAAC,CAAC;IAC1B,IAAI,CAACD,IAAI,CAACE,MAAM,CAAC,IAAI,CAAC3D,KAAK,CAAC;;IAE5B;IACA;IACA,IAAI,CAACO,QAAQ,GAAG,IAAI;EACxB;;EAEA;AACJ;AACA;AACA;EACIqD,aAAaA,CAACC,OAAO,EAAE;IACnB,MAAMC,QAAQ,GAAGD,OAAO,CAACC,QAAQ;IACjC,MAAMC,GAAG,GAAGD,QAAQ,CAACrE,MAAM;IAC3B,IAAIuE,QAAQ,GAAG,CAAC;IAChB,OAAOA,QAAQ,GAAGD,GAAG,EAAE;MACnB,MAAME,KAAK,GAAG,CAAC;MACf,IAAIC,aAAa,GAAG,KAAK;MACzB,MAAM9C,MAAM,GAAG,EAAE;MAEjB,KAAK,IAAIW,KAAK,IAAI,IAAI,CAAC0B,IAAI,CAACU,kBAAkB,CAACL,QAAQ,CAACvB,KAAK,CAACyB,QAAQ,CAAC,CAAC,EAAE;QACtE5C,MAAM,CAAC1B,IAAI,CAACqC,KAAK,CAAC;QAClB,MAAMqC,OAAO,GAAG,IAAI,CAACnE,aAAa,CAACyB,GAAG,CAACK,KAAK,CAAC;QAC7C,MAAMsC,UAAU,GAAG,IAAI,CAACzB,MAAM,CAACwB,OAAO,CAAC;QACvC,MAAME,CAAC,GAAGvC,KAAK,CAACtC,MAAM;QACtBoE,OAAO,CAACU,MAAM,CAACP,QAAQ,EAAEM,CAAC,EAAED,UAAU,EAAED,OAAO,CAAC;QAChD,IAAI,CAACF,aAAa,IAAII,CAAC,KAAKL,KAAK,EAAE;UAC/BC,aAAa,GAAG,IAAI;QACxB;MACJ;MACA,IAAI,CAACA,aAAa,EAAE;QAChBL,OAAO,CAACU,MAAM,CAACP,QAAQ,EAAEC,KAAK,EAAE,IAAI,CAACT,QAAQ,EAAE,IAAI,CAACrD,YAAY,CAAC;MACrE;MACA6D,QAAQ,IAAIC,KAAK;IACrB;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIO,QAAQA,CAACC,UAAU,EAAE;IACjB,MAAMZ,OAAO,GAAG,IAAIa,YAAY,CAACD,UAAU,EAAE,IAAI,CAACvB,UAAU,EAAE,IAAI,CAACG,UAAU,CAAC;IAC9E,IAAI,CAACO,aAAa,CAACC,OAAO,CAAC;IAC3B,OAAOA,OAAO,CAACzC,MAAM,CAAC,CAAC;EAC3B;;EAEA;AACJ;AACA;AACA;AACA;EACIC,MAAMA,CAACD,MAAM,EAAE;IACX,IAAIuD,QAAQ,GAAG,EAAE;IACjB,KAAK,IAAI5C,KAAK,IAAIX,MAAM,EAAE;MACtB,MAAMwD,SAAS,GAAG,IAAI,CAACJ,QAAQ,CAACzC,KAAK,CAAC;MACtC4C,QAAQ,CAACjF,IAAI,CAAC,GAAGkF,SAAS,CAAC;IAC/B;IACA,OAAOD,QAAQ;EACnB;AAEJ;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAME,gBAAgB,GAAG,CAAC,MAAM;EAC5B;EACA;EACA;;EAEA,MAAMC,EAAE,GAAG,CACP,GAAGlE,KAAK,CAACmE,IAAI,CAAC;IAAEtF,MAAM,EAAE,GAAG,CAACuF,UAAU,CAAC,CAAC,CAAC,GAAG,GAAG,CAACA,UAAU,CAAC,CAAC,CAAC,GAAG;EAAE,CAAC,EAAE,CAACC,CAAC,EAAEzF,CAAC,KAAKA,CAAC,GAAG,GAAG,CAACwF,UAAU,CAAC,CAAC,CAAC,CAAC,EACrG,GAAGpE,KAAK,CAACmE,IAAI,CAAC;IAAEtF,MAAM,EAAE,GAAG,CAACuF,UAAU,CAAC,CAAC,CAAC,GAAG,GAAG,CAACA,UAAU,CAAC,CAAC,CAAC,GAAG;EAAE,CAAC,EAAE,CAACC,CAAC,EAAEzF,CAAC,KAAKA,CAAC,GAAG,GAAG,CAACwF,UAAU,CAAC,CAAC,CAAC,CAAC,EACrG,GAAGpE,KAAK,CAACmE,IAAI,CAAC;IAAEtF,MAAM,EAAE,GAAG,CAACuF,UAAU,CAAC,CAAC,CAAC,GAAG,GAAG,CAACA,UAAU,CAAC,CAAC,CAAC,GAAG;EAAE,CAAC,EAAE,CAACC,CAAC,EAAEzF,CAAC,KAAKA,CAAC,GAAG,GAAG,CAACwF,UAAU,CAAC,CAAC,CAAC,CAAC,CACxG;EACD,IAAIE,EAAE,GAAGJ,EAAE,CAACvC,KAAK,CAAC,CAAC;EACnB,IAAI+B,CAAC,GAAG,CAAC;EACT,KAAK,IAAIa,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,GAAG,EAAE,EAAEA,CAAC,EAAE;IAC1B,IAAI,CAACL,EAAE,CAACM,QAAQ,CAACD,CAAC,CAAC,EAAE;MACjBL,EAAE,CAACpF,IAAI,CAACyF,CAAC,CAAC;MACVD,EAAE,CAACxF,IAAI,CAAC,GAAG,GAAG4E,CAAC,CAAC;MAChBA,CAAC,IAAI,CAAC;IACV;EACJ;EACA,IAAIe,GAAG,GAAGH,EAAE,CAAC1D,GAAG,CAAC8C,CAAC,IAAIxF,MAAM,CAACwG,YAAY,CAAChB,CAAC,CAAC,CAAC;EAC7C,OAAOiB,MAAM,CAACC,WAAW,CAACV,EAAE,CAACtD,GAAG,CAAC,CAAC2D,CAAC,EAAE3F,CAAC,KAAK,CAAC2F,CAAC,EAAEE,GAAG,CAAC7F,CAAC,CAAC,CAAC,CAAC,CAAC;AAC5D,CAAC,EAAE,CAAC;AAEJ,MAAMiG,gBAAgB,GAAG7H,iBAAiB,CAACiH,gBAAgB,CAAC;;AAE5D;AACA;AACA;AACA;AACA,MAAM5D,GAAG,SAASpB,cAAc,CAAC;EAC7B;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACE,aAAa,GAAGF,MAAM,CAACC,KAAK;IAEjC,IAAI,CAACG,YAAY,GAAG,IAAI,CAACF,aAAa,CAACyB,GAAG,CAAC3B,MAAM,CAACM,SAAS,CAAC;IAC5D,IAAI,CAACA,SAAS,GAAGN,MAAM,CAACM,SAAS;IAEjC,IAAI,CAACL,KAAK,GAAG,IAAIY,KAAK,CAAC,IAAI,CAACX,aAAa,CAAC2B,IAAI,CAAC;IAC/C,KAAK,MAAM,CAACC,GAAG,EAAEvC,KAAK,CAAC,IAAI,IAAI,CAACW,aAAa,EAAE;MAC3C,IAAI,CAACD,KAAK,CAACV,KAAK,CAAC,GAAGuC,GAAG;IAC3B;IAEA,IAAI,CAAC6D,SAAS,GAAGH,MAAM,CAACC,WAAW,CAACzF,MAAM,CAAC4F,MAAM,CAACnE,GAAG,CAAC,CAACwB,CAAC,EAAExD,CAAC,KAAK,CAACwD,CAAC,EAAExD,CAAC,CAAC,CAAC,CAAC;IACxE,IAAI,CAACmG,MAAM,GAAG5F,MAAM,CAAC4F,MAAM,CAACnE,GAAG,CAACwB,CAAC,IAAIA,CAAC,CAAC4C,KAAK,CAAC,KAAK,CAAC,CAAC;IAEpD,IAAI,CAACtF,kBAAkB,GAAGP,MAAM,CAACO,kBAAkB;IAEnD,IAAI,CAACuF,aAAa,GAAG,IAAI,CAAC9F,MAAM,CAAC8F,aAAa,IAAI,KAAK;IAEvD,IAAI,IAAI,CAACA,aAAa,EAAE;MACpB,IAAI,CAACC,YAAY,GAAG,IAAIC,WAAW,CAAC,CAAC;IACzC;IAEA,IAAI,CAACC,KAAK,GAAGT,MAAM,CAACU,MAAM,CAAC,IAAI,CAAC;IAEhC,IAAI,CAAC1F,QAAQ,KAAK,IAAI,CAACR,MAAM,CAACQ,QAAQ;EAC1C;;EAEA;AACJ;AACA;AACA;AACA;EACI2F,SAASA,CAACC,IAAI,EAAE;IACZ,IAAIC,KAAK,GAAG,IAAIC,GAAG,CAAC,CAAC;IACrB,IAAIC,SAAS,GAAGH,IAAI,CAAC,CAAC,CAAC;IACvB,KAAK,IAAI3G,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2G,IAAI,CAAC1G,MAAM,EAAE,EAAED,CAAC,EAAE;MAClC,IAAI+G,IAAI,GAAGJ,IAAI,CAAC3G,CAAC,CAAC;MAClB4G,KAAK,CAACI,GAAG,CAAE,GAAEF,SAAU,IAAGC,IAAK,EAAC,CAAC;MACjCD,SAAS,GAAGC,IAAI;IACpB;IACA,OAAO3F,KAAK,CAACmE,IAAI,CAACqB,KAAK,CAAC;EAC5B;;EAEA;AACJ;AACA;AACA;AACA;EACIK,GAAGA,CAAC1E,KAAK,EAAE;IACP,IAAIA,KAAK,IAAI,IAAI,CAACiE,KAAK,EAAE;MACrB,OAAO,IAAI,CAACA,KAAK,CAACjE,KAAK,CAAC;IAC5B;IACA,IAAIoE,IAAI,GAAGvF,KAAK,CAACmE,IAAI,CAAChD,KAAK,CAAC;IAC5B,IAAI,IAAI,CAACzB,kBAAkB,EAAE;MACzB6F,IAAI,CAACA,IAAI,CAAC1G,MAAM,GAAG,CAAC,CAAC,IAAI,IAAI,CAACa,kBAAkB;IACpD;IACA,IAAI8F,KAAK,GAAG,IAAI,CAACF,SAAS,CAACC,IAAI,CAAC;IAEhC,IAAI,CAACC,KAAK,CAAC3G,MAAM,EAAE;MACf,IAAI,IAAI,CAACa,kBAAkB,EAAE;QACzByB,KAAK,IAAI,IAAI,CAACzB,kBAAkB;MACpC;MACA,OAAOyB,KAAK;IAChB;IAEA,OAAO,IAAI,EAAE;MACT,IAAI2E,MAAM,GAAGN,KAAK,CAACO,MAAM,CAAC,CAACC,CAAC,EAAEzB,CAAC,KAAK;QAChC,IAAI0B,CAAC,GAAG,IAAI,CAACnB,SAAS,CAACkB,CAAC,CAAC,IAAIE,QAAQ;QACrC,IAAIC,CAAC,GAAG,IAAI,CAACrB,SAAS,CAACP,CAAC,CAAC,IAAI2B,QAAQ;QACrC,OAAOD,CAAC,IAAIE,CAAC,GAAGH,CAAC,GAAGzB,CAAC;MACzB,CAAC,CAAC;MACF,IAAI,EAAEuB,MAAM,IAAI,IAAI,CAAChB,SAAS,CAAC,EAAE;QAC7B;MACJ;MACA,IAAI,CAACsB,KAAK,EAAEC,MAAM,CAAC,GAAGP,MAAM,CAACd,KAAK,CAAC,MAAM,CAAC;MAC1C,IAAIsB,QAAQ,GAAG,EAAE;MACjB,IAAI1H,CAAC,GAAG,CAAC;MACT,IAAI2H,CAAC,GAAG,CAAC,CAAC;MAEV,OAAO3H,CAAC,GAAG2G,IAAI,CAAC1G,MAAM,EAAE;QACpB,IAAI;UACA0H,CAAC,GAAGhB,IAAI,CAACiB,OAAO,CAACJ,KAAK,EAAExH,CAAC,CAAC;UAC1B,IAAI2H,CAAC,KAAK,CAAC,CAAC,EAAE,MAAM,OAAO;QAC/B,CAAC,CAAC,OAAOE,CAAC,EAAE;UACRH,QAAQ,CAACxH,IAAI,CAAC,GAAGyG,IAAI,CAAC5D,KAAK,CAAC/C,CAAC,CAAC,CAAC;UAC/B;QACJ;QACA0H,QAAQ,CAACxH,IAAI,CAAC,GAAGyG,IAAI,CAAC5D,KAAK,CAAC/C,CAAC,EAAE2H,CAAC,CAAC,CAAC;QAClC3H,CAAC,GAAG2H,CAAC;QAEL,IAAIhB,IAAI,CAAC3G,CAAC,CAAC,KAAKwH,KAAK,IAAIxH,CAAC,GAAG2G,IAAI,CAAC1G,MAAM,GAAG,CAAC,IAAI0G,IAAI,CAAC3G,CAAC,GAAG,CAAC,CAAC,KAAKyH,MAAM,EAAE;UACpEC,QAAQ,CAACxH,IAAI,CAACsH,KAAK,GAAGC,MAAM,CAAC;UAC7BzH,CAAC,IAAI,CAAC;QACV,CAAC,MAAM;UACH0H,QAAQ,CAACxH,IAAI,CAACyG,IAAI,CAAC3G,CAAC,CAAC,CAAC;UACtBA,CAAC,IAAI,CAAC;QACV;MACJ;MACA2G,IAAI,GAAGe,QAAQ;MACf,IAAIf,IAAI,CAAC1G,MAAM,KAAK,CAAC,EAAE;QACnB;MACJ,CAAC,MAAM;QACH2G,KAAK,GAAG,IAAI,CAACF,SAAS,CAACC,IAAI,CAAC;MAChC;IACJ;IACA,IAAImB,UAAU,GAAGnB,IAAI,CAAC3D,IAAI,CAAC,GAAG,CAAC;IAC/B,IAAI,CAACwD,KAAK,CAACjE,KAAK,CAAC,GAAGuF,UAAU;IAC9B,OAAOA,UAAU;EACrB;;EAEA;AACJ;AACA;AACA;AACA;EACIjG,MAAMA,CAACD,MAAM,EAAE;IACX,IAAIU,YAAY,GAAG,EAAE;IAErB,KAAK,IAAIC,KAAK,IAAIX,MAAM,EAAE;MACtB,IAAImG,cAAc,GAAG,IAAI,CAACd,GAAG,CAAC1E,KAAK,CAAC,CAAC6D,KAAK,CAAC,GAAG,CAAC;MAE/C,KAAK,IAAInE,CAAC,IAAI8F,cAAc,EAAE;QAC1B,IAAI,IAAI,CAACtH,aAAa,CAACyC,GAAG,CAACjB,CAAC,CAAC,EAAE;UAC3BK,YAAY,CAACpC,IAAI,CAAC+B,CAAC,CAAC;QACxB,CAAC,MAAM;UACH,IAAI,IAAI,CAACoE,aAAa,EAAE;YACpB/D,YAAY,CAACpC,IAAI,CACb,GAAGkB,KAAK,CAACmE,IAAI,CAAC,IAAI,CAACe,YAAY,CAACzE,MAAM,CAACI,CAAC,CAAC,CAAC,CACrCD,GAAG,CAACwB,CAAC,IAAK,MAAKA,CAAC,CAACwE,QAAQ,CAAC,EAAE,CAAC,CAACC,WAAW,CAAC,CAAC,CAACC,QAAQ,CAAC,CAAC,EAAE,GAAG,CAAE,GAAE,CACxE,CAAC;UACL,CAAC,MAAM;YACH5F,YAAY,CAACpC,IAAI,CAAC,IAAI,CAACW,SAAS,CAAC;UACrC;QACJ;MACJ;IACJ;IAEA,OAAOyB,YAAY;EACvB;AAEJ;;AAEA;AACA;AACA;AACA;AACA,MAAM6F,UAAU,SAAShK,QAAQ,CAAC;EAC9B;AACJ;AACA;EACImC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,OAAOS,UAAUA,CAACT,MAAM,EAAE;IACtB,IAAIA,MAAM,KAAK,IAAI,EAAE,OAAO,IAAI;IAChC,QAAQA,MAAM,CAACe,IAAI;MACf,KAAK,gBAAgB;QACjB,OAAO,IAAI8G,cAAc,CAAC7H,MAAM,CAAC;MACrC,KAAK,aAAa;QACd,OAAO,IAAI8H,WAAW,CAAC9H,MAAM,CAAC;MAClC,KAAK,UAAU;QACX,OAAO,IAAI+H,kBAAkB,CAAC/H,MAAM,CAAC;MACzC,KAAK,SAAS;QACV,OAAO,IAAIgI,OAAO,CAAChI,MAAM,CAAC;MAC9B,KAAK,KAAK;QACN,OAAO,IAAIiI,GAAG,CAACjI,MAAM,CAAC;MAC1B,KAAK,MAAM;QACP,OAAO,IAAIkI,IAAI,CAAClI,MAAM,CAAC;MAC3B,KAAK,cAAc;QACf,OAAO,IAAImI,YAAY,CAACnI,MAAM,CAAC;MACnC,KAAK,WAAW;QACZ,OAAO,IAAIoI,SAAS,CAACpI,MAAM,CAAC;MAChC,KAAK,SAAS;QACV,OAAO,IAAIqI,OAAO,CAACrI,MAAM,CAAC;MAC9B;QACI,MAAM,IAAImB,KAAK,CAAE,4BAA2BnB,MAAM,CAACe,IAAK,EAAC,CAAC;IAClE;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACIuH,SAASA,CAACnJ,IAAI,EAAE;IACZ,MAAMgC,KAAK,CAAC,8CAA8C,CAAC;EAC/D;;EAEA;AACJ;AACA;AACA;AACA;EACIC,KAAKA,CAACjC,IAAI,EAAE;IACR,OAAO,IAAI,CAACmJ,SAAS,CAACnJ,IAAI,CAAC;EAC/B;AAEJ;;AAEA;AACA;AACA;AACA;AACA,MAAM6I,OAAO,SAASJ,UAAU,CAAC;EAC7B;AACJ;AACA;AACA;AACA;EACIU,SAASA,CAACnJ,IAAI,EAAE;IACZ,IAAIP,OAAO,GAAGD,aAAa,CAAC,IAAI,CAACqB,MAAM,CAACpB,OAAO,CAAC;IAChD,IAAIA,OAAO,KAAK,IAAI,EAAE;MAClB,OAAOO,IAAI;IACf;IAEAA,IAAI,GAAGA,IAAI,CAACoJ,UAAU,CAAC3J,OAAO,EAAE,IAAI,CAACoB,MAAM,CAACwI,OAAO,CAAC;IAEpD,OAAOrJ,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM8I,GAAG,SAASL,UAAU,CAAC;EACzB;AACJ;AACA;AACA;AACA;EACIU,SAASA,CAACnJ,IAAI,EAAE;IACZA,IAAI,GAAGA,IAAI,CAACmJ,SAAS,CAAC,KAAK,CAAC;IAC5B,OAAOnJ,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM+I,IAAI,SAASN,UAAU,CAAC;EAC1B;AACJ;AACA;AACA;AACA;EACIU,SAASA,CAACnJ,IAAI,EAAE;IACZA,IAAI,GAAGA,IAAI,CAACmJ,SAAS,CAAC,MAAM,CAAC;IAC7B,OAAOnJ,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMgJ,YAAY,SAASP,UAAU,CAAC;EAClC;AACJ;AACA;AACA;AACA;EACIU,SAASA,CAACnJ,IAAI,EAAE;IACZA,IAAI,GAAGA,IAAI,CAACC,OAAO,CAAC,kBAAkB,EAAE,EAAE,CAAC;IAC3C,OAAOD,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMiJ,SAAS,SAASR,UAAU,CAAC;EAC/B;AACJ;AACA;AACA;AACA;EACIU,SAASA,CAACnJ,IAAI,EAAE;IACZA,IAAI,GAAGA,IAAI,CAACsJ,WAAW,CAAC,CAAC;IACzB,OAAOtJ,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMkJ,OAAO,SAAST,UAAU,CAAC;EAC7B;AACJ;AACA;AACA;AACA;EACIU,SAASA,CAACnJ,IAAI,EAAE;IACZA,IAAI,GAAG,IAAI,CAACa,MAAM,CAAC0I,OAAO,GAAGvJ,IAAI;IACjC,OAAOA,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM4I,kBAAkB,SAASH,UAAU,CAAC;EACxC;AACJ;AACA;AACA;AACA;EACI7H,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb,IAAI,CAAC2I,WAAW,GAAG3I,MAAM,CAAC2I,WAAW,CAAClH,GAAG,CAACwB,CAAC,IAAI2E,UAAU,CAACnH,UAAU,CAACwC,CAAC,CAAC,CAAC;EAC5E;EACA;AACJ;AACA;AACA;AACA;EACIqF,SAASA,CAACnJ,IAAI,EAAE;IACZ,OAAO,IAAI,CAACwJ,WAAW,CAAC/B,MAAM,CAAC,CAAClF,CAAC,EAAEkH,UAAU,KAAK;MAC9C,OAAOA,UAAU,CAACN,SAAS,CAAC5G,CAAC,CAAC;IAClC,CAAC,EAAEvC,IAAI,CAAC;EACZ;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM0I,cAAc,SAASD,UAAU,CAAC;EACpC;AACJ;AACA;AACA;AACA;AACA;EACIiB,uBAAuBA,CAAC1J,IAAI,EAAE;IAC1B;IACA,IAAI2J,MAAM,GAAG,EAAE;IACf,KAAK,IAAIrJ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,IAAI,CAACO,MAAM,EAAE,EAAED,CAAC,EAAE;MAClC,IAAI+G,IAAI,GAAGrH,IAAI,CAACM,CAAC,CAAC;MAClB,IAAIsJ,EAAE,GAAGvC,IAAI,CAACvB,UAAU,CAAC,CAAC,CAAC;MAC3B,IAAI,IAAI,CAAC+D,gBAAgB,CAACD,EAAE,CAAC,EAAE;QAC3BD,MAAM,CAACnJ,IAAI,CAAC,GAAG,CAAC;QAChBmJ,MAAM,CAACnJ,IAAI,CAAC6G,IAAI,CAAC;QACjBsC,MAAM,CAACnJ,IAAI,CAAC,GAAG,CAAC;MACpB,CAAC,MAAM;QACHmJ,MAAM,CAACnJ,IAAI,CAAC6G,IAAI,CAAC;MACrB;IACJ;IACA,OAAOsC,MAAM,CAACrG,IAAI,CAAC,EAAE,CAAC;EAC1B;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIuG,gBAAgBA,CAACD,EAAE,EAAE;IACjB,OACKA,EAAE,IAAI,MAAM,IAAIA,EAAE,IAAI,MAAM,IACzBA,EAAE,IAAI,MAAM,IAAIA,EAAE,IAAI,MAAO,IAC7BA,EAAE,IAAI,OAAO,IAAIA,EAAE,IAAI,OAAQ,IAC/BA,EAAE,IAAI,OAAO,IAAIA,EAAE,IAAI,OAAQ,IAC/BA,EAAE,IAAI,OAAO,IAAIA,EAAE,IAAI,OAAQ,IAC/BA,EAAE,IAAI,OAAO,IAAIA,EAAE,IAAI,OAAQ,IAC/BA,EAAE,IAAI,MAAM,IAAIA,EAAE,IAAI,MAAO,IAC7BA,EAAE,IAAI,OAAO,IAAIA,EAAE,IAAI,OAAQ;EAE3C;EACA;AACJ;AACA;AACA;AACA;EACIE,YAAYA,CAAC9J,IAAI,EAAE;IACf,OAAOA,IAAI,CAACmJ,SAAS,CAAC,KAAK,CAAC,CAAClJ,OAAO,CAAC,kBAAkB,EAAE,EAAE,CAAC;EAChE;;EAEA;AACJ;AACA;AACA;AACA;EACIkJ,SAASA,CAACnJ,IAAI,EAAE;IACZ;IACA;IACA;IACA;IACA;;IAEA,IAAI,IAAI,CAACa,MAAM,CAACkJ,oBAAoB,EAAE;MAClC/J,IAAI,GAAG,IAAI,CAAC0J,uBAAuB,CAAC1J,IAAI,CAAC;IAC7C;IAEA,IAAI,IAAI,CAACa,MAAM,CAACmJ,SAAS,EAAE;MACvBhK,IAAI,GAAGA,IAAI,CAACsJ,WAAW,CAAC,CAAC;MAEzB,IAAI,IAAI,CAACzI,MAAM,CAACoJ,aAAa,KAAK,KAAK,EAAE;QACrCjK,IAAI,GAAG,IAAI,CAAC8J,YAAY,CAAC9J,IAAI,CAAC;MAClC;IACJ,CAAC,MAAM,IAAI,IAAI,CAACa,MAAM,CAACoJ,aAAa,EAAE;MAClCjK,IAAI,GAAG,IAAI,CAAC8J,YAAY,CAAC9J,IAAI,CAAC;IAClC;IAEA,OAAOA,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAMkK,YAAY,SAASzL,QAAQ,CAAC;EAChC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI,OAAO6C,UAAUA,CAACT,MAAM,EAAE;IACtB,IAAIA,MAAM,KAAK,IAAI,EAAE,OAAO,IAAI;IAEhC,QAAQA,MAAM,CAACe,IAAI;MACf,KAAK,kBAAkB;QACnB,OAAO,IAAIuI,gBAAgB,CAACtJ,MAAM,CAAC;MACvC,KAAK,UAAU;QACX,OAAO,IAAIuJ,oBAAoB,CAACvJ,MAAM,CAAC;MAC3C,KAAK,iBAAiB;QAClB,OAAO,IAAIwJ,eAAe,CAACxJ,MAAM,CAAC;MACtC,KAAK,WAAW;QACZ,OAAO,IAAIyJ,qBAAqB,CAACzJ,MAAM,CAAC;MAE5C,KAAK,WAAW;QACZ,OAAO,IAAI0J,qBAAqB,CAAC1J,MAAM,CAAC;MAC5C,KAAK,OAAO;QACR,OAAO,IAAI2J,iBAAiB,CAAC3J,MAAM,CAAC;MAExC;QACI,MAAM,IAAImB,KAAK,CAAE,8BAA6BnB,MAAM,CAACe,IAAK,EAAC,CAAC;IACpE;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI6I,iBAAiBA,CAACzK,IAAI,EAAE;IACpB,MAAMgC,KAAK,CAAC,sDAAsD,CAAC;EACvE;;EAEA;AACJ;AACA;AACA;AACA;EACI0I,YAAYA,CAAC1K,IAAI,EAAE;IACf,IAAI2K,MAAM,GAAG,EAAE;IACf,IAAIjJ,KAAK,CAACkJ,OAAO,CAAC5K,IAAI,CAAC,EAAE;MACrB2K,MAAM,GAAG3K,IAAI,CAACsC,GAAG,CAACwB,CAAC,IAAI,IAAI,CAAC2G,iBAAiB,CAAC3G,CAAC,CAAC,CAAC;IACrD,CAAC,MAAM;MACH6G,MAAM,GAAG,IAAI,CAACF,iBAAiB,CAACzK,IAAI,CAAC;IACzC;IACA,OAAO2K,MAAM,CAACE,IAAI,CAAC,CAAC;EACxB;;EAEA;AACJ;AACA;AACA;AACA;EACI5I,KAAKA,CAACjC,IAAI,EAAE;IACR,OAAO,IAAI,CAAC0K,YAAY,CAAC1K,IAAI,CAAC;EAClC;AACJ;;AAEA;AACA;AACA;AACA,MAAMmK,gBAAgB,SAASD,YAAY,CAAC;EACxC;AACJ;AACA;AACA;AACA;AACA;EACItJ,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP;;IAEA;IACA;IACA;IACA,MAAMiK,WAAW,GAAG,oEAAoE;IACxF,IAAI,CAACrL,OAAO,GAAG,IAAIE,MAAM,CAAE,QAAOmL,WAAY,OAAMA,WAAY,GAAE,EAAE,IAAI,CAAC;EAC7E;EACA;AACJ;AACA;AACA;AACA;AACA;EACIL,iBAAiBA,CAACzK,IAAI,EAAE;IACpB,OAAOA,IAAI,CAAC+K,IAAI,CAAC,CAAC,CAACrK,KAAK,CAAC,IAAI,CAACjB,OAAO,CAAC,IAAI,EAAE;EAChD;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM8K,qBAAqB,SAASL,YAAY,CAAC;EAC7C;AACJ;AACA;AACA;EACItJ,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;;IAEpB;AACR;AACA;AACA;IACQ,IAAI,CAACmK,gBAAgB,GAAG,IAAI,CAACnK,MAAM,CAACmK,gBAAgB;;IAEpD;AACR;AACA;AACA;AACA;IACQ,IAAI,CAACC,YAAY,GAAG,IAAI,CAACpK,MAAM,CAACoK,YAAY;;IAE5C;AACR;AACA;AACA;IACQ,IAAI,CAACC,SAAS,GAAG,IAAI,CAACrK,MAAM,CAACqK,SAAS,IAAI,IAAI;IAC9C,IAAI,CAACzL,OAAO,GAAG,8EAA8E;IAE7F,IAAI,CAAC0L,YAAY,GAAGxF,gBAAgB;IACpC,IAAI,CAACiB,YAAY,GAAG,IAAIC,WAAW,CAAC,CAAC;EACzC;;EAEA;AACJ;AACA;AACA;AACA;EACI4D,iBAAiBA,CAACzK,IAAI,EAAE;IACpB;IACA,IAAIkC,MAAM,GAAG,IAAI,CAACgJ,SAAS,GAAIlL,IAAI,CAACU,KAAK,CAAC,IAAI,CAACjB,OAAO,CAAC,IAAI,EAAE,GAAI,CAACO,IAAI,CAAC;IAEvE,OAAOkC,MAAM,CAACI,GAAG,CAACO,KAAK,IAAI;MACvB,IAAI,IAAI,CAACmI,gBAAgB,IAAI,CAACnI,KAAK,CAACuI,UAAU,CAAC,GAAG,CAAC,EAAE;QACjDvI,KAAK,GAAG,GAAG,GAAGA,KAAK;MACvB;;MAEA;MACAA,KAAK,GAAGnB,KAAK,CAACmE,IAAI,CAAC,IAAI,CAACe,YAAY,CAACzE,MAAM,CAACU,KAAK,CAAC,EAAEwI,IAAI,IAAI,IAAI,CAACF,YAAY,CAACE,IAAI,CAAC,CAAC,CAAC/H,IAAI,CAAC,EAAE,CAAC;MAE7F,OAAOT,KAAK;IAChB,CAAC,CAAC;EACN;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM2H,iBAAiB,SAASN,YAAY,CAAC;EACzC;AACJ;AACA;AACA;AACA;AACA;AACA;EACItJ,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;;EAEA;AACJ;AACA;AACA;AACA;EACI4J,iBAAiBA,CAACzK,IAAI,EAAE;IACpB,IAAIP,OAAO,GAAGD,aAAa,CAAC,IAAI,CAACqB,MAAM,CAACpB,OAAO,CAAC;IAChD,IAAIA,OAAO,KAAK,IAAI,EAAE;MAClB,OAAO,EAAE;IACb;IAEA,QAAQ,IAAI,CAACoB,MAAM,CAACyK,QAAQ,CAAChC,WAAW,CAAC,CAAC;MACtC;MACA;MACA,KAAK,UAAU;MACf,KAAK,SAAS;QACV,OAAOtJ,IAAI,CAACU,KAAK,CAACjB,OAAO,CAAC,IAAI,EAAE;MACpC;QACII,OAAO,CAACC,IAAI,CAAE,4BAA2B,IAAI,CAACe,MAAM,CAACyK,QAAS,GAAE,CAAC;QACjE,OAAO,EAAE;IACjB;EACJ;AACJ;;AAEA;AACA;AACA;AACA,MAAMC,aAAa,SAAS9M,QAAQ,CAAC;EAEjC;AACJ;AACA;EACImC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,OAAOS,UAAUA,CAACT,MAAM,EAAE;IACtB,QAAQA,MAAM,CAACe,IAAI;MACf,KAAK,oBAAoB;QACrB,OAAO,IAAI4J,kBAAkB,CAAC3K,MAAM,CAAC;MAEzC,KAAK,WAAW;QACZ,OAAO,IAAI4K,sBAAsB,CAAC5K,MAAM,CAAC;MAE7C,KAAK,mBAAmB;QACpB,OAAO,IAAI6K,iBAAiB,CAAC7K,MAAM,CAAC;MAExC;QACI,MAAM,IAAImB,KAAK,CAAE,+BAA8BnB,MAAM,CAACe,IAAK,EAAC,CAAC;IACrE;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI+J,YAAYA,CAACzJ,MAAM,EAAW;IAC1B,MAAMF,KAAK,CAAC,iDAAiD,CAAC;EAClE;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIC,KAAKA,CAACC,MAAM,EAAW;IAAA,SAAA0J,KAAA,GAAApK,SAAA,CAAAjB,MAAA,EAANkB,IAAI,OAAAC,KAAA,CAAAkK,KAAA,OAAAA,KAAA,WAAAC,KAAA,MAAAA,KAAA,GAAAD,KAAA,EAAAC,KAAA;MAAJpK,IAAI,CAAAoK,KAAA,QAAArK,SAAA,CAAAqK,KAAA;IAAA;IACjB,OAAO,IAAI,CAACF,YAAY,CAACzJ,MAAM,EAAE,GAAGT,IAAI,CAAC;EAC7C;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMiK,iBAAiB,SAASH,aAAa,CAAC;EAC1C;AACJ;AACA;AACA;AACA;EACI3K,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb;;IAEA,IAAI,CAACiL,GAAG,GAAGjL,MAAM,CAACiL,GAAG,CAAC,CAAC,CAAC;IACxB,IAAI,CAACC,GAAG,GAAGlL,MAAM,CAACkL,GAAG,CAAC,CAAC,CAAC;EAC5B;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIJ,YAAYA,CAACzJ,MAAM,EAAsB;IAAA,IAApB8J,WAAW,GAAAxK,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,IAAI;IACnCU,MAAM,GAAGrD,WAAW,CAAC,CAAC,IAAI,CAACiN,GAAG,CAAC,EAAE5J,MAAM,EAAE,CAAC,IAAI,CAAC6J,GAAG,CAAC,CAAC;;IAEpD;IACA;IACA,IAAIC,WAAW,KAAK,IAAI,EAAE;MACtB9J,MAAM,GAAGrD,WAAW,CAACqD,MAAM,EAAE,CAAC,IAAI,CAAC6J,GAAG,CAAC,EAAEC,WAAW,EAAE,CAAC,IAAI,CAACD,GAAG,CAAC,CAAC;IACrE;IACA,OAAO7J,MAAM;EACjB;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMsJ,kBAAkB,SAASD,aAAa,CAAC;EAC3C;AACJ;AACA;AACA;AACA;AACA;EACI3K,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACoL,MAAM,GAAGpL,MAAM,CAACoL,MAAM;IAC3B,IAAI,CAACC,IAAI,GAAGrL,MAAM,CAACqL,IAAI;EAC3B;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIP,YAAYA,CAACzJ,MAAM,EAAsB;IAAA,IAApB8J,WAAW,GAAAxK,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,IAAI;IACnC,IAAII,IAAI,GAAGoK,WAAW,KAAK,IAAI,GAAG,IAAI,CAACC,MAAM,GAAG,IAAI,CAACC,IAAI;IAEzD,IAAIzG,QAAQ,GAAG,EAAE;IACjB,KAAK,IAAI0G,IAAI,IAAIvK,IAAI,EAAE;MACnB,IAAI,cAAc,IAAIuK,IAAI,EAAE;QACxB1G,QAAQ,CAACjF,IAAI,CAAC2L,IAAI,CAACC,YAAY,CAACC,EAAE,CAAC;MAEvC,CAAC,MAAM,IAAI,UAAU,IAAIF,IAAI,EAAE;QAC3B,IAAIA,IAAI,CAACG,QAAQ,CAACD,EAAE,KAAK,GAAG,EAAE;UAC1B5G,QAAQ,GAAG5G,WAAW,CAAC4G,QAAQ,EAAEvD,MAAM,CAAC;QAE5C,CAAC,MAAM,IAAIiK,IAAI,CAACG,QAAQ,CAACD,EAAE,KAAK,GAAG,EAAE;UACjC5G,QAAQ,GAAG5G,WAAW,CAAC4G,QAAQ,EAAEuG,WAAW,CAAC;QACjD;MACJ;IACJ;IACA,OAAOvG,QAAQ;EACnB;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMgG,sBAAsB,SAASF,aAAa,CAAC;EAC/C;AACJ;AACA;AACA;AACA;EACII,YAAYA,CAACzJ,MAAM,EAAE;IACjB,OAAOA,MAAM;EACjB;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMqK,OAAO,SAAS9N,QAAQ,CAAC;EAE3B;AACJ;AACA;AACA;AACA;EACImC,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAACA,MAAM,GAAGA,MAAM;IAEpB,IAAI,CAAC2L,YAAY,GAAG,EAAE;IACtB,IAAI,CAACpL,kBAAkB,GAAG,IAAI;IAC9B,IAAI,CAAC6J,YAAY,GAAGpK,MAAM,CAACoK,YAAY;EAC3C;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI,OAAO3J,UAAUA,CAACT,MAAM,EAAE;IACtB,QAAQA,MAAM,CAACe,IAAI;MACf,KAAK,WAAW;QACZ,OAAO,IAAI6K,gBAAgB,CAAC5L,MAAM,CAAC;MACvC,KAAK,WAAW;QACZ,OAAO,IAAI6L,gBAAgB,CAAC7L,MAAM,CAAC;MACvC,KAAK,WAAW;QACZ,OAAO,IAAI8L,gBAAgB,CAAC9L,MAAM,CAAC;MAEvC,KAAK,SAAS;QACV,OAAO,IAAI+L,cAAc,CAAC/L,MAAM,CAAC;MACrC,KAAK,cAAc;QACf,OAAO,IAAIgM,YAAY,CAAChM,MAAM,CAAC;MACnC,KAAK,MAAM;QACP,OAAO,IAAIiM,WAAW,CAACjM,MAAM,CAAC;MAClC,KAAK,OAAO;QACR,OAAO,IAAIkM,YAAY,CAAClM,MAAM,CAAC;MAEnC,KAAK,UAAU;QACX,OAAO,IAAImM,eAAe,CAACnM,MAAM,CAAC;MAEtC;QACI,MAAM,IAAImB,KAAK,CAAE,yBAAwBnB,MAAM,CAACe,IAAK,EAAC,CAAC;IAC/D;EACJ;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIK,KAAKA,CAACC,MAAM,EAAE;IACV,OAAO,IAAI,CAAC+K,MAAM,CAAC/K,MAAM,CAAC;EAC9B;;EAEA;AACJ;AACA;AACA;AACA;EACI+K,MAAMA,CAAC/K,MAAM,EAAE;IACX,OAAO,IAAI,CAACgL,YAAY,CAAChL,MAAM,CAAC,CAACoB,IAAI,CAAC,EAAE,CAAC;EAC7C;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACI4J,YAAYA,CAAChL,MAAM,EAAE;IACjB,MAAMF,KAAK,CAAC,mDAAmD,CAAC;EACpE;AAEJ;AAEA,MAAM4K,cAAc,SAASL,OAAO,CAAC;EACjC3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;EACjB;;EAEA;EACAqM,YAAYA,CAAChL,MAAM,EAAE;IACjB,IAAIzC,OAAO,GAAGD,aAAa,CAAC,IAAI,CAACqB,MAAM,CAACpB,OAAO,CAAC;IAChD,IAAIA,OAAO,KAAK,IAAI,EAAE;MAClB,OAAOyC,MAAM;IACjB;IAEA,OAAOA,MAAM,CAACI,GAAG,CAACO,KAAK,IAAIA,KAAK,CAACuG,UAAU,CAAC3J,OAAO,EAAE,IAAI,CAACoB,MAAM,CAACwI,OAAO,CAAC,CAAC;EAC9E;AACJ;AAGA,MAAMwD,YAAY,SAASN,OAAO,CAAC;EAC/B3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACsM,YAAY,GAAG,IAAIC,WAAW,CAAC,CAAC;EACzC;;EAEA;EACAF,YAAYA,CAAChL,MAAM,EAAE;IAEjB,IAAImL,UAAU,GAAG,EAAE;IACnB,IAAIC,oBAAoB,GAAG,EAAE;IAE7B,KAAK,IAAIzK,KAAK,IAAIX,MAAM,EAAE;MACtB,IAAIqL,KAAK,GAAG,IAAI;MAChB,IAAI1K,KAAK,CAACtC,MAAM,KAAK,CAAC,IAAIsC,KAAK,CAACuI,UAAU,CAAC,KAAK,CAAC,IAAIvI,KAAK,CAAC2K,QAAQ,CAAC,GAAG,CAAC,EAAE;QACtE,IAAInC,IAAI,GAAGoC,QAAQ,CAAC5K,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC;QAC1C,IAAI,CAACqK,KAAK,CAACrC,IAAI,CAAC,EAAE;UACdkC,KAAK,GAAGlC,IAAI;QAChB;MACJ;MACA,IAAIkC,KAAK,KAAK,IAAI,EAAE;QAChBD,oBAAoB,CAAC9M,IAAI,CAAC+M,KAAK,CAAC;MACpC,CAAC,MAAM;QACH,IAAID,oBAAoB,CAAC/M,MAAM,GAAG,CAAC,EAAE;UACjC,IAAIoN,MAAM,GAAG,IAAI,CAACR,YAAY,CAACF,MAAM,CAACW,UAAU,CAAC/H,IAAI,CAACyH,oBAAoB,CAAC,CAAC;UAC5ED,UAAU,CAAC7M,IAAI,CAACmN,MAAM,CAAC;UACvBL,oBAAoB,GAAG,EAAE;QAC7B;QACAD,UAAU,CAAC7M,IAAI,CAACqC,KAAK,CAAC;MAC1B;IACJ;IACA,IAAIyK,oBAAoB,CAAC/M,MAAM,GAAG,CAAC,EAAE;MACjC,IAAIoN,MAAM,GAAG,IAAI,CAACR,YAAY,CAACF,MAAM,CAACW,UAAU,CAAC/H,IAAI,CAACyH,oBAAoB,CAAC,CAAC;MAC5ED,UAAU,CAAC7M,IAAI,CAACmN,MAAM,CAAC;MACvBL,oBAAoB,GAAG,EAAE;IAC7B;IAEA,OAAOD,UAAU;EACrB;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAMP,WAAW,SAASP,OAAO,CAAC;EAC9B3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;EACjB;;EAEA;EACAqM,YAAYA,CAAChL,MAAM,EAAE;IACjB,OAAO,CAACA,MAAM,CAACoB,IAAI,CAAC,EAAE,CAAC,CAAC;EAC5B;AACJ;AAEA,MAAMyJ,YAAY,SAASR,OAAO,CAAC;EAC/B3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACwI,OAAO,GAAG,IAAI,CAACxI,MAAM,CAACwI,OAAO;IAClC,IAAI,CAACrG,KAAK,GAAG,IAAI,CAACnC,MAAM,CAACmC,KAAK;IAC9B,IAAI,CAAC6K,IAAI,GAAG,IAAI,CAAChN,MAAM,CAACgN,IAAI;EAChC;;EAEA;EACAX,YAAYA,CAAChL,MAAM,EAAE;IACjB,OAAOA,MAAM,CAACI,GAAG,CAACO,KAAK,IAAI;MACvB,IAAIiL,SAAS,GAAG,CAAC;MACjB,KAAK,IAAIxN,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAAC0C,KAAK,EAAE,EAAE1C,CAAC,EAAE;QACjC,IAAIuC,KAAK,CAACvC,CAAC,CAAC,KAAK,IAAI,CAAC+I,OAAO,EAAE;UAC3ByE,SAAS,GAAGxN,CAAC,GAAG,CAAC;UACjB;QACJ,CAAC,MAAM;UACH;QACJ;MACJ;MAEA,IAAIyN,QAAQ,GAAGlL,KAAK,CAACtC,MAAM;MAC3B,KAAK,IAAID,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACuN,IAAI,EAAE,EAAEvN,CAAC,EAAE;QAChC,MAAM0N,KAAK,GAAGnL,KAAK,CAACtC,MAAM,GAAGD,CAAC,GAAG,CAAC;QAClC,IAAIuC,KAAK,CAACmL,KAAK,CAAC,KAAK,IAAI,CAAC3E,OAAO,EAAE;UAC/B0E,QAAQ,GAAGC,KAAK;UAChB;QACJ,CAAC,MAAM;UACH;QACJ;MACJ;MAEA,OAAOnL,KAAK,CAACQ,KAAK,CAACyK,SAAS,EAAEC,QAAQ,CAAC;IAC3C,CAAC,CAAC;EACN;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMtB,gBAAgB,SAASF,OAAO,CAAC;EAEnC;AACJ;AACA;AACA;AACA;AACA;EACI3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb,IAAI,CAACoN,OAAO,GAAGpN,MAAM,CAACoN,OAAO;EACjC;;EAEA;EACAf,YAAYA,CAAChL,MAAM,EAAE;IACjB,OAAOA,MAAM,CAACI,GAAG,CAAC,CAACO,KAAK,EAAEvC,CAAC,KAAK;MAC5B,IAAIA,CAAC,KAAK,CAAC,EAAE;QACT,IAAIuC,KAAK,CAACuI,UAAU,CAAC,IAAI,CAACvK,MAAM,CAACqN,MAAM,CAAC,EAAE;UACtC;UACArL,KAAK,GAAGA,KAAK,CAAC5C,OAAO,CAAC,IAAI,CAACY,MAAM,CAACqN,MAAM,EAAE,EAAE,CAAC;QACjD,CAAC,MAAM;UACHrL,KAAK,GAAG,GAAG,GAAGA,KAAK;QACvB;MACJ;MACA,IAAI,IAAI,CAACoL,OAAO,EAAE;QACdpL,KAAK,GAAG9C,qBAAqB,CAAC8C,KAAK,CAAC;MACxC;MAEA,OAAOA,KAAK;IAChB,CAAC,CAAC;EACN;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM8J,gBAAgB,SAASJ,OAAO,CAAC;EAEnC;AACJ;AACA;AACA;EACI3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACsN,YAAY,GAAG5H,gBAAgB;IACpC,IAAI,CAAC4G,YAAY,GAAG,IAAIC,WAAW,CAAC,OAAO,EAAE;MACzCgB,KAAK,EAAE,KAAK;MACZC,SAAS,EAAE;IACf,CAAC,CAAC;IAEF,IAAI,CAACjN,kBAAkB,GAAG,IAAI;EAClC;;EAEA;AACJ;AACA;AACA;AACA;EACIkN,wBAAwBA,CAACpM,MAAM,EAAE;IAC7B,IAAIlC,IAAI,GAAGkC,MAAM,CAACoB,IAAI,CAAC,EAAE,CAAC;IAE1B,IAAIiL,SAAS,GAAG,IAAIX,UAAU,CAAC,CAAC,GAAG5N,IAAI,CAAC,CAACsC,GAAG,CAACqF,CAAC,IAAI,IAAI,CAACwG,YAAY,CAACxG,CAAC,CAAC,CAAC,CAAC;IACxE,IAAI6G,YAAY,GAAG,IAAI,CAACrB,YAAY,CAACF,MAAM,CAACsB,SAAS,CAAC;IACtD,OAAOC,YAAY;EACvB;;EAEA;EACAtB,YAAYA,CAAChL,MAAM,EAAE;IACjB;IACA;;IAEA;IACA;IACA;IACA,IAAIuM,SAAS,GAAG,EAAE;IAClB,IAAIC,gBAAgB,GAAG,EAAE;IACzB,KAAK,IAAI7L,KAAK,IAAIX,MAAM,EAAE;MACtB;MACA;MACA;MACA;;MAEA,IAAI,IAAI,CAACsK,YAAY,CAACtG,QAAQ,CAACrD,KAAK,CAAC,EAAE;QACnC,IAAI6L,gBAAgB,CAACnO,MAAM,GAAG,CAAC,EAAE;UAC7BkO,SAAS,CAACjO,IAAI,CAAC,IAAI,CAAC8N,wBAAwB,CAACI,gBAAgB,CAAC,CAAC;UAC/DA,gBAAgB,GAAG,EAAE;QACzB;QACAD,SAAS,CAACjO,IAAI,CAACqC,KAAK,CAAC;MACzB,CAAC,MAAM;QACH6L,gBAAgB,CAAClO,IAAI,CAACqC,KAAK,CAAC;MAChC;IACJ;IACA,IAAI6L,gBAAgB,CAACnO,MAAM,GAAG,CAAC,EAAE;MAC7BkO,SAAS,CAACjO,IAAI,CAAC,IAAI,CAAC8N,wBAAwB,CAACI,gBAAgB,CAAC,CAAC;IACnE;;IAEA;;IAEA,OAAOD,SAAS;EACpB;AACJ;;AAGA;AACA;AACA;AACA;AACA,MAAMzB,eAAe,SAAST,OAAO,CAAC;EAElC;AACJ;AACA;AACA;AACA;EACI3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb,IAAI,CAAC8N,QAAQ,GAAG9N,MAAM,CAAC8N,QAAQ,CAACrM,GAAG,CAACwB,CAAC,IAAIyI,OAAO,CAACjL,UAAU,CAACwC,CAAC,CAAC,CAAC;EACnE;;EAEA;EACAoJ,YAAYA,CAAChL,MAAM,EAAE;IACjB;IACA,OAAO,IAAI,CAACyM,QAAQ,CAAClH,MAAM,CAAC,CAACmH,IAAI,EAAEC,OAAO,KAAK;MAC3C,OAAOA,OAAO,CAAC3B,YAAY,CAAC0B,IAAI,CAAC;IACrC,CAAC,EAAE1M,MAAM,CAAC;EACd;AAEJ;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAMoI,qBAAqB,SAASJ,YAAY,CAAC;EAC7C;AACJ;AACA;AACA;AACA;AACA;EACItJ,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IAEP,IAAI,CAACiO,cAAc,GAAGjO,MAAM,CAACmK,gBAAgB;IAC7C,IAAI,CAAC+D,WAAW,GAAGlO,MAAM,CAACkO,WAAW;IACrC,IAAI,CAACC,MAAM,GAAGnO,MAAM,CAACoO,OAAO,IAAI,IAAI,CAACF,WAAW;EACpD;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIrE,YAAYA,CAACwE,gBAAgB,EAAE;IAC3B,IAAI,OAAOA,gBAAgB,KAAK,QAAQ,EAAE;MACtC;MACA;MACA;MACAA,gBAAgB,GAAGA,gBAAgB,CAACC,SAAS,CAAC,CAAC,CAACzI,KAAK,CAAC,KAAK,CAAC;IAChE;IAEA,MAAMiE,MAAM,GAAG,EAAE;IACjB,KAAK,IAAI9H,KAAK,IAAIqM,gBAAgB,EAAE;MAChC,IAAI3J,UAAU,GAAG1C,KAAK,CAACuG,UAAU,CAAC,GAAG,EAAE,IAAI,CAAC4F,MAAM,CAAC;MACnD,IAAI,IAAI,CAACF,cAAc,IAAI,CAACvJ,UAAU,CAAC6F,UAAU,CAAC,IAAI,CAAC2D,WAAW,CAAC,EAAE;QACjExJ,UAAU,GAAG,IAAI,CAACyJ,MAAM,GAAGzJ,UAAU;MACzC;MACAoF,MAAM,CAACnK,IAAI,CAAC+E,UAAU,CAAC;IAC3B;IACA,OAAOoF,MAAM;EACjB;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAM+B,gBAAgB,SAASH,OAAO,CAAC;EACnC;AACJ;AACA;AACA;AACA;AACA;EACI3L,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IAEb,IAAI,CAACiO,cAAc,GAAGjO,MAAM,CAACmK,gBAAgB;IAC7C,IAAI,CAAC+D,WAAW,GAAGlO,MAAM,CAACkO,WAAW;EACzC;;EAEA;EACA7B,YAAYA,CAAChL,MAAM,EAAE;IACjB,IAAIyI,MAAM,GAAG,EAAE;IACf,KAAK,IAAIrK,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG4B,MAAM,CAAC3B,MAAM,EAAE,EAAED,CAAC,EAAE;MACpC,IAAIiF,UAAU,GAAGrD,MAAM,CAAC5B,CAAC,CAAC,CAAC8I,UAAU,CAAC,IAAI,CAAC2F,WAAW,EAAE,GAAG,CAAC;MAC5D,IAAI,IAAI,CAACD,cAAc,IAAIxO,CAAC,IAAI,CAAC,IAAIiF,UAAU,CAAC6F,UAAU,CAAC,GAAG,CAAC,EAAE;QAC7D7F,UAAU,GAAGA,UAAU,CAAC6J,SAAS,CAAC,CAAC,CAAC;MACxC;MACAzE,MAAM,CAACnK,IAAI,CAAC+E,UAAU,CAAC;IAC3B;IACA,OAAOoF,MAAM;EACjB;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAMhC,WAAW,SAASF,UAAU,CAAC;EACjC;AACJ;AACA;AACA;AACA;EACI7H,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACb,IAAI,CAACwO,QAAQ,GAAGxO,MAAM,CAACyO,oBAAoB;EAC/C;;EAEA;AACJ;AACA;AACA;AACA;EACInG,SAASA,CAACnJ,IAAI,EAAE;IACZ;IACA,OAAOA,IAAI;EACf;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMoK,oBAAoB,SAASF,YAAY,CAAC;EAC5C;AACJ;AACA;AACA;AACA;EACItJ,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;IACP,IAAI,CAAC0O,UAAU,GAAG1O,MAAM,CAAC2O,aAAa,CAAClN,GAAG,CAACwB,CAAC,IAAIoG,YAAY,CAAC5I,UAAU,CAACwC,CAAC,CAAC,CAAC;EAC/E;;EAEA;AACJ;AACA;AACA;AACA;EACI2G,iBAAiBA,CAACzK,IAAI,EAAE;IACpB,IAAI,OAAOA,IAAI,KAAK,QAAQ,EAAE;MAC1BA,IAAI,GAAG,CAACA,IAAI,CAAC;IACjB;IACA;IACA,OAAO,IAAI,CAACuP,UAAU,CAAC9H,MAAM,CAAC,CAACgI,gBAAgB,EAAEC,SAAS,KAAK;MAC3D,OAAOA,SAAS,CAAChF,YAAY,CAAC+E,gBAAgB,CAAC;IACnD,CAAC,EAAEzP,IAAI,CAAC;EACZ;AACJ;;AAEA;AACA;AACA;AACA;AACA,MAAMqK,eAAe,SAASH,YAAY,CAAC;EACvC;AACJ;AACA;AACA;EACItJ,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAAC,CAAC;EACX;EACA;AACJ;AACA;AACA;AACA;EACI4J,iBAAiBA,CAACzK,IAAI,EAAE;IACpB,OAAOS,gBAAgB,CAACT,IAAI,CAAC;EACjC;AACJ;AAEA,OAAO,MAAM2P,mBAAmB,SAASlR,QAAQ,CAAC;EAC9C;AACJ;AACA;AACA;AACA;EACImC,WAAWA,CAACgP,aAAa,EAAEC,eAAe,EAAE;IACxC,KAAK,CAAC,CAAC;;IAEP;IACA,IAAI,CAACpG,UAAU,GAAGhB,UAAU,CAACnH,UAAU,CAACsO,aAAa,CAACnG,UAAU,CAAC;IACjE,IAAI,CAACqG,aAAa,GAAG5F,YAAY,CAAC5I,UAAU,CAACsO,aAAa,CAACE,aAAa,CAAC;;IAEzE;IACA,IAAIF,aAAa,CAACG,KAAK,CAACjP,KAAK,EAAE;MAC3B,IAAI,CAACY,KAAK,CAACkJ,OAAO,CAACgF,aAAa,CAACG,KAAK,CAACjP,KAAK,CAAC,EAAE;QAC3C8O,aAAa,CAACG,KAAK,CAACjP,KAAK,GAAGuF,MAAM,CAAC2J,OAAO,CAACJ,aAAa,CAACG,KAAK,CAACjP,KAAK,CAAC;MACzE;MACA8O,aAAa,CAACG,KAAK,CAACjP,KAAK,GAAG,IAAIE,GAAG,CAAC4O,aAAa,CAACG,KAAK,CAACjP,KAAK,CAAC;IAClE;IACA,IAAI,CAACiP,KAAK,GAAGpP,cAAc,CAACW,UAAU,CAACsO,aAAa,CAACG,KAAK,EAAEF,eAAe,CAAC;IAC5E,IAAI,CAACI,cAAc,GAAG1E,aAAa,CAACjK,UAAU,CAACsO,aAAa,CAACK,cAAc,CAAC;;IAE5E;IACA,IAAI,CAACpB,OAAO,GAAGtC,OAAO,CAACjL,UAAU,CAACsO,aAAa,CAACf,OAAO,CAAC;;IAGxD;IACA;IACA;IACA;IACA,IAAI,CAACA,OAAO,CAACzN,kBAAkB,GAAG,IAAI,CAAC2O,KAAK,CAAC3O,kBAAkB;;IAE/D;IACA,IAAI,CAAC8O,cAAc,GAAG,EAAE;IACxB,IAAI,CAACC,eAAe,GAAG,EAAE;IACzB,IAAI,CAAC3D,YAAY,GAAG,EAAE;IACtB,KAAK,IAAI4D,UAAU,IAAIR,aAAa,CAACpD,YAAY,EAAE;MAC/C,IAAIH,EAAE,GAAG+D,UAAU,CAAC/D,EAAE;MACtB,IAAIhD,OAAO,GAAG+G,UAAU,CAAC/G,OAAO;MAEhC,IAAI,CAACmD,YAAY,CAAChM,IAAI,CAAC6I,OAAO,CAAC;MAE/B,IAAI,CAAC0G,KAAK,CAAChP,aAAa,CAACsP,GAAG,CAAChH,OAAO,EAAEgD,EAAE,CAAC;MACzC,IAAI,CAAC0D,KAAK,CAACjP,KAAK,CAACuL,EAAE,CAAC,GAAGhD,OAAO;MAE9B,IAAI+G,UAAU,CAACE,OAAO,EAAE;QACpB,IAAI,CAACJ,cAAc,CAAC1P,IAAI,CAAC6I,OAAO,CAAC;QACjC,IAAI,CAAC8G,eAAe,CAAC3P,IAAI,CAAC6L,EAAE,CAAC;MACjC;IACJ;;IAEA;IACA,IAAI,CAACwC,OAAO,CAACrC,YAAY,GAAG,IAAI,CAACA,YAAY;IAE7C,IAAI,CAAC+D,kBAAkB,GAAG,IAAI5Q,MAAM,CAChC,GAAG,GAAG,IAAI,CAAC6M,YAAY,CAAClK,GAAG,CAAC3D,YAAY,CAAC,CAAC2E,IAAI,CAAC,GAAG,CAAC,GAAG,GAC1D,CAAC;;IAED;IACA,IAAI,CAACkN,UAAU,GAAG,IAAI,CAACC,QAAQ,CAACZ,eAAe,EAAE,YAAY,CAAC;IAC9D,IAAI,CAACa,aAAa,GAAG,IAAI,CAACX,KAAK,CAAChP,aAAa,CAACyB,GAAG,CAAC,IAAI,CAACgO,UAAU,CAAC;IAElE,IAAI,CAACG,SAAS,GAAG,IAAI,CAACF,QAAQ,CAACZ,eAAe,EAAE,WAAW,EAAE,WAAW,CAAC;IACzE,IAAI,CAACe,YAAY,GAAG,IAAI,CAACb,KAAK,CAAChP,aAAa,CAACyB,GAAG,CAAC,IAAI,CAACmO,SAAS,CAAC;IAEhE,IAAI,CAACE,SAAS,GAAG,IAAI,CAACJ,QAAQ,CAACZ,eAAe,EAAE,WAAW,CAAC;IAC5D,IAAI,CAACiB,YAAY,GAAG,IAAI,CAACf,KAAK,CAAChP,aAAa,CAACyB,GAAG,CAAC,IAAI,CAACqO,SAAS,CAAC;IAEhE,IAAI,CAACE,gBAAgB,GAAGlB,eAAe,CAACkB,gBAAgB;;IAExD;IACA,IAAI,CAACC,YAAY,GAAGnB,eAAe,CAACmB,YAAY;IAEhD,IAAI,CAACC,4BAA4B,GAAGpB,eAAe,CAACoB,4BAA4B,IAAI,IAAI;;IAExF;IACA,IAAI,CAACC,YAAY,GAAG,OAAO;EAC/B;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIT,QAAQA,CAACZ,eAAe,EAAW;IAAA,SAAAsB,KAAA,GAAA3P,SAAA,CAAAjB,MAAA,EAAN6Q,IAAI,OAAA1P,KAAA,CAAAyP,KAAA,OAAAA,KAAA,WAAAE,KAAA,MAAAA,KAAA,GAAAF,KAAA,EAAAE,KAAA;MAAJD,IAAI,CAAAC,KAAA,QAAA7P,SAAA,CAAA6P,KAAA;IAAA;IAC7B,KAAK,IAAI1O,GAAG,IAAIyO,IAAI,EAAE;MAClB,IAAIjF,IAAI,GAAG0D,eAAe,CAAClN,GAAG,CAAC;MAE/B,IAAI,CAACwJ,IAAI,EAAE;MAEX,IAAI,OAAOA,IAAI,KAAK,QAAQ,EAAE;QAC1B,IAAIA,IAAI,CAACmF,MAAM,KAAK,YAAY,EAAE;UAC9B,OAAOnF,IAAI,CAAC9C,OAAO;QACvB,CAAC,MAAM;UACH,MAAMrH,KAAK,CAAE,kBAAiBmK,IAAK,EAAC,CAAC;QACzC;MACJ,CAAC,MAAM;QACH,OAAOA,IAAI;MACf;IACJ;IACA,OAAO,IAAI;EACf;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,aAAaoF,eAAeA,CAACpS,6BAA6B,EAMlD;IAAA,IANoD;MACxDqS,iBAAiB,GAAG,IAAI;MACxB3Q,MAAM,GAAG,IAAI;MACb4Q,SAAS,GAAG,IAAI;MAChBC,gBAAgB,GAAG,KAAK;MACxBC,QAAQ,GAAG;IACf,CAAC,GAAAnQ,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,CAAC,CAAC;IAEF,IAAInC,IAAI,GAAG,MAAMH,aAAa,CAACC,6BAA6B,EAAE;MAC1DqS,iBAAiB;MACjB3Q,MAAM;MACN4Q,SAAS;MACTC,gBAAgB;MAChBC;IACJ,CAAC,CAAC;;IAEF;IACA,OAAO,IAAI,IAAI,CAAC,GAAGtS,IAAI,CAAC;EAC5B;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIuS,oBAAoBA,CAACC,MAAM,EAAE;IACzB,OAAOA,MAAM;EACjB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI5P,KAAKA;EACD;EACAjC,IAAI,EAWN;IAAA,IARE;MACI8R,SAAS,GAAG,IAAI;MAChB;MACAC,OAAO,GAAG,KAAK;MACfC,UAAU,GAAG,IAAI;MACjBC,UAAU,GAAG,IAAI;MACjBC,aAAa,GAAG,IAAI,CAAE;IAC1B,CAAC,GAAA1Q,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,CAAC,CAAC;IAGN;IACA,IAAIU,MAAM;IAEV,IAAIR,KAAK,CAACkJ,OAAO,CAAC5K,IAAI,CAAC,EAAE;MACrB,IAAIA,IAAI,CAACO,MAAM,KAAK,CAAC,EAAE;QACnB,MAAMyB,KAAK,CAAC,8BAA8B,CAAC;MAC/C;MAEA,IAAI8P,SAAS,KAAK,IAAI,EAAE;QACpB,IAAI,CAACpQ,KAAK,CAACkJ,OAAO,CAACkH,SAAS,CAAC,EAAE;UAC3B,MAAM9P,KAAK,CAAC,iCAAiC,CAAC;QAElD,CAAC,MAAM,IAAIhC,IAAI,CAACO,MAAM,KAAKuR,SAAS,CAACvR,MAAM,EAAE;UACzC,MAAMyB,KAAK,CAAC,8CAA8C,CAAC;QAC/D;QAEAE,MAAM,GAAGlC,IAAI,CAACsC,GAAG,CACb,CAACC,CAAC,EAAEjC,CAAC,KAAK,IAAI,CAAC6B,MAAM,CAACI,CAAC,EAAEuP,SAAS,CAACxR,CAAC,CAAC,CACzC,CAAC;MAEL,CAAC,MAAM;QACH4B,MAAM,GAAGlC,IAAI,CAACsC,GAAG,CAACwB,CAAC,IAAI,IAAI,CAAC3B,MAAM,CAAC2B,CAAC,CAAC,CAAC;MAC1C;IAEJ,CAAC,MAAM;MACH,IAAI9D,IAAI,KAAK,IAAI,EAAE;QACf,MAAMgC,KAAK,CAAC,sBAAsB,CAAC;MACvC;MAEA,IAAIN,KAAK,CAACkJ,OAAO,CAACkH,SAAS,CAAC,EAAE;QAC1B,MAAM9P,KAAK,CAAC,gHAAgH,CAAC;MACjI;;MAEA;MACAE,MAAM,GAAG,CAAC,IAAI,CAACC,MAAM,CAACnC,IAAI,EAAE8R,SAAS,CAAC,CAAC;IAC3C;IACA;IACA;;IAEA,IAAIK,gBAAgB,GAAGpT,GAAG,CAACmD,MAAM,CAACI,GAAG,CAACwB,CAAC,IAAIA,CAAC,CAACvD,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;;IAExD;IACA,IAAI0R,UAAU,KAAK,IAAI,EAAE;MACrBA,UAAU,GAAGE,gBAAgB;IACjC;;IAEA;IACAF,UAAU,GAAGG,IAAI,CAACpT,GAAG,CAACiT,UAAU,EAAE,IAAI,CAAClB,gBAAgB,CAAC;;IAExD;IACA,IAAIsB,cAAc,GAAG,EAAE;IACvB,IAAIN,OAAO,IAAIC,UAAU,EAAE;MACvB;MACA,KAAK,IAAI1R,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG4B,MAAM,CAAC3B,MAAM,EAAE,EAAED,CAAC,EAAE;QACpC,IAAI4B,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM,KAAK0R,UAAU,EAAE;UACjCI,cAAc,CAAC7R,IAAI,CAAC,IAAIkB,KAAK,CAACQ,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CAAC,CAAC;UACxD;QAEJ,CAAC,MAAM,IAAIpQ,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM,GAAG0R,UAAU,EAAE;UACtC;UACA,IAAID,UAAU,EAAE;YACZ9P,MAAM,CAAC5B,CAAC,CAAC,GAAG4B,MAAM,CAAC5B,CAAC,CAAC,CAAC+C,KAAK,CAAC,CAAC,EAAE4O,UAAU,CAAC;UAC9C;UACAI,cAAc,CAAC7R,IAAI,CAAC,IAAIkB,KAAK,CAACQ,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CAAC,CAAC;QAE5D,CAAC,MAAM;UAAE;UACL,IAAIP,OAAO,EAAE;YACT,IAAIQ,IAAI,GAAGN,UAAU,GAAG/P,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM;YAExC,IAAI,IAAI,CAAC2Q,YAAY,KAAK,OAAO,EAAE;cAC/BmB,cAAc,CAAC7R,IAAI,CACd,IAAIkB,KAAK,CAACQ,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CAAC,CAAEE,MAAM,CAAC,IAAI9Q,KAAK,CAAC6Q,IAAI,CAAC,CAACD,IAAI,CAAC,CAAC,CAAC,CACxE,CAAC;cACDpQ,MAAM,CAAC5B,CAAC,CAAC,CAACE,IAAI,CAAC,GAAG,IAAIkB,KAAK,CAAC6Q,IAAI,CAAC,CAACD,IAAI,CAAC,IAAI,CAAC1B,YAAY,CAAC,CAAC;YAC9D,CAAC,MAAM;cAAE;cACLyB,cAAc,CAAC7R,IAAI,CACd,IAAIkB,KAAK,CAAC6Q,IAAI,CAAC,CAACD,IAAI,CAAC,CAAC,CAAC,CAAEE,MAAM,CAAC,IAAI9Q,KAAK,CAACQ,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CAAC,CACxE,CAAC;cACDpQ,MAAM,CAAC5B,CAAC,CAAC,CAACmS,OAAO,CAAC,GAAG,IAAI/Q,KAAK,CAAC6Q,IAAI,CAAC,CAACD,IAAI,CAAC,IAAI,CAAC1B,YAAY,CAAC,CAAC;YACjE;UAEJ,CAAC,MAAM;YACHyB,cAAc,CAAC7R,IAAI,CAAC,IAAIkB,KAAK,CAACQ,MAAM,CAAC5B,CAAC,CAAC,CAACC,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CAAC,CAAC;UAC5D;QACJ;MACJ;IACJ,CAAC,MAAM;MACHD,cAAc,GAAGnQ,MAAM,CAACI,GAAG,CAACwB,CAAC,IAAI,IAAIpC,KAAK,CAACoC,CAAC,CAACvD,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CAAC,CAAC;IACjE;IAEA,IAAIJ,aAAa,EAAE;MACf,IAAI,EAAEH,OAAO,IAAIC,UAAU,CAAC,EAAE;QAC1B;QACA;;QAEA,IAAI9P,MAAM,CAACwQ,IAAI,CAAC5O,CAAC,IAAIA,CAAC,CAACvD,MAAM,KAAK2B,MAAM,CAAC,CAAC,CAAC,CAAC3B,MAAM,CAAC,EAAE;UACjD,MAAMyB,KAAK,CACP,kFAAkF,GAClF,yFACJ,CAAC;QACL;MACJ;;MAEA;MACA;MACA;MACA,IAAI2Q,IAAI,GAAG,CAACzQ,MAAM,CAAC3B,MAAM,EAAE2B,MAAM,CAAC,CAAC,CAAC,CAAC3B,MAAM,CAAC;MAE5C2B,MAAM,GAAG,IAAIjD,MAAM,CAAC,OAAO,EACvB2T,aAAa,CAAC/M,IAAI,CAAC3D,MAAM,CAAC2I,IAAI,CAAC,CAAC,CAACvI,GAAG,CAACuQ,MAAM,CAAC,CAAC,EAC7CF,IACJ,CAAC;MAEDN,cAAc,GAAG,IAAIpT,MAAM,CACvB,OAAO,EACP2T,aAAa,CAAC/M,IAAI,CAACwM,cAAc,CAACxH,IAAI,CAAC,CAAC,CAACvI,GAAG,CAACuQ,MAAM,CAAC,CAAC,EACrDF,IACJ,CAAC;IACL,CAAC,MAAM;MACH;MACA,IAAI,CAACjR,KAAK,CAACkJ,OAAO,CAAC5K,IAAI,CAAC,EAAE;QACtB;QACAkC,MAAM,GAAGA,MAAM,CAAC,CAAC,CAAC;QAClBmQ,cAAc,GAAGA,cAAc,CAAC,CAAC,CAAC;MACtC;IACJ;;IAGA;IACA,IAAIS,WAAW,GAAG;MACdC,SAAS,EAAE7Q,MAAM;MACjBmQ,cAAc,EAAEA;IACpB,CAAC;;IAED;IACAS,WAAW,GAAG,IAAI,CAAClB,oBAAoB,CAACkB,WAAW,CAAC;IAEpD,OAAOA,WAAW;EACtB;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIE,YAAYA,CAAChT,IAAI,EAAE;IACf,IAAIA,IAAI,KAAK,IAAI,EAAE,OAAO,IAAI;;IAE9B;IACA;IACA;IACA,MAAMiT,QAAQ,GAAGjT,IAAI,CAAC0G,KAAK,CAAC,IAAI,CAAC6J,kBAAkB,CAAC,CAAC2C,MAAM,CAACpP,CAAC,IAAIA,CAAC,CAAC;IAEnE,IAAI5B,MAAM,GAAG+Q,QAAQ,CAAC3Q,GAAG,CAACwB,CAAC,IAAI;MAC3B,IAAI,IAAI,CAAC0I,YAAY,CAACtG,QAAQ,CAACpC,CAAC,CAAC,EAAE;QAC/B;QACA,OAAOA,CAAC;MACZ,CAAC,MAAM;QACH,IAAI,IAAI,CAACkN,YAAY,KAAK,IAAI,EAAE;UAC5BlN,CAAC,GAAGA,CAAC,CAACiH,IAAI,CAAC,CAAC,CAACrE,KAAK,CAAC,KAAK,CAAC,CAACpD,IAAI,CAAC,GAAG,CAAC;QACvC;QAEA,IAAI,IAAI,CAACmG,UAAU,KAAK,IAAI,EAAE;UAC1B3F,CAAC,GAAG,IAAI,CAAC2F,UAAU,CAAC3F,CAAC,CAAC;QAC1B;QAEA,IAAIqP,aAAa,GAAI,IAAI,CAACrD,aAAa,KAAK,IAAI,GAAI,IAAI,CAACA,aAAa,CAAChM,CAAC,CAAC,GAAG,CAACA,CAAC,CAAC;QAE/E,IAAI5B,MAAM,GAAG,IAAI,CAAC6N,KAAK,CAACoD,aAAa,CAAC;QAEtC,OAAOjR,MAAM;MACjB;IACJ,CAAC,CAAC,CAAC2I,IAAI,CAAC,CAAC;IAET,OAAO3I,MAAM;EACjB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACIC,MAAMA,CAACnC,IAAI,EAAoB;IAAA,IAAlB8R,SAAS,GAAAtQ,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,IAAI;IACzB;IACA,IAAIU,MAAM,GAAG,IAAI,CAAC8Q,YAAY,CAAChT,IAAI,CAAC;IACpC,IAAIoT,OAAO,GAAG,IAAI,CAACJ,YAAY,CAAClB,SAAS,CAAC;IAE1C,IAAIuB,cAAc,GAAG,IAAI,CAACpD,cAAc,CAAC/N,MAAM,EAAEkR,OAAO,CAAC;IACzD,IAAI/Q,GAAG,GAAG,IAAI,CAAC0N,KAAK,CAAC3N,qBAAqB,CAACiR,cAAc,CAAC;IAE1D,OAAOhR,GAAG;EACd;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIiR,YAAYA,CAACC,KAAK,EAAoB;IAAA,IAAlBC,WAAW,GAAAhS,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,CAAC,CAAC;IAChC,OAAO+R,KAAK,CAACjR,GAAG,CAACwB,CAAC,IAAI,IAAI,CAACmJ,MAAM,CAACnJ,CAAC,EAAE0P,WAAW,CAAC,CAAC;EACtD;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIvG,MAAMA,CACFwG,SAAS,EAEX;IAAA,IADED,WAAW,GAAAhS,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,CAAC,CAAC;IAEhB,IAAI,CAACE,KAAK,CAACkJ,OAAO,CAAC6I,SAAS,CAAC,IAAIA,SAAS,CAAClT,MAAM,KAAK,CAAC,IAAI,CAAC3B,gBAAgB,CAAC6U,SAAS,CAAC,CAAC,CAAC,CAAC,EAAE;MACxF,MAAMzR,KAAK,CAAC,kDAAkD,CAAC;IACnE;IAEA,OAAO,IAAI,CAAC0R,aAAa,CAACD,SAAS,EAAED,WAAW,CAAC;EACrD;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIE,aAAaA,CACTD,SAAS,EAAAE,IAAA,EAKX;IAAA,IAJE;MACIC,mBAAmB,GAAG,KAAK;MAC3B3C,4BAA4B,GAAG;IACnC,CAAC,GAAA0C,IAAA;IAED,IAAIzR,MAAM,GAAG,IAAI,CAAC6N,KAAK,CAACtN,qBAAqB,CAACgR,SAAS,CAAC;IACxD,IAAIG,mBAAmB,EAAE;MACrB1R,MAAM,GAAGA,MAAM,CAACgR,MAAM,CAACpP,CAAC,IAAI,CAAC,IAAI,CAACoM,cAAc,CAAChK,QAAQ,CAACpC,CAAC,CAAC,CAAC;IACjE;;IAEA;IACA,IAAI+P,OAAO,GAAG,IAAI,CAAChF,OAAO,CAAC3M,MAAM,CAAC;;IAGlC;IACA;IACA,IAAI,IAAI,CAAC2M,OAAO,CAACzN,kBAAkB,EAAE;MACjCyS,OAAO,GAAGA,OAAO,CAACzK,UAAU,CAAC,IAAI,CAACyF,OAAO,CAACzN,kBAAkB,EAAE,GAAG,CAAC;MAClE,IAAIwS,mBAAmB,EAAE;QACrBC,OAAO,GAAGA,OAAO,CAAC9I,IAAI,CAAC,CAAC;MAC5B;IACJ;IAEA,IAAIkG,4BAA4B,IAAI,IAAI,CAACA,4BAA4B,EAAE;MACnE4C,OAAO,GAAG9T,qBAAqB,CAAC8T,OAAO,CAAC;IAC5C;IAEA,OAAOA,OAAO;EAClB;AAEJ;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASC,eAAeA,CAACjC,MAAM,EAAE;EAC7B,IAAIA,MAAM,CAACkB,SAAS,YAAY9T,MAAM,EAAE;IACpC4S,MAAM,CAACkC,cAAc,GAAG,IAAI9U,MAAM,CAC9B,OAAO,EACP,IAAI2T,aAAa,CAACf,MAAM,CAACkB,SAAS,CAACiB,IAAI,CAACzT,MAAM,CAAC,EAC/CsR,MAAM,CAACkB,SAAS,CAACJ,IACrB,CAAC;EACL,CAAC,MAAM,IAAIjR,KAAK,CAACkJ,OAAO,CAACiH,MAAM,CAACkB,SAAS,CAAC,EAAE;IAExC,IAAIrR,KAAK,CAACkJ,OAAO,CAACiH,MAAM,CAACkB,SAAS,CAAC,CAAC,CAAC,CAAC,EAAE;MACpC;MACAlB,MAAM,CAACkC,cAAc,GAAGlC,MAAM,CAACkB,SAAS,CAACzQ,GAAG,CACxCwB,CAAC,IAAI,IAAIpC,KAAK,CAACoC,CAAC,CAACvD,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CACnC,CAAC;IACL,CAAC,MAAM;MACHT,MAAM,CAACkC,cAAc,GAAG,IAAIrS,KAAK,CAACmQ,MAAM,CAACkB,SAAS,CAACxS,MAAM,CAAC,CAAC+R,IAAI,CAAC,CAAC,CAAC;IACtE;EACJ,CAAC,MAAM;IACH,MAAM,IAAItQ,KAAK,CAAC,wCAAwC,CAAC;EAC7D;EAEA,OAAO6P,MAAM;AACjB;;AAEA;AACA;AACA;AACA;AACA,OAAO,MAAMoC,aAAa,SAAStE,mBAAmB,CAAC;EACnD;EACAiC,oBAAoBA,CAACC,MAAM,EAAE;IACzB,OAAOiC,eAAe,CAACjC,MAAM,CAAC;EAClC;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMqC,eAAe,SAASvE,mBAAmB,CAAC;EACrD;EACAiC,oBAAoBA,CAACC,MAAM,EAAE;IACzB,OAAOiC,eAAe,CAACjC,MAAM,CAAC;EAClC;AACJ;AACA,OAAO,MAAMsC,mBAAmB,SAASxE,mBAAmB,CAAC;EACzD;EACAiC,oBAAoBA,CAACC,MAAM,EAAE;IACzB,OAAOiC,eAAe,CAACjC,MAAM,CAAC;EAClC;AACJ;AACA,OAAO,MAAMuC,oBAAoB,SAASzE,mBAAmB,CAAC;EAC1D;EACAiC,oBAAoBA,CAACC,MAAM,EAAE;IACzB,OAAOiC,eAAe,CAACjC,MAAM,CAAC;EAClC;AACJ;AACA,OAAO,MAAMwC,mBAAmB,SAAS1E,mBAAmB,CAAC;AAC7D,OAAO,MAAM2E,WAAW,SAAS3E,mBAAmB,CAAC;AACrD,OAAO,MAAM4E,aAAa,SAAS5E,mBAAmB,CAAC;AACvD,OAAO,MAAM6E,aAAa,SAAS7E,mBAAmB,CAAC;AACvD,OAAO,MAAM8E,gBAAgB,SAAS9E,mBAAmB,CAAC;AAE1D,OAAO,MAAM+E,cAAc,SAAS/E,mBAAmB,CAAC;AACxD,OAAO,MAAMgF,cAAc,SAAShF,mBAAmB,CAAC;EACpD;EACAiC,oBAAoBA,CAACC,MAAM,EAAE;IACzB,OAAOiC,eAAe,CAACjC,MAAM,CAAC;EAClC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM+C,aAAa,SAASjF,mBAAmB,CAAC;EAEnD/O,WAAWA,CAACgP,aAAa,EAAEC,eAAe,EAAE;IACxC,KAAK,CAACD,aAAa,EAAEC,eAAe,CAAC;IAErC,IAAI,CAACgF,aAAa,GAAG,0BAA0B;IAC/C,IAAI,CAACC,cAAc,GAAG,IAAI,CAAC5E,cAAc,CAACgD,MAAM,CAACpP,CAAC,IAAI,IAAI,CAAC+Q,aAAa,CAACE,IAAI,CAACjR,CAAC,CAAC,CAAC;EACrF;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;EACIkR,yBAAyBA,CAACC,UAAU,EAAEC,iBAAiB,EAAEC,eAAe,EAAE;IAGtE;IACA,IAAI,CAAC,IAAI,CAACL,cAAc,CAAC5O,QAAQ,CAACiP,eAAe,CAACC,QAAQ,CAAC,EAAE;MACzD,MAAM,IAAIpT,KAAK,CAAE,yBAAwBmT,eAAe,CAACC,QAAS,oCAAmC,IAAI,CAACN,cAAc,CAACxR,IAAI,CAAC,IAAI,CAAE,GAAE,CAAC;IAC3I;;IAEA;IACA,IAAI6R,eAAe,CAACE,QAAQ,KAAKnU,SAAS,EAAE;MACxC;MACA,IAAI,CAAC,IAAI,CAAC4T,cAAc,CAAC5O,QAAQ,CAACiP,eAAe,CAACE,QAAQ,CAAC,EAAE;QACzD,MAAM,IAAIrT,KAAK,CAAE,yBAAwBmT,eAAe,CAACE,QAAS,oCAAmC,IAAI,CAACP,cAAc,CAACxR,IAAI,CAAC,IAAI,CAAE,GAAE,CAAC;MAC3I;;MAEA;MACA;MACA,KAAK,IAAI6I,IAAI,IAAI,IAAI,CAAC8D,cAAc,CAACpP,MAAM,CAACoL,MAAM,EAAE;QAChD,IAAI,cAAc,IAAIE,IAAI,IAAI,IAAI,CAAC0I,aAAa,CAACE,IAAI,CAAC5I,IAAI,CAACC,YAAY,CAACC,EAAE,CAAC,EAAE;UACzEF,IAAI,CAACC,YAAY,CAACC,EAAE,GAAG8I,eAAe,CAACE,QAAQ;UAC/C;QACJ;MACJ;IACJ;;IAEA;IACAF,eAAe,CAACG,mBAAmB,GAAG,IAAI,CAACvF,KAAK,CAAC3N,qBAAqB,CAAC,CAAC+S,eAAe,CAACC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;IAErG,OAAO,IAAI,CAACnT,KAAK,CAACgT,UAAU,EAAEC,iBAAiB,CAAC;EACpD;AACJ;AAGA,MAAMK,iBAAiB,GAAG,CACtB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,YAAY,CAAC,EACpB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,YAAY,CAAC,EACpB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,YAAY,CAAC,EACpB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,MAAM,CAAC,EACd,CAAC,IAAI,EAAE,MAAM,CAAC,EACd,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,YAAY,CAAC,EACpB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,aAAa,CAAC,EACrB,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,YAAY,CAAC,EACpB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,WAAW,CAAC,EACnB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,YAAY,CAAC,EACpB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,KAAK,CAAC,EACb,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,gBAAgB,CAAC,EACxB,CAAC,IAAI,EAAE,QAAQ,CAAC,EAChB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,eAAe,CAAC,EACvB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,KAAK,EAAE,UAAU,CAAC,EACnB,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,OAAO,CAAC,EACf,CAAC,IAAI,EAAE,SAAS,CAAC,EACjB,CAAC,IAAI,EAAE,UAAU,CAAC,EAClB,CAAC,IAAI,EAAE,WAAW,CAAC,CACtB;;AAED;AACA,MAAMC,wBAAwB,GAAG,IAAIxU,GAAG,CAACuU,iBAAiB,CAAC;AAC3D;AACA,MAAME,gCAAgC,GAAG,IAAIzU,GAAG,CAAC,CAC7C,GAAGuU,iBAAiB,CAACjT,GAAG,CAACoT,KAAA;EAAA,IAAC,CAACC,CAAC,EAAEC,CAAC,CAAC,GAAAF,KAAA;EAAA,OAAK,CAACE,CAAC,EAAED,CAAC,CAAC;AAAA,EAAC,EAC5C,GAAG,CACC,CAAC,SAAS,EAAE,IAAI,CAAC,EACjB,CAAC,WAAW,EAAE,IAAI,CAAC,EACnB,CAAC,SAAS,EAAE,IAAI,CAAC,EACjB,CAAC,SAAS,EAAE,IAAI,CAAC,EACjB,CAAC,eAAe,EAAE,IAAI,CAAC,EACvB,CAAC,QAAQ,EAAE,IAAI,CAAC,EAChB,CAAC,SAAS,EAAE,IAAI,CAAC,EACjB,CAAC,WAAW,EAAE,IAAI,CAAC,EACnB,CAAC,UAAU,EAAE,IAAI,CAAC,EAClB,CAAC,WAAW,EAAE,IAAI,CAAC,EACnB,CAAC,WAAW,EAAE,IAAI,CAAC,CACtB,CACJ,CAAC;;AAEF;AACA;AACA;AACA;AACA,OAAO,MAAME,gBAAgB,SAASlG,mBAAmB,CAAC;EAEtD;AACJ;AACA;AACA;AACA;AACA;EACImG,WAAWA,CAACC,SAAS,EAKb;IAAA,IALe;MACnBC,iBAAiB,GAAG,KAAK;MACzBC,eAAe,GAAG,KAAK;MACvBC,cAAc,GAAG,IAAI;MACrBC,oBAAoB,GAAG;IAC3B,CAAC,GAAA3U,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,CAAC,CAAC;IACF;IACA;;IAEA;IACA;IACA;;IAEA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IAEA,IAAI0U,cAAc,KAAK,IAAI,EAAE;MACzB,MAAMlU,KAAK,CAAC,6BAA6B,CAAC;IAC9C;IACA,IAAIoU,aAAa,GAAG,IAAI;IAExB,SAASC,SAASA,CAAA,EAAG;MACjB,OAAO;QAAE,UAAU,EAAED,aAAa;QAAE,WAAW,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC;QAAE,MAAM,EAAE;MAAG,CAAC;IAC/E;;IAEA;IACA,MAAME,MAAM,GAAG,EAAE;IACjB,IAAIC,KAAK,GAAGF,SAAS,CAAC,CAAC;IACvB,IAAIG,WAAW,GAAG,GAAG;IACrB,MAAMC,eAAe,GAAG,IAAI,CAAC1G,KAAK,CAAC3N,qBAAqB,CAAC,CAAC,kBAAkB,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC;IAErF,IAAIsU,eAAe,GAAG,EAAE;IACxB,IAAIC,IAAI,GAAG,KAAK;IAChB,IAAIC,kBAAkB,GAAG,IAAI;IAG7B,MAAMzG,eAAe,GAAG,IAAIhJ,GAAG,CAAC,IAAI,CAACgJ,eAAe,CAAC;IAErD,KAAK,IAAIxG,MAAM,IAAIoM,SAAS,EAAE;MAC1B;MACA,MAAMtC,SAAS,GAAG9J,MAAM,CAACzH,MAAM;;MAE/B;MACA;MACA,IAAI2U,cAAc,GAAG,IAAI;MACzB,IAAIC,eAAe,GAAGL,eAAe;MAErC,IAAI,QAAQ,IAAI9M,MAAM,EAAE;QACpB,MAAM,CAACoN,SAAS,EAAEC,WAAW,EAAEC,YAAY,CAAC,GAAGtN,MAAM,CAACuN,MAAM;;QAE5D;QACAV,WAAW,IAAIQ,WAAW;QAC1BJ,kBAAkB,GAAGG,SAAS,GAAGE,YAAY;;QAE7C;QACA;QACA;QACA;QACA,IAAID,WAAW,EAAE;UACbF,eAAe,GAAGE,WAAW,GAAGd,cAAc,GAAGO,eAAe;QACpE;QAEA,IAAIQ,YAAY,EAAE;UACd,KAAK,IAAI3W,CAAC,GAAGmT,SAAS,CAAClT,MAAM,GAAG,CAAC,EAAED,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;YAC5C,MAAMuC,KAAK,GAAG4Q,SAAS,CAACnT,CAAC,CAAC;YAC1B,IAAIuC,KAAK,IAAI4T,eAAe,EAAE;cAC1B;cACA;cACA,IAAII,cAAc,KAAK,IAAI,IAAI,CAAChU,KAAK,GAAG4T,eAAe,IAAIP,cAAc,GAAGU,kBAAkB,EAAE;gBAC5F;cACJ;cACAC,cAAc,GAAGhU,KAAK;YAC1B;UACJ;QACJ;MACJ;MAEA,IAAIsU,cAAc,GAAG,EAAE;;MAEvB;MACA,KAAK,MAAMtU,KAAK,IAAI4Q,SAAS,EAAE;QAC3B;QACA;QACA;QACA;QACA;;QAEA,IAAItD,eAAe,CAAC3M,GAAG,CAACX,KAAK,CAAC,EAAE;UAC5B,MAAM7C,IAAI,GAAG,IAAI,CAACiN,MAAM,CAAC,CAACpK,KAAK,CAAC,CAAC;UACjC,IAAI7C,IAAI,CAAC,CAAC,CAAC,KAAK,GAAG,IAAIA,IAAI,CAACA,IAAI,CAACO,MAAM,GAAG,CAAC,CAAC,KAAK,GAAG,EAAE;YAClD,MAAM6W,QAAQ,GAAG5B,wBAAwB,CAAChT,GAAG,CAACxC,IAAI,CAACqD,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAEhE,IAAI+T,QAAQ,KAAKlW,SAAS,EAAE;cACxB;cACA;cACA;cACA,IAAIkV,aAAa,KAAK,IAAI,IAAIgB,QAAQ,KAAKhB,aAAa,IAAI,CAACJ,iBAAiB,EAAE;gBAC5EU,eAAe,CAAClW,IAAI,CAAC2W,cAAc,CAAC;gBACpC,MAAME,eAAe,GAAG,IAAI,CAACC,yBAAyB,CAACZ,eAAe,CAAC;gBACvE,MAAMa,aAAa,GAAG,IAAI,CAACtK,MAAM,CAACoK,eAAe,CAAC;gBAClDd,KAAK,CAACvW,IAAI,GAAGuX,aAAa;gBAC1BjB,MAAM,CAAC9V,IAAI,CAAC+V,KAAK,CAAC;;gBAElB;gBACAG,eAAe,GAAG,EAAE;gBACpBS,cAAc,GAAG,EAAE;gBACnBZ,KAAK,GAAGF,SAAS,CAAC,CAAC;cACvB;cAEAD,aAAa,GAAGG,KAAK,CAACa,QAAQ,GAAGA,QAAQ;YAC7C,CAAC,MAAM;cACH;YAAA;UAER;QACJ,CAAC,MAAM,IAAIvU,KAAK,IAAI4T,eAAe,EAAE;UACjC;UACA,MAAMe,IAAI,GAAG,CAAC3U,KAAK,GAAG4T,eAAe,IAAIP,cAAc,GAAGM,WAAW;UACrE,MAAMiB,YAAY,GAAGrF,IAAI,CAACsF,KAAK,CAACF,IAAI,GAAG,GAAG,CAAC,GAAG,GAAG;UAEjD,IAAIX,cAAc,KAAK,IAAI,IAAIhU,KAAK,IAAIgU,cAAc,EAAE;YACpD;YACA;YACA;YACA;YACA;YACAF,IAAI,GAAG,IAAI;UACf,CAAC,MAAM,IAAIA,IAAI,IAAKD,eAAe,CAACnW,MAAM,GAAG,CAAC,IAAIsC,KAAK,GAAGiU,eAAgB,EAAE;YACxEH,IAAI,GAAG,KAAK;UAChB,CAAC,MAAM,IAAIJ,KAAK,CAACoB,SAAS,CAAC,CAAC,CAAC,KAAK,IAAI,EAAE;YACpCpB,KAAK,CAACoB,SAAS,CAAC,CAAC,CAAC,GAAGF,YAAY;UACrC,CAAC,MAAM;YACH;YACA,IAAIA,YAAY,KAAKlB,KAAK,CAACoB,SAAS,CAAC,CAAC,CAAC,EAAE;cACrC;cACA;cACA;cACA;cACA;YAAA,CACH,MAAM;cACHpB,KAAK,CAACoB,SAAS,CAAC,CAAC,CAAC,GAAGF,YAAY;;cAEjC;cACAf,eAAe,CAAClW,IAAI,CAAC2W,cAAc,CAAC;cACpC,MAAME,eAAe,GAAG,IAAI,CAACC,yBAAyB,CAACZ,eAAe,CAAC;cACvE,MAAMa,aAAa,GAAG,IAAI,CAACtK,MAAM,CAACoK,eAAe,CAAC;cAClDd,KAAK,CAACvW,IAAI,GAAGuX,aAAa;cAC1BjB,MAAM,CAAC9V,IAAI,CAAC+V,KAAK,CAAC;;cAElB;cACAG,eAAe,GAAG,EAAE;cACpBS,cAAc,GAAG,EAAE;cACnBZ,KAAK,GAAGF,SAAS,CAAC,CAAC;YACvB;UACJ;QAEJ,CAAC,MAAM;UACH;UACA;UACA;UACAc,cAAc,CAAC3W,IAAI,CAACqC,KAAK,CAAC;QAE9B;MACJ;MAEA,IAAI,QAAQ,IAAI8G,MAAM,EAAE;QACpB,MAAM,CAACoN,SAAS,EAAEC,WAAW,EAAEC,YAAY,CAAC,GAAGtN,MAAM,CAACuN,MAAM;QAC5DV,WAAW,IAAIO,SAAS,GAAGE,YAAY;MAC3C;;MAEA;MACA,IAAIE,cAAc,CAAC5W,MAAM,GAAG,CAAC,EAAE;QAC3BmW,eAAe,CAAClW,IAAI,CAAC2W,cAAc,CAAC;MACxC,CAAC,MAAM,IAAIT,eAAe,CAACkB,KAAK,CAACC,CAAC,IAAIA,CAAC,CAACtX,MAAM,KAAK,CAAC,CAAC,EAAE;QACnD;QACAgW,KAAK,GAAGF,SAAS,CAAC,CAAC;QACnBK,eAAe,GAAG,EAAE;QACpBS,cAAc,GAAG,EAAE;MACvB;IAEJ;IAEA,IAAIT,eAAe,CAACnW,MAAM,GAAG,CAAC,EAAE;MAC5B,IAAI4V,oBAAoB,IAAIH,iBAAiB,EAAE;QAC3C;QACA;QACA,MAAM,IAAIhU,KAAK,CAAC,6FAA6F,CAAC;MAClH;;MAEA;MACA,MAAMqV,eAAe,GAAG,IAAI,CAACC,yBAAyB,CAACZ,eAAe,CAAC;;MAEvE;MACA,MAAMa,aAAa,GAAG,IAAI,CAACtK,MAAM,CAACoK,eAAe,CAAC;MAClDd,KAAK,CAACvW,IAAI,GAAGuX,aAAa;MAC1BjB,MAAM,CAAC9V,IAAI,CAAC+V,KAAK,CAAC;IACtB;IAEA,IAAIuB,QAAQ,GAAGzR,MAAM,CAACU,MAAM,CAAC,IAAI,CAAC;;IAElC;IACA,MAAMgR,SAAS,GAAGzB,MAAM,CAAChU,GAAG,CAACiU,KAAK,IAAIA,KAAK,CAACvW,IAAI,CAAC,CAACsD,IAAI,CAAC,EAAE,CAAC;IAC1D,IAAI0S,iBAAiB,IAAIC,eAAe,EAAE;MACtC,KAAK,IAAI3V,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGgW,MAAM,CAAC/V,MAAM,EAAE,EAAED,CAAC,EAAE;QACpC,MAAMiW,KAAK,GAAGD,MAAM,CAAChW,CAAC,CAAC;QACvB,IAAI,CAAC0V,iBAAiB,EAAE;UACpB,OAAOO,KAAK,CAAC,WAAW,CAAC;QAC7B;QAEA,IAAI,CAACN,eAAe,EAAE;UAClB,OAAOM,KAAK,CAAC,UAAU,CAAC;QAC5B;MACJ;MACAuB,QAAQ,GAAG;QAAE,QAAQ,EAAExB;MAAO,CAAC;IACnC;IACA,OAAO,CAACyB,SAAS,EAAED,QAAQ,CAAC;EAEhC;;EAEA;AACJ;AACA;AACA;AACA;AACA;EACIR,yBAAyBA,CAACvB,SAAS,EAAE;IACjC;IACA;IACA;IACA,IAAIiC,YAAY,GAAGjC,SAAS,CAAC,CAAC,CAAC;IAC/B,IAAIkC,UAAU,GAAGD,YAAY,CAACzX,MAAM;IACpC,IAAI2X,aAAa,GAAG,EAAE;IACtB,KAAK,IAAI5X,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGyV,SAAS,CAACxV,MAAM,EAAE,EAAED,CAAC,EAAE;MACvC,MAAM6X,aAAa,GAAGpC,SAAS,CAACzV,CAAC,CAAC;MAClC,IAAIvB,GAAG,GAAG,GAAG;MACb,IAAIqZ,UAAU,GAAG,CAACH,UAAU,EAAEA,UAAU,EAAE,CAAC,EAAE,CAAC,CAAC;MAC/C;MACA;MACA;MACA;;MAEA;MACA;MACA;;MAGA;MACA;;MAEA;;MAEA;MACA;;MAEA;;MAEA;MACA;;MAEA;;MAEA;MACA;;MAEA;;MAEA,MAAMI,WAAW,GAAGF,aAAa,CAAC5X,MAAM;MACxC,KAAK,IAAI0H,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGgQ,UAAU,GAAGI,WAAW,EAAE,EAAEpQ,CAAC,EAAE;QAC/C,MAAMqQ,GAAG,GAAGrQ,CAAC,GAAG,OAAO;QACvB,MAAMsQ,SAAS,GAAGnG,IAAI,CAACrT,GAAG,CAAC,CAAC,EAAEkZ,UAAU,GAAGhQ,CAAC,CAAC;QAC7C,MAAMuQ,QAAQ,GAAGpG,IAAI,CAACpT,GAAG,CAACiZ,UAAU,EAAEA,UAAU,GAAGI,WAAW,GAAGpQ,CAAC,CAAC;QACnE,MAAMwQ,IAAI,GAAGT,YAAY,CAAC3U,KAAK,CAACkV,SAAS,EAAEC,QAAQ,CAAC;QACpD,MAAME,UAAU,GAAGtG,IAAI,CAACrT,GAAG,CAAC,CAAC,EAAEkJ,CAAC,GAAGgQ,UAAU,CAAC;QAC9C,MAAMU,SAAS,GAAGvG,IAAI,CAACpT,GAAG,CAACqZ,WAAW,EAAEpQ,CAAC,CAAC;QAC1C,MAAM2Q,KAAK,GAAGT,aAAa,CAAC9U,KAAK,CAACqV,UAAU,EAAEC,SAAS,CAAC;QACxD,IAAIF,IAAI,CAAClY,MAAM,KAAKqY,KAAK,CAACrY,MAAM,EAAE;UAC9B,MAAM,IAAIyB,KAAK,CAAC,2GAA2G,CAAC;QAChI;QACA,MAAM6W,OAAO,GAAGJ,IAAI,CAACvF,MAAM,CAAC,CAAC4F,IAAI,EAAEC,GAAG,KAAKD,IAAI,KAAKF,KAAK,CAACG,GAAG,CAAC,CAAC,CAACxY,MAAM;QACtE,MAAMyY,QAAQ,GAAGH,OAAO,GAAG5Q,CAAC,GAAGqQ,GAAG;QAClC,IAAIO,OAAO,GAAG,CAAC,IAAIG,QAAQ,GAAGja,GAAG,EAAE;UAC/BA,GAAG,GAAGia,QAAQ;UACdZ,UAAU,GAAG,CAACG,SAAS,EAAEC,QAAQ,EAAEE,UAAU,EAAEC,SAAS,CAAC;QAC7D;MACJ;MACA,MAAM,CAACJ,SAAS,EAAEC,QAAQ,EAAEE,UAAU,EAAEC,SAAS,CAAC,GAAGP,UAAU;MAC/D,MAAMa,OAAO,GAAG7G,IAAI,CAAC8G,KAAK,CAAC,CAACV,QAAQ,GAAGD,SAAS,IAAI,CAAC,CAAC;MACtD,MAAMY,QAAQ,GAAG/G,IAAI,CAAC8G,KAAK,CAAC,CAACP,SAAS,GAAGD,UAAU,IAAI,CAAC,CAAC;MACzDR,aAAa,CAAC1X,IAAI,CAAC,GAAGwX,YAAY,CAAC3U,KAAK,CAAC,CAAC,EAAE4V,OAAO,CAAC,CAAC;MACrDjB,YAAY,GAAGG,aAAa,CAAC9U,KAAK,CAAC8V,QAAQ,CAAC;MAC5ClB,UAAU,GAAGD,YAAY,CAACzX,MAAM;IACpC;IACA2X,aAAa,CAAC1X,IAAI,CAAC,GAAGwX,YAAY,CAAC;IACnC,OAAOE,aAAa;EACxB;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACIkB,sBAAsBA,CAAA,EAId;IAAA,IAJe;MACnBhC,QAAQ,GAAG,IAAI;MACfiC,IAAI,GAAG,IAAI;MACXC,aAAa,GAAG;IACpB,CAAC,GAAA9X,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,CAAC,CAAC;IAEF;;IAEA,IAAI+X,kBAAkB,GAAG,EAAE;IAE3B,IAAInC,QAAQ,EAAE;MACV;MACAA,QAAQ,GAAGA,QAAQ,CAAC9N,WAAW,CAAC,CAAC;;MAEjC;MACA,IAAIkQ,aAAa,GAAG/D,gCAAgC,CAACjT,GAAG,CAAC4U,QAAQ,CAAC;MAElE,IAAIoC,aAAa,KAAKtY,SAAS,EAAE;QAC7B;;QAEA,IAAIsU,wBAAwB,CAAChS,GAAG,CAAC4T,QAAQ,CAAC,EAAE;UACxC;UACAoC,aAAa,GAAGpC,QAAQ;QAE5B,CAAC,MAAM;UACH;UACA,MAAMqC,gBAAgB,GAAGrC,QAAQ,CAAC7W,MAAM,KAAK,CAAC;UAC9C,MAAMmZ,KAAK,GAAGD,gBAAgB,GAAGjE,wBAAwB,CAACpE,IAAI,CAAC,CAAC,GAAGoE,wBAAwB,CAACmE,MAAM,CAAC,CAAC;UAEpG,MAAM,IAAI3X,KAAK,CAAE,aAAYoV,QAAS,uCAAsCwC,IAAI,CAACC,SAAS,CAACH,KAAK,CAAE,EAAC,CAAC;QACxG;MACJ;MAEA,IAAII,iBAAiB,GAAG,IAAI,CAAC/J,KAAK,CAAChP,aAAa,CAACyB,GAAG,CAAE,KAAIgX,aAAc,IAAG,CAAC;MAC5E,IAAIM,iBAAiB,KAAK5Y,SAAS,EAAE;QACjC,MAAM,IAAIc,KAAK,CAAE,4BAA2BwX,aAAc,iHAAgH,CAAC;MAC/K;MAEAD,kBAAkB,CAAC/Y,IAAI,CAACsZ,iBAAiB,CAAC;IAC9C,CAAC,MAAM;MACH;MACAP,kBAAkB,CAAC/Y,IAAI,CAAC,IAAI,CAAC;IACjC;IAEA,IAAI6Y,IAAI,EAAE;MACNA,IAAI,GAAGA,IAAI,CAAC/P,WAAW,CAAC,CAAC;MACzB,IAAI+P,IAAI,KAAK,YAAY,IAAIA,IAAI,KAAK,WAAW,EAAE;QAC/C,MAAM,IAAIrX,KAAK,CAAE,SAAQqX,IAAK,iEAAgE,CAAC;MACnG;MAEA,IAAIU,aAAa,GAAG,IAAI,CAAChK,KAAK,CAAChP,aAAa,CAACyB,GAAG,CAAE,KAAI6W,IAAK,IAAG,CAAC;MAC/D,IAAIU,aAAa,KAAK7Y,SAAS,EAAE;QAC7B,MAAM,IAAIc,KAAK,CAAE,wBAAuBqX,IAAK,iHAAgH,CAAC;MAClK;MAEAE,kBAAkB,CAAC/Y,IAAI,CAACuZ,aAAa,CAAC;IAC1C,CAAC,MAAM;MACH;MACAR,kBAAkB,CAAC/Y,IAAI,CAAC,IAAI,CAAC;IACjC;IAEA,IAAI8Y,aAAa,EAAE;MACf,IAAIU,gBAAgB,GAAG,IAAI,CAACjK,KAAK,CAAChP,aAAa,CAACyB,GAAG,CAAE,kBAAiB,CAAC;MACvE,IAAIwX,gBAAgB,KAAK9Y,SAAS,EAAE;QAChC,MAAM,IAAIc,KAAK,CAAC,iJAAiJ,CAAC;MACtK;MAEAuX,kBAAkB,CAAC/Y,IAAI,CAACwZ,gBAAgB,CAAC;IAC7C;IAEA,OAAOT,kBAAkB,CAACjX,GAAG,CAAC,CAACwB,CAAC,EAAExD,CAAC,KAAK,CAACA,CAAC,GAAG,CAAC,EAAEwD,CAAC,CAAC,CAAC,CAACoP,MAAM,CAACpP,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,IAAI,CAAC;EAElF;AACJ;AACA,OAAO,MAAMmW,gBAAgB,SAAStK,mBAAmB,CAAC;AAC1D,OAAO,MAAMuK,aAAa,SAASvK,mBAAmB,CAAC;;AAGvD;AACA;AACA;AACA;AACA,OAAO,MAAMwK,eAAe,SAASxK,mBAAmB,CAAC;EACrD;AACJ;AACA;AACA;AACA;EACI/O,WAAWA,CAACgP,aAAa,EAAEC,eAAe,EAAE;IACxC,KAAK,CAACD,aAAa,EAAEC,eAAe,CAAC;IAErC,IAAI,CAACgF,aAAa,GAAG,gBAAgB;IAErC,IAAI,CAACuF,wBAAwB,GAAG,IAAI,CAACrK,KAAK,CAACjP,KAAK,CAACoS,MAAM,CACnDpP,CAAC,IAAI,IAAI,CAAC+Q,aAAa,CAACE,IAAI,CAACjR,CAAC,CAClC,CAAC;IAEDjE,OAAO,CAACC,IAAI,CAAC,0JAA0J,CAAC;EAC5K;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIkT,YAAYA,CAAChT,IAAI,EAAE;IACf,IAAIA,IAAI,KAAK,IAAI,EAAE,OAAO,IAAI;;IAE9B;IACA,IAAI,CAACqa,SAAS,EAAE,GAAGC,SAAS,CAAC,GAAGta,IAAI,CAAC+K,IAAI,CAAC,CAAC,CAACrE,KAAK,CAAC,IAAI,CAACmO,aAAa,CAAC;IAErE,IAAIyF,SAAS,CAAC/Z,MAAM,KAAK,CAAC,EAAE;MACxB;MACA,OAAO,KAAK,CAACyS,YAAY,CAACqH,SAAS,CAAC;IAExC,CAAC,MAAM,IAAIC,SAAS,CAAC/Z,MAAM,KAAK,CAAC,EAAE;MAC/B;MACA,IAAI,CAAC6W,QAAQ,EAAEpX,IAAI,CAAC,GAAGsa,SAAS;MAEhC,IAAI,CAAC,IAAI,CAACF,wBAAwB,CAAClU,QAAQ,CAACkR,QAAQ,CAAC,EAAE;QACnDvX,OAAO,CAACC,IAAI,CAAE,8BAA6BsX,QAAS,wEAAuEwC,IAAI,CAACC,SAAS,CAAC,IAAI,CAACO,wBAAwB,CAAE,EAAC,CAAC;MAC/K;MACA,OAAOvb,WAAW,CAAC,CAACuY,QAAQ,CAAC,EAAE,KAAK,CAACpE,YAAY,CAAChT,IAAI,CAAC,CAAC;IAC5D;EACJ;AAEJ;;AAEA;AACA;AACA;AACA,MAAMwE,QAAQ,CAAC;EACX5D,WAAWA,CAAA,EAAG;IACV,IAAI,CAAC2Z,IAAI,GAAGC,YAAY,CAACC,OAAO,CAAC,CAAC;EACtC;;EAEA;AACJ;AACA;AACA;EACIhW,MAAMA,CAACiW,KAAK,EAAE;IACV,KAAK,IAAI1a,IAAI,IAAI0a,KAAK,EAAE;MACpB,IAAI,CAACla,IAAI,CAACR,IAAI,CAAC;IACnB;EACJ;;EAEA;AACJ;AACA;AACA;EACIQ,IAAIA,CAACR,IAAI,EAAE;IACP,IAAI2a,IAAI,GAAG,IAAI,CAACJ,IAAI;IACpB,KAAK,IAAIK,EAAE,IAAI5a,IAAI,EAAE;MACjB,IAAI6a,KAAK,GAAGF,IAAI,CAACG,QAAQ,CAACtY,GAAG,CAACoY,EAAE,CAAC;MACjC,IAAIC,KAAK,KAAK3Z,SAAS,EAAE;QACrB2Z,KAAK,GAAGL,YAAY,CAACC,OAAO,CAAC,CAAC;QAC9BE,IAAI,CAACG,QAAQ,CAACzK,GAAG,CAACuK,EAAE,EAAEC,KAAK,CAAC;MAChC;MACAF,IAAI,GAAGE,KAAK;IAChB;IACAF,IAAI,CAACI,MAAM,GAAG,IAAI;EACtB;;EAEA;AACJ;AACA;AACA;AACA;EACI,CAAC9V,kBAAkBA,CAACjF,IAAI,EAAE;IACtB,IAAI2a,IAAI,GAAG,IAAI,CAACJ,IAAI;IACpB,IAAIrM,MAAM,GAAG,EAAE;IACf,KAAK,IAAI5N,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,IAAI,CAACO,MAAM,IAAIoa,IAAI,KAAKzZ,SAAS,EAAE,EAAEZ,CAAC,EAAE;MACxD,MAAMsa,EAAE,GAAG5a,IAAI,CAACM,CAAC,CAAC;MAClB4N,MAAM,IAAI0M,EAAE;MACZD,IAAI,GAAGA,IAAI,CAACG,QAAQ,CAACtY,GAAG,CAACoY,EAAE,CAAC;MAC5B,IAAID,IAAI,KAAKzZ,SAAS,IAAIyZ,IAAI,CAACI,MAAM,EAAE;QACnC,MAAM7M,MAAM;MAChB;IACJ;EACJ;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAMsM,YAAY,CAAC;EACf5Z,WAAWA,CAACma,MAAM,EAAED,QAAQ,EAAE;IAC1B,IAAI,CAACC,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACD,QAAQ,GAAGA,QAAQ;EAC5B;;EAEA;AACJ;AACA;AACA;EACI,OAAOL,OAAOA,CAAA,EAAG;IACb,OAAO,IAAID,YAAY,CAAC,KAAK,EAAE,IAAIxZ,GAAG,CAAC,CAAC,CAAC;EAC7C;AACJ;AAEA,MAAMwE,YAAY,CAAC;EACf;AACJ;AACA;AACA;AACA;AACA;AACA;EACI5E,WAAWA,CAACgE,QAAQ,EAAEZ,UAAU,EAAEG,UAAU,EAAE;IAC1C,IAAI,CAACS,QAAQ,GAAGA,QAAQ;IACxB,IAAI,CAACC,GAAG,GAAGD,QAAQ,CAACrE,MAAM;IAC1B,IAAI,CAACyD,UAAU,GAAGA,UAAU;IAC5B,IAAI,CAACG,UAAU,GAAGA,UAAU;IAC5B,IAAI,CAAC6W,KAAK,GAAG,EAAE;IACf,IAAI,CAACC,UAAU,GAAG,IAAIvZ,KAAK,CAAC,IAAI,CAACmD,GAAG,GAAG,CAAC,CAAC;IACzC,IAAI,CAACqW,QAAQ,GAAG,IAAIxZ,KAAK,CAAC,IAAI,CAACmD,GAAG,GAAG,CAAC,CAAC;IACvC,KAAK,IAAIvE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACuE,GAAG,GAAG,CAAC,EAAE,EAAEvE,CAAC,EAAE;MACnC,IAAI,CAAC2a,UAAU,CAAC3a,CAAC,CAAC,GAAG,EAAE;MACvB,IAAI,CAAC4a,QAAQ,CAAC5a,CAAC,CAAC,GAAG,EAAE;IACzB;IACA,MAAM6a,GAAG,GAAG,IAAIC,gBAAgB,CAAC,IAAI,CAACpX,UAAU,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,GAAG,CAAC;IAC/D,MAAMqX,GAAG,GAAG,IAAID,gBAAgB,CAAC,IAAI,CAACjX,UAAU,EAAE,CAAC,EAAE,IAAI,CAACU,GAAG,EAAE,CAAC,EAAE,GAAG,CAAC;IACtE,IAAI,CAACmW,KAAK,CAACxa,IAAI,CAAC2a,GAAG,CAACG,KAAK,CAAC,CAAC,CAAC;IAC5B,IAAI,CAACN,KAAK,CAACxa,IAAI,CAAC6a,GAAG,CAACC,KAAK,CAAC,CAAC,CAAC;IAC5B,IAAI,CAACL,UAAU,CAAC,IAAI,CAACpW,GAAG,CAAC,CAACrE,IAAI,CAAC6a,GAAG,CAAC;IACnC,IAAI,CAACH,QAAQ,CAAC,CAAC,CAAC,CAAC1a,IAAI,CAAC2a,GAAG,CAAC;EAC9B;;EAEA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI9V,MAAMA,CAACkW,GAAG,EAAEhb,MAAM,EAAEib,KAAK,EAAEtW,OAAO,EAAE;IAChC,MAAMuW,MAAM,GAAG,IAAI,CAACT,KAAK,CAACza,MAAM;IAChC,MAAMoa,IAAI,GAAG,IAAIS,gBAAgB,CAAClW,OAAO,EAAEuW,MAAM,EAAEF,GAAG,EAAEhb,MAAM,EAAEib,KAAK,CAAC;IACtE,IAAI,CAACP,UAAU,CAACM,GAAG,CAAC,CAAC/a,IAAI,CAACma,IAAI,CAAC;IAC/B,IAAI,CAACO,QAAQ,CAACK,GAAG,GAAGhb,MAAM,CAAC,CAACC,IAAI,CAACma,IAAI,CAAC;IACtC,IAAI,CAACK,KAAK,CAACxa,IAAI,CAACma,IAAI,CAAC;EACzB;;EAEA;AACJ;AACA;AACA;AACA;EACIe,OAAOA,CAAA,EAAG;IACN,MAAM7W,GAAG,GAAG,IAAI,CAACA,GAAG;IACpB,IAAI0W,GAAG,GAAG,CAAC;IACX,OAAOA,GAAG,IAAI1W,GAAG,EAAE;MACf,IAAI,IAAI,CAACoW,UAAU,CAACM,GAAG,CAAC,CAAChb,MAAM,IAAI,CAAC,EAAE;QAClC,OAAO,EAAE;MACb;MACA,KAAK,IAAIob,KAAK,IAAI,IAAI,CAACV,UAAU,CAACM,GAAG,CAAC,EAAE;QACpCI,KAAK,CAACC,IAAI,GAAG,IAAI;QACjB,IAAIC,SAAS,GAAG,GAAG;QACnB,IAAIC,QAAQ,GAAG,IAAI;QACnB,KAAK,IAAIC,KAAK,IAAI,IAAI,CAACb,QAAQ,CAACK,GAAG,CAAC,EAAE;UAClC,MAAMC,KAAK,GAAGO,KAAK,CAACC,cAAc,GAAGL,KAAK,CAACH,KAAK;UAChD,IAAIM,QAAQ,KAAK,IAAI,IAAIN,KAAK,GAAGK,SAAS,EAAE;YACxCC,QAAQ,GAAGC,KAAK,CAACT,KAAK,CAAC,CAAC;YACxBO,SAAS,GAAGL,KAAK;UACrB;QACJ;QAEA,IAAIM,QAAQ,KAAK,IAAI,EAAE;UACnBH,KAAK,CAACC,IAAI,GAAGE,QAAQ;UACrBH,KAAK,CAACK,cAAc,GAAGH,SAAS;QACpC,CAAC,MAAM;UACH,OAAO,EAAE;QACb;MACJ;MACA,EAAEN,GAAG;IACT;IAEA,MAAMU,OAAO,GAAG,EAAE;IAClB,MAAM1B,IAAI,GAAG,IAAI,CAACU,UAAU,CAACpW,GAAG,CAAC,CAAC,CAAC,CAAC;IACpC,MAAM+W,IAAI,GAAGrB,IAAI,CAACqB,IAAI;IACtB,IAAIA,IAAI,KAAK,IAAI,EAAE;MACf,OAAO,EAAE;IACb;IAEA,IAAIjB,IAAI,GAAGiB,IAAI,CAACN,KAAK,CAAC,CAAC;IACvB,OAAOX,IAAI,CAACiB,IAAI,KAAK,IAAI,EAAE;MACvBK,OAAO,CAACzb,IAAI,CAACma,IAAI,CAACW,KAAK,CAAC,CAAC,CAAC;MAC1B,MAAMlW,CAAC,GAAGuV,IAAI,CAACW,KAAK,CAAC,CAAC;MACtBX,IAAI,GAAGvV,CAAC,CAACwW,IAAI,CAACN,KAAK,CAAC,CAAC;IACzB;IAEAW,OAAO,CAACC,OAAO,CAAC,CAAC;IACjB,OAAOD,OAAO;EAClB;;EAEA;AACJ;AACA;AACA;EACIE,KAAKA,CAACxB,IAAI,EAAE;IACR,OAAO,IAAI,CAAC/V,QAAQ,CAACvB,KAAK,CAACsX,IAAI,CAACY,GAAG,EAAEZ,IAAI,CAACY,GAAG,GAAGZ,IAAI,CAACpa,MAAM,CAAC;EAChE;;EAEA;AACJ;AACA;EACI2B,MAAMA,CAAA,EAAG;IACL,MAAM8Y,KAAK,GAAG,IAAI,CAACU,OAAO,CAAC,CAAC;IAC5B,OAAOV,KAAK,CAAC1Y,GAAG,CAACwB,CAAC,IAAI,IAAI,CAACqY,KAAK,CAACrY,CAAC,CAAC,CAAC;EACxC;;EAEA;AACJ;AACA;EACIsY,QAAQA,CAAA,EAAG;IACP,MAAMpB,KAAK,GAAG,IAAI,CAACU,OAAO,CAAC,CAAC;IAC5B,OAAOV,KAAK,CAAC1Y,GAAG,CAACwB,CAAC,IAAIA,CAAC,CAACoB,OAAO,CAAC;EACpC;AACJ;AACA,MAAMkW,gBAAgB,CAAC;EACnB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIxa,WAAWA,CAACsE,OAAO,EAAEuW,MAAM,EAAEF,GAAG,EAAEhb,MAAM,EAAEib,KAAK,EAAE;IAC7C,IAAI,CAACtW,OAAO,GAAGA,OAAO;IACtB,IAAI,CAACuW,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACF,GAAG,GAAGA,GAAG;IACd,IAAI,CAAChb,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACib,KAAK,GAAGA,KAAK;IAClB,IAAI,CAACI,IAAI,GAAG,IAAI;IAChB,IAAI,CAACI,cAAc,GAAG,GAAG;EAC7B;;EAEA;AACJ;AACA;AACA;EACIV,KAAKA,CAAA,EAAG;IACJ,MAAMlW,CAAC,GAAG,IAAIgW,gBAAgB,CAAC,IAAI,CAAClW,OAAO,EAAE,IAAI,CAACuW,MAAM,EAAE,IAAI,CAACF,GAAG,EAAE,IAAI,CAAChb,MAAM,EAAE,IAAI,CAACib,KAAK,CAAC;IAC5FpW,CAAC,CAACwW,IAAI,GAAG,IAAI,CAACA,IAAI;IAClBxW,CAAC,CAAC4W,cAAc,GAAG,IAAI,CAACA,cAAc;IACtC,OAAO5W,CAAC;EACZ;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMiX,aAAa,CAAC;EACvB,OAAOC,uBAAuB,GAAG;IAC7B,aAAa,EAAEhI,WAAW;IAC1B,qBAAqB,EAAED,mBAAmB;IAC1C,eAAe,EAAEJ,aAAa;IAC9B,qBAAqB,EAAEE,mBAAmB;IAC1C,sBAAsB,EAAEC,oBAAoB;IAC5C,iBAAiB,EAAEF,eAAe;IAClC,eAAe,EAAEK,aAAa;IAC9B,eAAe,EAAEC,aAAa;IAC9B,kBAAkB,EAAEC,gBAAgB;IACpC,kBAAkB,EAAEoB,gBAAgB;IACpC,kBAAkB,EAAEoE,gBAAgB;IACpC,eAAe,EAAEC,aAAa;IAC9B,iBAAiB,EAAEC,eAAe;IAElC,gBAAgB,EAAEzF,cAAc;IAChC,eAAe,EAAEE,aAAa;IAC9B,gBAAgB,EAAED;EACtB,CAAC;;EAGD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,aAAapD,eAAeA,CAACpS,6BAA6B,EAOlD;IAAA,IAPoD;MACxDod,SAAS,GAAG,IAAI;MAChB/K,iBAAiB,GAAG,IAAI;MACxB3Q,MAAM,GAAG,IAAI;MACb4Q,SAAS,GAAG,IAAI;MAChBC,gBAAgB,GAAG,KAAK;MACxBC,QAAQ,GAAG;IACf,CAAC,GAAAnQ,SAAA,CAAAjB,MAAA,QAAAiB,SAAA,QAAAN,SAAA,GAAAM,SAAA,MAAG,CAAC,CAAC;IAEF,IAAI,CAACoO,aAAa,EAAEC,eAAe,CAAC,GAAG,MAAM3Q,aAAa,CAACC,6BAA6B,EAAE;MACtFod,SAAS;MACT/K,iBAAiB;MACjB3Q,MAAM;MACN4Q,SAAS;MACTC,gBAAgB;MAChBC;IACJ,CAAC,CAAC;;IAEF;IACA,IAAI6K,aAAa,GAAG3M,eAAe,CAAC4M,eAAe,CAACxc,OAAO,CAAC,OAAO,EAAE,EAAE,CAAC;IAExE,IAAI6L,GAAG,GAAG,IAAI,CAACwQ,uBAAuB,CAACE,aAAa,CAAC;IACrD,IAAI,CAAC1Q,GAAG,EAAE;MACNjM,OAAO,CAACC,IAAI,CAAE,4BAA2B0c,aAAc,6CAA4C,CAAC;MACpG1Q,GAAG,GAAG6D,mBAAmB;IAC7B;IACA,OAAO,IAAI7D,GAAG,CAAC8D,aAAa,EAAEC,eAAe,CAAC;EAClD;AACJ"},"metadata":{},"sourceType":"module","externalDependencies":[]}